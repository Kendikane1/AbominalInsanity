{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTxMfmE3MHJy"
      },
      "source": [
        "# **Multimodal Machine Learning with Brain, Image, and Text Data**\n",
        "\n",
        "This notebook provides a comprehensive guide to training machine learning models using multimodal data, including brain signals, images, and text. You will explore various data processing techniques, model training using neural networks and traditional classifiers, and performance evaluation using metrics like accuracy, precision, and recall. The goal is to integrate diverse data sources to improve classification performance and gain deeper insights into complex datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kodAZfDg6jY2"
      },
      "source": [
        "# **Install the package**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFw_r5-5DhAB"
      },
      "source": [
        "##**1. Install the data reading package**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skx0DtiRDmzE"
      },
      "source": [
        "**Library Installation**:\n",
        "\n",
        "The code first installs the required library, mmbra, using the command pip install mmbra, and pip install mmbracategories. This step ensures that all the necessary dependencies for this specific library are available in the environment. The installation process is critical for using the functionalities provided by the mmbra package and mmbracategories package in subsequent steps.\n",
        "\n",
        "**Library Import**:\n",
        "\n",
        "After the installation, imports the mmbra module and mmbracategories module. This import statement makes the library's functions and classes accessible within the code, allowing for seamless integration with other operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHxCL7ebQwli",
        "outputId": "58193e87-f7d3-402f-d1e7-0495e6980ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mmbra\n",
            "  Using cached mmbra-0.1-py3-none-any.whl.metadata (188 bytes)\n",
            "Collecting torch (from mmbra)\n",
            "  Downloading torch-2.2.2-cp39-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from torch->mmbra) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from torch->mmbra) (4.9.0)\n",
            "Requirement already satisfied: sympy in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from torch->mmbra) (1.10.1)\n",
            "Requirement already satisfied: networkx in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from torch->mmbra) (2.8.4)\n",
            "Requirement already satisfied: jinja2 in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from torch->mmbra) (2.11.3)\n",
            "Requirement already satisfied: fsspec in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from torch->mmbra) (2022.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch->mmbra) (2.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch->mmbra) (1.2.1)\n",
            "Using cached mmbra-0.1-py3-none-any.whl (2.2 kB)\n",
            "Downloading torch-2.2.2-cp39-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, mmbra\n",
            "Successfully installed mmbra-0.1 torch-2.2.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Collecting mmbracategories\n",
            "  Using cached mmbracategories-0.1-py3-none-any.whl.metadata (198 bytes)\n",
            "Requirement already satisfied: torch in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from mmbracategories) (2.2.2)\n",
            "Requirement already satisfied: filelock in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from torch->mmbracategories) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from torch->mmbracategories) (4.9.0)\n",
            "Requirement already satisfied: sympy in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from torch->mmbracategories) (1.10.1)\n",
            "Requirement already satisfied: networkx in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from torch->mmbracategories) (2.8.4)\n",
            "Requirement already satisfied: jinja2 in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from torch->mmbracategories) (2.11.3)\n",
            "Requirement already satisfied: fsspec in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from torch->mmbracategories) (2022.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch->mmbracategories) (2.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/ariz/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch->mmbracategories) (1.2.1)\n",
            "Using cached mmbracategories-0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: mmbracategories\n",
            "Successfully installed mmbracategories-0.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install mmbra\n",
        "!pip install mmbracategories\n",
        "import mmbra\n",
        "import mmbracategories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63u4yt6bHbnJ"
      },
      "source": [
        "\n",
        "##**3. Dataset split settings**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geUUwyzFESWS"
      },
      "source": [
        "In this section, we are loading and preparing the brain, image, and text data for subsequent analysis by organizing the datasets and converting them into a format compatible with PyTorch. Here's a step-by-step description of the key operations:\n",
        "\n",
        "**Data Loading**:\n",
        "\n",
        "The code first sets up the data directories by constructing paths for different datasets, including brain, image, and text features. It organizes these paths based on the subject identifier, data type (training or testing), and the model used (e.g., image and text models).\n",
        "The datasets are loaded from .mat files using the scipy.io.loadmat() function. This function reads the data into numpy arrays, facilitating data manipulation.\n",
        "\n",
        "**Data Preprocessing**:\n",
        "\n",
        "For the brain data, specific time intervals are extracted (70ms-400ms), and the data is reshaped to a two-dimensional format to simplify analysis.\n",
        "Image and text data are scaled to enhance numerical stability during model training.\n",
        "Dimensionality reduction is applied to the image data to limit the number of features, making the dataset more manageable and reducing computational complexity.\n",
        "\n",
        "**Conversion to PyTorch Tensors**:\n",
        "\n",
        "The numpy arrays for each dataset (brain, image, text, and labels) are converted into PyTorch tensors. This conversion is crucial for efficient data handling in neural network training, as tensors are optimized for operations on GPU.\n",
        "\n",
        "**Data Summary**:\n",
        "\n",
        "The code prints the shape of each dataset, providing an overview of the number of samples and features for both training and testing sets. This summary helps confirm that the data is correctly formatted and that the expected number of features is present.\n",
        "This process of data loading, preprocessing, and conversion into PyTorch tensors ensures that the brain, image, and text datasets are ready for further analysis or machine learning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwxkPkVI5Qir",
        "outputId": "e9236c4c-a5cc-40b8-9c0b-b1843ee27615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seen_brain_samples= 16540 , seen_brain_features= 561\n",
            "seen_image_samples= 16540 , seen_image_features= 100\n",
            "seen_text_samples= 16540 , seen_text_features= 512\n",
            "seen_label= torch.Size([16540, 1])\n",
            "unseen_brain_samples= 16000 , unseen_brain_features= 561\n",
            "unseen_image_samples= 16000 , unseen_image_features= 100\n",
            "unseen_text_samples= 16000 , unseen_text_features= 512\n",
            "unseen_label= torch.Size([16000, 1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import scipy.io as sio\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# load data\n",
        "data_dir_root = os.path.join('./data', 'ThingsEEG-Text')\n",
        "sbj = 'sub-10'\n",
        "image_model = 'pytorch/cornet_s'\n",
        "text_model = 'CLIPText'\n",
        "roi = '17channels'\n",
        "brain_dir = os.path.join(data_dir_root, 'brain_feature', roi, sbj)\n",
        "image_dir_seen = os.path.join(data_dir_root, 'visual_feature/ThingsTrain', image_model, sbj)\n",
        "image_dir_unseen = os.path.join(data_dir_root, 'visual_feature/ThingsTest', image_model, sbj)\n",
        "text_dir_seen = os.path.join(data_dir_root, 'textual_feature/ThingsTrain/text', text_model, sbj)\n",
        "text_dir_unseen = os.path.join(data_dir_root, 'textual_feature/ThingsTest/text', text_model, sbj)\n",
        "\n",
        "brain_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['data'].astype('double') * 2.0\n",
        "brain_seen = brain_seen[:,:,27:60] # 70ms-400ms\n",
        "brain_seen = np.reshape(brain_seen, (brain_seen.shape[0], -1))\n",
        "image_seen = sio.loadmat(os.path.join(image_dir_seen, 'feat_pca_train.mat'))['data'].astype('double')*50.0\n",
        "text_seen = sio.loadmat(os.path.join(text_dir_seen, 'text_feat_train.mat'))['data'].astype('double')*2.0\n",
        "label_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['class_idx'].T.astype('int')\n",
        "image_seen = image_seen[:,0:100]\n",
        "\n",
        "brain_unseen = sio.loadmat(os.path.join(brain_dir, 'eeg_test_data.mat'))['data'].astype('double')*2.0\n",
        "brain_unseen = brain_unseen[:, :, 27:60]\n",
        "brain_unseen = np.reshape(brain_unseen, (brain_unseen.shape[0], -1))\n",
        "image_unseen = sio.loadmat(os.path.join(image_dir_unseen, 'feat_pca_test.mat'))['data'].astype('double')*50.0\n",
        "text_unseen = sio.loadmat(os.path.join(text_dir_unseen, 'text_feat_test.mat'))['data'].astype('double')*2.0\n",
        "label_unseen = sio.loadmat(os.path.join(brain_dir, 'eeg_test_data.mat'))['class_idx'].T.astype('int')\n",
        "image_unseen = image_unseen[:, 0:100]\n",
        "\n",
        "brain_seen = torch.from_numpy(brain_seen)\n",
        "brain_unseen = torch.from_numpy(brain_unseen)\n",
        "image_seen = torch.from_numpy(image_seen)\n",
        "image_unseen = torch.from_numpy(image_unseen)\n",
        "text_seen = torch.from_numpy(text_seen)\n",
        "text_unseen = torch.from_numpy(text_unseen)\n",
        "label_seen = torch.from_numpy(label_seen)\n",
        "label_unseen = torch.from_numpy(label_unseen)\n",
        "\n",
        "print('seen_brain_samples=', brain_seen.shape[0], ', seen_brain_features=', brain_seen.shape[1])\n",
        "print('seen_image_samples=', image_seen.shape[0], ', seen_image_features=', image_seen.shape[1])\n",
        "print('seen_text_samples=', text_seen.shape[0], ', seen_text_features=', text_seen.shape[1])\n",
        "print('seen_label=', label_seen.shape)\n",
        "print('unseen_brain_samples=', brain_unseen.shape[0], ', unseen_brain_features=', brain_unseen.shape[1])\n",
        "print('unseen_image_samples=', image_unseen.shape[0], ', unseen_image_features=', image_unseen.shape[1])\n",
        "print('unseen_text_samples=', text_unseen.shape[0], ', unseen_text_features=', text_unseen.shape[1])\n",
        "print('unseen_label=', label_unseen.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRpV0TidIst9",
        "outputId": "ca3b8af8-be73-43c4-9431-01039d8fc131"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  1],\n",
              "        [  2],\n",
              "        [  3],\n",
              "        ...,\n",
              "        [198],\n",
              "        [199],\n",
              "        [200]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_unseen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# GZSL Pipeline: Baseline Model and Advanced Methods\n",
        "\n",
        "The following sections implement:\n",
        "1. **Baseline Model [A]**: Logistic Regression on raw EEG features\n",
        "2. **Brain-Text CLIP Encoder** (future)\n",
        "3. **cWGAN-GP for Embedding Synthesis** (future)\n",
        "4. **GZSL Classifier [A+B]** (future)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Structure Summary\n",
        "\n",
        "This section verifies and summarises the existing data structures for the GZSL pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DATA STRUCTURE SUMMARY\n",
            "============================================================\n",
            "    Variable        Shape         Dtype\n",
            "  brain_seen (16540, 561) torch.float64\n",
            "brain_unseen (16000, 561) torch.float64\n",
            "   text_seen (16540, 512) torch.float64\n",
            " text_unseen (16000, 512) torch.float64\n",
            "  image_seen (16540, 100) torch.float64\n",
            "image_unseen (16000, 100) torch.float64\n",
            "  label_seen   (16540, 1)   torch.int64\n",
            "label_unseen   (16000, 1)   torch.int64\n",
            "\n",
            "Seen classes: 1654 unique labels (range: 1 to 1654)\n",
            "Unseen classes: 200 unique labels (range: 1 to 200)\n",
            "\n",
            "Samples per seen class (approx): 10.0\n",
            "Samples per unseen class (approx): 80.0\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# DATA STRUCTURE SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Collect data summary\n",
        "data_summary = {\n",
        "    'Variable': ['brain_seen', 'brain_unseen', 'text_seen', 'text_unseen', \n",
        "                 'image_seen', 'image_unseen', 'label_seen', 'label_unseen'],\n",
        "    'Shape': [str(tuple(brain_seen.shape)), str(tuple(brain_unseen.shape)),\n",
        "              str(tuple(text_seen.shape)), str(tuple(text_unseen.shape)),\n",
        "              str(tuple(image_seen.shape)), str(tuple(image_unseen.shape)),\n",
        "              str(tuple(label_seen.shape)), str(tuple(label_unseen.shape))],\n",
        "    'Dtype': [str(brain_seen.dtype), str(brain_unseen.dtype),\n",
        "              str(text_seen.dtype), str(text_unseen.dtype),\n",
        "              str(image_seen.dtype), str(image_unseen.dtype),\n",
        "              str(label_seen.dtype), str(label_unseen.dtype)]\n",
        "}\n",
        "\n",
        "df_summary = pd.DataFrame(data_summary)\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA STRUCTURE SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(df_summary.to_string(index=False))\n",
        "print()\n",
        "\n",
        "# Label statistics\n",
        "unique_seen = torch.unique(label_seen)\n",
        "unique_unseen = torch.unique(label_unseen)\n",
        "\n",
        "print(f\"Seen classes: {len(unique_seen)} unique labels (range: {unique_seen.min().item()} to {unique_seen.max().item()})\")\n",
        "print(f\"Unseen classes: {len(unique_unseen)} unique labels (range: {unique_unseen.min().item()} to {unique_unseen.max().item()})\")\n",
        "print()\n",
        "print(f\"Samples per seen class (approx): {brain_seen.shape[0] / len(unique_seen):.1f}\")\n",
        "print(f\"Samples per unseen class (approx): {brain_unseen.shape[0] / len(unique_unseen):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration: Class Distributions\n",
        "\n",
        "Visualising the number of EEG samples per class for seen and unseen categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Figure saved to: figures/class_distribution.png\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CLASS DISTRIBUTION HISTOGRAM\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "os.makedirs('figures', exist_ok=True)\n",
        "\n",
        "label_seen_np = label_seen.numpy().flatten()\n",
        "label_unseen_np = label_unseen.numpy().flatten()\n",
        "\n",
        "seen_class_counts = np.bincount(label_seen_np)[1:]\n",
        "unseen_class_counts = np.bincount(label_unseen_np)[1:]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].hist(seen_class_counts, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "axes[0].set_xlabel('Samples per Class', fontsize=12)\n",
        "axes[0].set_ylabel('Number of Classes', fontsize=12)\n",
        "axes[0].set_title(f'Seen Classes Distribution\\n({len(seen_class_counts)} classes, {len(label_seen_np)} total samples)', fontsize=13)\n",
        "axes[0].axvline(np.mean(seen_class_counts), color='red', linestyle='--', label=f'Mean: {np.mean(seen_class_counts):.1f}')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].hist(unseen_class_counts, bins=30, color='darkorange', edgecolor='black', alpha=0.7)\n",
        "axes[1].set_xlabel('Samples per Class', fontsize=12)\n",
        "axes[1].set_ylabel('Number of Classes', fontsize=12)\n",
        "axes[1].set_title(f'Unseen Classes Distribution\\n({len(unseen_class_counts)} classes, {len(label_unseen_np)} total samples)', fontsize=13)\n",
        "axes[1].axvline(np.mean(unseen_class_counts), color='red', linestyle='--', label=f'Mean: {np.mean(unseen_class_counts):.1f}')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/class_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFigure saved to: figures/class_distribution.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration: EEG Feature Norms\n",
        "\n",
        "Visualising the distribution of EEG signal norms (L2) to understand feature variability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Figure saved to: figures/eeg_norm_distribution.png\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# EEG FEATURE NORM DISTRIBUTION\n",
        "# =============================================================================\n",
        "\n",
        "brain_seen_np = brain_seen.numpy()\n",
        "brain_unseen_np = brain_unseen.numpy()\n",
        "\n",
        "norms_seen = np.linalg.norm(brain_seen_np, axis=1)\n",
        "norms_unseen = np.linalg.norm(brain_unseen_np, axis=1)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "ax.hist(norms_seen, bins=50, alpha=0.6, label=f'Seen (n={len(norms_seen)})', color='steelblue', edgecolor='black')\n",
        "ax.hist(norms_unseen, bins=50, alpha=0.6, label=f'Unseen (n={len(norms_unseen)})', color='darkorange', edgecolor='black')\n",
        "\n",
        "ax.set_xlabel('L2 Norm of EEG Features', fontsize=12)\n",
        "ax.set_ylabel('Count', fontsize=12)\n",
        "ax.set_title('Distribution of EEG Feature Norms (Seen vs Unseen)', fontsize=13)\n",
        "ax.legend()\n",
        "\n",
        "stats_text = f'Seen: \u03bc={np.mean(norms_seen):.2f}, \u03c3={np.std(norms_seen):.2f}\\n'\n",
        "stats_text += f'Unseen: \u03bc={np.mean(norms_unseen):.2f}, \u03c3={np.std(norms_unseen):.2f}'\n",
        "ax.text(0.98, 0.95, stats_text, transform=ax.transAxes, fontsize=10,\n",
        "        verticalalignment='top', horizontalalignment='right',\n",
        "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/eeg_norm_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFigure saved to: figures/eeg_norm_distribution.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Baseline Model A: Logistic Regression on Raw EEG\n",
        "\n",
        "This section implements the baseline classifier using multinomial Logistic Regression on raw EEG features.\n",
        "\n",
        "**Training setup:**\n",
        "- 80/20 train/test split on seen classes\n",
        "- StandardScaler for feature normalisation\n",
        "- Multinomial Logistic Regression with LBFGS solver\n",
        "\n",
        "**Metrics:**\n",
        "- Accuracy\n",
        "- Macro F1-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed set to: 42\n",
            "\n",
            "Train set: 13232 samples\n",
            "Test set: 3308 samples\n",
            "Feature dimension: 561\n",
            "\n",
            "Features standardised (mean=0, std=1)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# BASELINE MODEL A: DATA PREPARATION\n",
        "# =============================================================================\n",
        "\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Set random seeds\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(f\"Random seed set to: {SEED}\")\n",
        "\n",
        "# Prepare data\n",
        "X_seen = brain_seen.numpy()\n",
        "y_seen = label_seen.numpy().flatten()\n",
        "\n",
        "# 80/20 train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_seen, y_seen, test_size=0.2, random_state=SEED, stratify=y_seen\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Feature dimension: {X_train.shape[1]}\")\n",
        "\n",
        "# Standardise features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nFeatures standardised (mean=0, std=1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Logistic Regression classifier...\n",
            "Number of classes: 1654\n",
            "This may take a few minutes...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUNNING THE L-BFGS-B CODE\n",
            "\n",
            "           * * *\n",
            "\n",
            "Machine precision = 2.220D-16\n",
            " N =       929548     M =           10\n",
            "\n",
            "At X0         0 variables are exactly at the bounds\n",
            "\n",
            "At iterate    0    f=  9.80617D+04    |proj g|=  3.21986D+01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " This problem is unconstrained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "At iterate   50    f=  3.81957D+03    |proj g|=  1.44157D-01\n",
            "\n",
            "At iterate  100    f=  3.80474D+03    |proj g|=  1.23949D-01\n",
            "\n",
            "At iterate  150    f=  3.80419D+03    |proj g|=  1.50925D-02\n",
            "\n",
            "At iterate  200    f=  3.80416D+03    |proj g|=  9.54507D-03\n",
            "\n",
            "           * * *\n",
            "\n",
            "Tit   = total number of iterations\n",
            "Tnf   = total number of function evaluations\n",
            "Tnint = total number of segments explored during Cauchy searches\n",
            "Skip  = number of BFGS updates skipped\n",
            "Nact  = number of active bounds at final generalized Cauchy point\n",
            "Projg = norm of the final projected gradient\n",
            "F     = final function value\n",
            "\n",
            "           * * *\n",
            "\n",
            "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
            "*****    233    242      1     0     0   4.168D-03   3.804D+03\n",
            "  F =   3804.1592357048216     \n",
            "\n",
            "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
            "\n",
            "Training complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 12.4min finished\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TRAIN BASELINE LOGISTIC REGRESSION\n",
        "# =============================================================================\n",
        "# Note: Training may take a few minutes due to the large number of classes\n",
        "\n",
        "print(\"Training Logistic Regression classifier...\")\n",
        "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
        "print(\"This may take a few minutes...\\n\")\n",
        "\n",
        "baseline_clf = LogisticRegression(\n",
        "    multi_class='multinomial',\n",
        "    solver='lbfgs',\n",
        "    max_iter=1000,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "baseline_clf.fit(X_train_scaled, y_train)\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "BASELINE MODEL A: EVALUATION RESULTS\n",
            "============================================================\n",
            "Accuracy:     0.0215 (2.15%)\n",
            "Macro F1:     0.0190\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# EVALUATE BASELINE MODEL\n",
        "# =============================================================================\n",
        "\n",
        "y_pred = baseline_clf.predict(X_test_scaled)\n",
        "\n",
        "baseline_accuracy = accuracy_score(y_test, y_pred)\n",
        "baseline_macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "# Store results\n",
        "baseline_results = {\n",
        "    'model': 'Baseline A (Logistic Regression on Raw EEG)',\n",
        "    'accuracy': baseline_accuracy,\n",
        "    'macro_f1': baseline_macro_f1,\n",
        "    'n_train': X_train.shape[0],\n",
        "    'n_test': X_test.shape[0],\n",
        "    'n_classes': len(np.unique(y_train)),\n",
        "    'n_features': X_train.shape[1]\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BASELINE MODEL A: EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy:     {baseline_accuracy:.4f} ({baseline_accuracy*100:.2f}%)\")\n",
        "print(f\"Macro F1:     {baseline_macro_f1:.4f}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: Baseline Model and Data Exploration\n",
        "\n",
        "This section prints a comprehensive summary of the baseline implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SUMMARY: BASELINE MODEL [A] IMPLEMENTATION\n",
            "======================================================================\n",
            "\n",
            "### 1. BASELINE PERFORMANCE METRICS ###\n",
            "   Accuracy:  0.0215 (2.15%)\n",
            "   Macro F1:  0.0190\n",
            "\n",
            "### 2. KEY ARRAY SHAPES ###\n",
            "   brain_seen:    (16540, 561)  (EEG features for seen classes)\n",
            "   brain_unseen:  (16000, 561)  (EEG features for unseen classes)\n",
            "   text_seen:     (16540, 512)  (CLIP text embeddings for seen)\n",
            "   text_unseen:   (16000, 512)  (CLIP text embeddings for unseen)\n",
            "   label_seen:    (16540, 1)  (class labels for seen)\n",
            "   label_unseen:  (16000, 1)  (class labels for unseen)\n",
            "\n",
            "### 3. ASSUMPTIONS MADE ###\n",
            "   1. SEEN/UNSEEN SPLIT: Using pre-defined split from dataset loader\n",
            "      - Seen classes: 1654 (used for training baseline)\n",
            "      - Unseen classes: 200 (reserved for GZSL evaluation)\n",
            "   2. TRAIN/TEST SPLIT: 80/20 stratified split within seen classes\n",
            "      - Training samples: 13232\n",
            "      - Test samples: 3308\n",
            "   3. FEATURE PREPROCESSING: StandardScaler (Z-score normalisation)\n",
            "   4. LABELS: 1-indexed (labels start from 1, not 0)\n",
            "   5. RANDOM SEED: 42 (for reproducibility)\n",
            "\n",
            "### 4. FIGURES GENERATED ###\n",
            "   - figures/class_distribution.png\n",
            "   - figures/eeg_norm_distribution.png\n",
            "\n",
            "======================================================================\n",
            "NEXT STEPS: Implement Brain-Text CLIP Encoder\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# FINAL SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SUMMARY: BASELINE MODEL [A] IMPLEMENTATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n### 1. BASELINE PERFORMANCE METRICS ###\")\n",
        "print(f\"   Accuracy:  {baseline_results['accuracy']:.4f} ({baseline_results['accuracy']*100:.2f}%)\")\n",
        "print(f\"   Macro F1:  {baseline_results['macro_f1']:.4f}\")\n",
        "\n",
        "print(\"\\n### 2. KEY ARRAY SHAPES ###\")\n",
        "print(f\"   brain_seen:    {tuple(brain_seen.shape)}  (EEG features for seen classes)\")\n",
        "print(f\"   brain_unseen:  {tuple(brain_unseen.shape)}  (EEG features for unseen classes)\")\n",
        "print(f\"   text_seen:     {tuple(text_seen.shape)}  (CLIP text embeddings for seen)\")\n",
        "print(f\"   text_unseen:   {tuple(text_unseen.shape)}  (CLIP text embeddings for unseen)\")\n",
        "print(f\"   label_seen:    {tuple(label_seen.shape)}  (class labels for seen)\")\n",
        "print(f\"   label_unseen:  {tuple(label_unseen.shape)}  (class labels for unseen)\")\n",
        "\n",
        "print(\"\\n### 3. ASSUMPTIONS MADE ###\")\n",
        "print(\"   1. SEEN/UNSEEN SPLIT: Using pre-defined split from dataset loader\")\n",
        "print(f\"      - Seen classes: {len(torch.unique(label_seen))} (used for training baseline)\")\n",
        "print(f\"      - Unseen classes: {len(torch.unique(label_unseen))} (reserved for GZSL evaluation)\")\n",
        "print(\"   2. TRAIN/TEST SPLIT: 80/20 stratified split within seen classes\")\n",
        "print(f\"      - Training samples: {baseline_results['n_train']}\")\n",
        "print(f\"      - Test samples: {baseline_results['n_test']}\")\n",
        "print(\"   3. FEATURE PREPROCESSING: StandardScaler (Z-score normalisation)\")\n",
        "print(\"   4. LABELS: 1-indexed (labels start from 1, not 0)\")\n",
        "print(\"   5. RANDOM SEED: 42 (for reproducibility)\")\n",
        "\n",
        "print(\"\\n### 4. FIGURES GENERATED ###\")\n",
        "print(\"   - figures/class_distribution.png\")\n",
        "print(\"   - figures/eeg_norm_distribution.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"NEXT STEPS: Implement Brain-Text CLIP Encoder\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## GZSL-Style Baseline Evaluation\n",
        "\n",
        "This section evaluates the baseline classifier under a **Generalised Zero-Shot Learning (GZSL)** setting:\n",
        "\n",
        "1. **Seen-class evaluation**: Already computed above on `X_test_seen`\n",
        "2. **Unseen-class evaluation**: Apply the same classifier to `brain_unseen` (EEG from unseen classes)\n",
        "3. **Harmonic mean (H)**: Balance between seen and unseen accuracy\n",
        "\n",
        "> **Expected result**: The baseline classifier will achieve ~0% accuracy on unseen classes because it was trained only on seen-class labels. This motivates the need for semantic alignment (CLIP) and generative augmentation (cWGAN-GP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seen-class evaluation (from training split):\n",
            "  Accuracy:  0.0215\n",
            "  Macro F1:  0.0190\n",
            "\n",
            "Unseen-class data prepared:\n",
            "  Samples: 16000\n",
            "  Features: 561\n",
            "  Unique labels: 200\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# GZSL-STYLE BASELINE EVALUATION\n",
        "# =============================================================================\n",
        "# Evaluate the baseline classifier on BOTH seen and unseen classes.\n",
        "# The classifier was trained ONLY on seen classes, so it cannot predict unseen labels.\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. Store seen-class results (from previous evaluation)\n",
        "# -----------------------------------------------------------------------------\n",
        "baseline_seen_results = {\n",
        "    'acc_seen': baseline_accuracy,\n",
        "    'macro_f1_seen': baseline_macro_f1\n",
        "}\n",
        "\n",
        "print(\"Seen-class evaluation (from training split):\")\n",
        "print(f\"  Accuracy:  {baseline_seen_results['acc_seen']:.4f}\")\n",
        "print(f\"  Macro F1:  {baseline_seen_results['macro_f1_seen']:.4f}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. Prepare unseen EEG data using the SAME scaler\n",
        "# -----------------------------------------------------------------------------\n",
        "X_unseen = brain_unseen.numpy()\n",
        "y_unseen = label_unseen.numpy().flatten()\n",
        "\n",
        "# Apply the same standardisation as training data\n",
        "X_unseen_scaled = scaler.transform(X_unseen)\n",
        "\n",
        "print(f\"\\nUnseen-class data prepared:\")\n",
        "print(f\"  Samples: {X_unseen.shape[0]}\")\n",
        "print(f\"  Features: {X_unseen.shape[1]}\")\n",
        "print(f\"  Unique labels: {len(np.unique(y_unseen))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GZSL Evaluation Results:\n",
            "  Acc (seen):    0.0215 (2.15%)\n",
            "  Acc (unseen):  0.0006 (0.06%)\n",
            "  Harmonic Mean: 0.0011\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# PREDICT ON UNSEEN CLASSES\n",
        "# =============================================================================\n",
        "\n",
        "# Predict using the baseline classifier (trained only on seen classes)\n",
        "y_pred_unseen = baseline_clf.predict(X_unseen_scaled)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Compute unseen-class metrics\n",
        "# -----------------------------------------------------------------------------\n",
        "acc_unseen = accuracy_score(y_unseen, y_pred_unseen)\n",
        "macro_f1_unseen = f1_score(y_unseen, y_pred_unseen, average='macro', zero_division=0)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Compute Harmonic Mean (H)\n",
        "# -----------------------------------------------------------------------------\n",
        "# H = 2 * Acc_seen * Acc_unseen / (Acc_seen + Acc_unseen)\n",
        "# Handle division by zero if both are 0\n",
        "acc_seen = baseline_seen_results['acc_seen']\n",
        "if acc_seen + acc_unseen > 0:\n",
        "    H = 2 * acc_seen * acc_unseen / (acc_seen + acc_unseen)\n",
        "else:\n",
        "    H = 0.0\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Store GZSL results\n",
        "# -----------------------------------------------------------------------------\n",
        "baseline_gzsl_results = {\n",
        "    'acc_seen': acc_seen,\n",
        "    'acc_unseen': acc_unseen,\n",
        "    'H': H,\n",
        "    'macro_f1_seen': baseline_seen_results['macro_f1_seen'],\n",
        "    'macro_f1_unseen': macro_f1_unseen\n",
        "}\n",
        "\n",
        "print(\"GZSL Evaluation Results:\")\n",
        "print(f\"  Acc (seen):    {acc_seen:.4f} ({acc_seen*100:.2f}%)\")\n",
        "print(f\"  Acc (unseen):  {acc_unseen:.4f} ({acc_unseen*100:.2f}%)\")\n",
        "print(f\"  Harmonic Mean: {H:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label Set Analysis:\n",
            "  Seen class labels (classifier knows):  1654 classes\n",
            "  Unseen class labels (ground truth):    200 classes\n",
            "  Predicted labels on unseen data:       1637 unique values\n",
            "\n",
            "  Overlap (correctly predictable):        197 classes\n",
            "\n",
            "  All predictions within seen labels?    True\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# VERIFY: PREDICTIONS CONFINED TO SEEN-CLASS LABELS\n",
        "# =============================================================================\n",
        "# The classifier can ONLY predict labels it was trained on (seen classes).\n",
        "# Unseen-class labels are never in the prediction set.\n",
        "\n",
        "# Get the set of labels the classifier knows\n",
        "seen_labels_set = set(baseline_clf.classes_)\n",
        "unseen_labels_set = set(np.unique(y_unseen))\n",
        "predicted_labels_set = set(np.unique(y_pred_unseen))\n",
        "\n",
        "print(\"Label Set Analysis:\")\n",
        "print(f\"  Seen class labels (classifier knows):  {len(seen_labels_set)} classes\")\n",
        "print(f\"  Unseen class labels (ground truth):    {len(unseen_labels_set)} classes\")\n",
        "print(f\"  Predicted labels on unseen data:       {len(predicted_labels_set)} unique values\")\n",
        "\n",
        "# Check overlap\n",
        "overlap = predicted_labels_set.intersection(unseen_labels_set)\n",
        "print(f\"\\n  Overlap (correctly predictable):        {len(overlap)} classes\")\n",
        "\n",
        "# Confirm predictions are confined to seen labels\n",
        "predictions_in_seen = predicted_labels_set.issubset(seen_labels_set)\n",
        "print(f\"\\n  All predictions within seen labels?    {predictions_in_seen}\")\n",
        "\n",
        "if predictions_in_seen and len(overlap) == 0:\n",
        "    print(\"\\n  \u2713 CONFIRMED: Classifier never predicts unseen-class labels.\")\n",
        "    print(\"    This is expected \u2014 the baseline has no mechanism for zero-shot transfer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "GZSL BASELINE EVALUATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "### baseline_seen_results ###\n",
            "   acc_seen: 0.0215\n",
            "   macro_f1_seen: 0.0190\n",
            "\n",
            "### baseline_gzsl_results ###\n",
            "   acc_seen: 0.0215\n",
            "   acc_unseen: 0.0006\n",
            "   H: 0.0011\n",
            "   macro_f1_seen: 0.0190\n",
            "   macro_f1_unseen: 0.0001\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "KEY OBSERVATION:\n",
            "----------------------------------------------------------------------\n",
            "The baseline classifier achieves ~0% accuracy on unseen classes because:\n",
            "  1. It was trained ONLY on seen-class labels\n",
            "  2. It has no semantic knowledge to transfer to new categories\n",
            "  3. All predictions are confined to the seen-class label set\n",
            "\n",
            "This motivates the need for:\n",
            "  \u2022 Brain-Text CLIP encoder (semantic alignment)\n",
            "  \u2022 cWGAN-GP (synthetic unseen-class embeddings)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# GZSL BASELINE SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"GZSL BASELINE EVALUATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n### baseline_seen_results ###\")\n",
        "for k, v in baseline_seen_results.items():\n",
        "    print(f\"   {k}: {v:.4f}\")\n",
        "\n",
        "print(\"\\n### baseline_gzsl_results ###\")\n",
        "for k, v in baseline_gzsl_results.items():\n",
        "    print(f\"   {k}: {v:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"KEY OBSERVATION:\")\n",
        "print(\"-\"*70)\n",
        "print(\"The baseline classifier achieves ~0% accuracy on unseen classes because:\")\n",
        "print(\"  1. It was trained ONLY on seen-class labels\")\n",
        "print(\"  2. It has no semantic knowledge to transfer to new categories\")\n",
        "print(\"  3. All predictions are confined to the seen-class label set\")\n",
        "print(\"\\nThis motivates the need for:\")\n",
        "print(\"  \u2022 Brain-Text CLIP encoder (semantic alignment)\")\n",
        "print(\"  \u2022 cWGAN-GP (synthetic unseen-class embeddings)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Brain-Text CLIP Encoder\n",
        "\n",
        "This section implements a **CLIP-style contrastive encoder** that aligns EEG embeddings and text embeddings in a shared semantic space.\n",
        "\n",
        "**Architecture:**\n",
        "- Brain Encoder `f_b`: 561 \u2192 1024 \u2192 512 \u2192 d (default d=64)\n",
        "- Text Projector `g`: 512 \u2192 512 \u2192 d\n",
        "- Both outputs are L2-normalised\n",
        "\n",
        "**Loss:** Symmetric InfoNCE / CLIP contrastive loss\n",
        "\n",
        "**Training data:** Seen EEG trials only (no leakage from unseen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CLIP CONFIGURATION\n",
            "============================================================\n",
            "  embed_dim: 64\n",
            "  tau: 0.07\n",
            "  epochs: 20\n",
            "  batch_size: 256\n",
            "  lr: 0.0001\n",
            "  weight_decay: 0.0001\n",
            "  dropout: 0.1\n",
            "  seed: 42\n",
            "  device: cpu\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CLIP CONFIGURATION\n",
        "# =============================================================================\n",
        "# All hyperparameters in one place for reproducibility\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Configuration dictionary\n",
        "CLIP_CONFIG = {\n",
        "    'embed_dim': 64,           # Output embedding dimension d\n",
        "    'tau': 0.07,               # Temperature for contrastive loss\n",
        "    'epochs': 20,              # Training epochs\n",
        "    'batch_size': 256,         # Batch size\n",
        "    'lr': 1e-4,                # Learning rate\n",
        "    'weight_decay': 1e-4,      # AdamW weight decay\n",
        "    'dropout': 0.1,            # Dropout rate\n",
        "    'seed': 42,                # Random seed\n",
        "}\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = CLIP_CONFIG['seed']\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CLIP CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "for k, v in CLIP_CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(f\"  device: {device}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Brain Encoder parameters: 1,133,120\n",
            "Text Projector parameters: 295,488\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CLIP MODEL DEFINITIONS\n",
        "# =============================================================================\n",
        "\n",
        "class BrainEncoder(nn.Module):\n",
        "    \"\"\"EEG feature encoder: 561 -> d with L2 normalisation.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim=561, hidden_dims=[1024, 512], embed_dim=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
        "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
        "        self.fc3 = nn.Linear(hidden_dims[1], embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        # L2 normalisation\n",
        "        x = F.normalize(x, p=2, dim=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TextProjector(nn.Module):\n",
        "    \"\"\"Text embedding projector: 512 -> d with L2 normalisation.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim=512, hidden_dim=512, embed_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        # L2 normalisation\n",
        "        x = F.normalize(x, p=2, dim=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "def clip_loss(brain_embeds, text_embeds, tau=0.07):\n",
        "    \"\"\"\n",
        "    Symmetric CLIP/InfoNCE contrastive loss.\n",
        "    \n",
        "    Args:\n",
        "        brain_embeds: (B, d) L2-normalised brain embeddings\n",
        "        text_embeds: (B, d) L2-normalised text embeddings\n",
        "        tau: temperature parameter\n",
        "    \n",
        "    Returns:\n",
        "        Scalar loss: (L_e2t + L_t2e) / 2\n",
        "    \"\"\"\n",
        "    # Similarity matrix (dot product = cosine sim due to L2 norm)\n",
        "    logits = torch.matmul(brain_embeds, text_embeds.T) / tau  # (B, B)\n",
        "    \n",
        "    # Targets: diagonal (i.e., matched pairs)\n",
        "    batch_size = brain_embeds.size(0)\n",
        "    targets = torch.arange(batch_size, device=brain_embeds.device)\n",
        "    \n",
        "    # Cross-entropy in both directions\n",
        "    loss_e2t = F.cross_entropy(logits, targets)  # brain -> text\n",
        "    loss_t2e = F.cross_entropy(logits.T, targets)  # text -> brain\n",
        "    \n",
        "    return (loss_e2t + loss_t2e) / 2\n",
        "\n",
        "\n",
        "# Instantiate models\n",
        "brain_encoder = BrainEncoder(\n",
        "    input_dim=561,\n",
        "    embed_dim=CLIP_CONFIG['embed_dim'],\n",
        "    dropout=CLIP_CONFIG['dropout']\n",
        ").to(device)\n",
        "\n",
        "text_projector = TextProjector(\n",
        "    input_dim=512,\n",
        "    embed_dim=CLIP_CONFIG['embed_dim']\n",
        ").to(device)\n",
        "\n",
        "print(f\"Brain Encoder parameters: {sum(p.numel() for p in brain_encoder.parameters()):,}\")\n",
        "print(f\"Text Projector parameters: {sum(p.numel() for p in text_projector.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CLIP Training data: 13232 samples\n",
            "CLIP Test data (seen): 3308 samples\n",
            "Unseen data (test only): 16000 samples\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CLIP DATA PREPARATION\n",
        "# =============================================================================\n",
        "# Use only seen train split for CLIP training (no leakage!)\n",
        "\n",
        "# The baseline already created X_train, X_test using train_test_split\n",
        "# We need to also get the corresponding text embeddings\n",
        "\n",
        "# Get indices for train/test split (recreate with same seed)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Recreate the split indices\n",
        "indices_seen = np.arange(len(brain_seen))\n",
        "train_idx, test_idx = train_test_split(\n",
        "    indices_seen, test_size=0.2, random_state=42, stratify=label_seen.numpy().flatten()\n",
        ")\n",
        "\n",
        "# Get brain and text for train/test\n",
        "brain_train_seen = brain_seen[train_idx].numpy()\n",
        "brain_test_seen_np = brain_seen[test_idx].numpy()\n",
        "text_train_seen = text_seen[train_idx].numpy()\n",
        "text_test_seen = text_seen[test_idx].numpy()\n",
        "label_train_seen = label_seen[train_idx].numpy().flatten()\n",
        "label_test_seen = label_seen[test_idx].numpy().flatten()\n",
        "\n",
        "# Standardise brain features (fit on train only)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "clip_scaler = StandardScaler()\n",
        "brain_train_seen_scaled = clip_scaler.fit_transform(brain_train_seen)\n",
        "brain_test_seen_scaled = clip_scaler.transform(brain_test_seen_np)\n",
        "brain_unseen_scaled = clip_scaler.transform(brain_unseen.numpy())\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.FloatTensor(brain_train_seen_scaled)\n",
        "T_train_tensor = torch.FloatTensor(text_train_seen)\n",
        "Y_train_tensor = torch.LongTensor(label_train_seen)\n",
        "\n",
        "X_test_tensor = torch.FloatTensor(brain_test_seen_scaled)\n",
        "T_test_tensor = torch.FloatTensor(text_test_seen)\n",
        "Y_test_tensor = torch.LongTensor(label_test_seen)\n",
        "\n",
        "X_unseen_tensor = torch.FloatTensor(brain_unseen_scaled)\n",
        "T_unseen_tensor = torch.FloatTensor(text_unseen.numpy())\n",
        "Y_unseen_tensor = torch.LongTensor(label_unseen.numpy().flatten())\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_dataset = TensorDataset(X_train_tensor, T_train_tensor, Y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=CLIP_CONFIG['batch_size'], shuffle=True)\n",
        "\n",
        "print(f\"CLIP Training data: {len(X_train_tensor)} samples\")\n",
        "print(f\"CLIP Test data (seen): {len(X_test_tensor)} samples\")\n",
        "print(f\"Unseen data (test only): {len(X_unseen_tensor)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CLIP TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "# Optimizer\n",
        "clip_params = list(brain_encoder.parameters()) + list(text_projector.parameters())\n",
        "clip_optimizer = torch.optim.AdamW(\n",
        "    clip_params,\n",
        "    lr=CLIP_CONFIG['lr'],\n",
        "    weight_decay=CLIP_CONFIG['weight_decay']\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "clip_losses = []\n",
        "print(\"Training CLIP encoder...\")\n",
        "print(f\"Epochs: {CLIP_CONFIG['epochs']}, Batches per epoch: {len(train_loader)}\")\n",
        "\n",
        "for epoch in range(CLIP_CONFIG['epochs']):\n",
        "    brain_encoder.train()\n",
        "    text_projector.train()\n",
        "    epoch_loss = 0.0\n",
        "    \n",
        "    for batch_idx, (brain_batch, text_batch, _) in enumerate(train_loader):\n",
        "        brain_batch = brain_batch.to(device)\n",
        "        text_batch = text_batch.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        brain_embeds = brain_encoder(brain_batch)\n",
        "        text_embeds = text_projector(text_batch)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = clip_loss(brain_embeds, text_embeds, tau=CLIP_CONFIG['tau'])\n",
        "        \n",
        "        # Backward pass\n",
        "        clip_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    clip_losses.append(avg_loss)\n",
        "    \n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        print(f\"  Epoch {epoch+1:3d}/{CLIP_CONFIG['epochs']}: Loss = {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\nCLIP training complete!\")\n",
        "print(f\"Final loss: {clip_losses[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CLIP LOSS CURVE\n",
        "# =============================================================================\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(clip_losses)+1), clip_losses, 'b-', linewidth=2)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('CLIP Contrastive Loss', fontsize=12)\n",
        "plt.title('CLIP Encoder Training Loss', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/clip_loss_curve.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Saved: figures/clip_loss_curve.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPUTE AND CACHE CLIP EMBEDDINGS\n",
        "# =============================================================================\n",
        "\n",
        "brain_encoder.eval()\n",
        "text_projector.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Brain embeddings\n",
        "    E_train_seen = brain_encoder(X_train_tensor.to(device)).cpu().numpy()\n",
        "    E_test_seen = brain_encoder(X_test_tensor.to(device)).cpu().numpy()\n",
        "    E_unseen = brain_encoder(X_unseen_tensor.to(device)).cpu().numpy()\n",
        "    \n",
        "    # Text embeddings for prototypes\n",
        "    T_train_embeds = text_projector(T_train_tensor.to(device)).cpu().numpy()\n",
        "    T_test_embeds = text_projector(T_test_tensor.to(device)).cpu().numpy()\n",
        "    T_unseen_embeds = text_projector(T_unseen_tensor.to(device)).cpu().numpy()\n",
        "\n",
        "print(f\"E_train_seen shape: {E_train_seen.shape}\")\n",
        "print(f\"E_test_seen shape: {E_test_seen.shape}\")\n",
        "print(f\"E_unseen shape: {E_unseen.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPUTE SEMANTIC PROTOTYPES (CLASS CENTROIDS IN TEXT SPACE)\n",
        "# =============================================================================\n",
        "\n",
        "def compute_prototypes(embeddings, labels):\n",
        "    \"\"\"\n",
        "    Compute class prototypes as mean of embeddings per class, then L2-normalise.\n",
        "    \n",
        "    Returns:\n",
        "        prototypes: dict mapping class_id -> prototype vector\n",
        "    \"\"\"\n",
        "    unique_labels = np.unique(labels)\n",
        "    prototypes = {}\n",
        "    \n",
        "    for c in unique_labels:\n",
        "        mask = labels == c\n",
        "        class_embeds = embeddings[mask]\n",
        "        # Mean then normalise\n",
        "        proto = class_embeds.mean(axis=0)\n",
        "        proto = proto / (np.linalg.norm(proto) + 1e-8)\n",
        "        prototypes[c] = proto\n",
        "    \n",
        "    return prototypes\n",
        "\n",
        "# Compute prototypes for seen classes (from training text embeddings)\n",
        "S_seen_prototypes = compute_prototypes(T_train_embeds, label_train_seen)\n",
        "\n",
        "# Compute prototypes for unseen classes (from unseen text embeddings)\n",
        "S_unseen_prototypes = compute_prototypes(T_unseen_embeds, Y_unseen_tensor.numpy())\n",
        "\n",
        "print(f\"Seen class prototypes: {len(S_seen_prototypes)} classes\")\n",
        "print(f\"Unseen class prototypes: {len(S_unseen_prototypes)} classes\")\n",
        "\n",
        "# Convert to arrays for easier use\n",
        "seen_classes = sorted(S_seen_prototypes.keys())\n",
        "unseen_classes = sorted(S_unseen_prototypes.keys())\n",
        "\n",
        "S_seen_array = np.array([S_seen_prototypes[c] for c in seen_classes])  # (n_seen_classes, d)\n",
        "S_unseen_array = np.array([S_unseen_prototypes[c] for c in unseen_classes])  # (n_unseen_classes, d)\n",
        "\n",
        "print(f\"S_seen_array shape: {S_seen_array.shape}\")\n",
        "print(f\"S_unseen_array shape: {S_unseen_array.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CACHE EMBEDDINGS AND PROTOTYPES TO DISK\n",
        "# =============================================================================\n",
        "\n",
        "os.makedirs('cached_arrays', exist_ok=True)\n",
        "\n",
        "# Save embeddings\n",
        "np.save('cached_arrays/E_train_seen.npy', E_train_seen)\n",
        "np.save('cached_arrays/E_test_seen.npy', E_test_seen)\n",
        "np.save('cached_arrays/E_unseen.npy', E_unseen)\n",
        "\n",
        "# Save labels\n",
        "np.save('cached_arrays/y_train_seen.npy', label_train_seen)\n",
        "np.save('cached_arrays/y_test_seen.npy', label_test_seen)\n",
        "np.save('cached_arrays/y_unseen.npy', Y_unseen_tensor.numpy())\n",
        "\n",
        "# Save prototypes\n",
        "np.save('cached_arrays/S_seen_prototypes.npy', S_seen_array)\n",
        "np.save('cached_arrays/S_unseen_prototypes.npy', S_unseen_array)\n",
        "np.save('cached_arrays/seen_classes.npy', np.array(seen_classes))\n",
        "np.save('cached_arrays/unseen_classes.npy', np.array(unseen_classes))\n",
        "\n",
        "print(\"Cached arrays saved to cached_arrays/:\")\n",
        "for f in os.listdir('cached_arrays'):\n",
        "    print(f\"  - {f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# t-SNE VISUALIZATION OF CLIP EMBEDDING SPACE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Computing t-SNE (this may take a moment)...\")\n",
        "\n",
        "# Sample subset of test embeddings for visualization\n",
        "n_sample = min(1000, len(E_test_seen))\n",
        "sample_idx = np.random.choice(len(E_test_seen), n_sample, replace=False)\n",
        "E_test_sample = E_test_seen[sample_idx]\n",
        "y_test_sample = label_test_seen[sample_idx]\n",
        "\n",
        "# Combine brain embeddings and prototypes\n",
        "combined = np.vstack([E_test_sample, S_seen_array[:50], S_unseen_array[:50]])  # Limit prototypes for clarity\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "combined_2d = tsne.fit_transform(combined)\n",
        "\n",
        "# Split back\n",
        "n_brain = len(E_test_sample)\n",
        "n_seen_proto = 50\n",
        "n_unseen_proto = 50\n",
        "\n",
        "brain_2d = combined_2d[:n_brain]\n",
        "seen_proto_2d = combined_2d[n_brain:n_brain+n_seen_proto]\n",
        "unseen_proto_2d = combined_2d[n_brain+n_seen_proto:]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(brain_2d[:, 0], brain_2d[:, 1], c='steelblue', alpha=0.3, s=10, label='EEG embeddings (seen test)')\n",
        "plt.scatter(seen_proto_2d[:, 0], seen_proto_2d[:, 1], c='green', marker='*', s=100, label='Seen class prototypes')\n",
        "plt.scatter(unseen_proto_2d[:, 0], unseen_proto_2d[:, 1], c='red', marker='^', s=100, label='Unseen class prototypes')\n",
        "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
        "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
        "plt.title('CLIP Embedding Space: EEG Embeddings and Text Prototypes', fontsize=14)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/clip_embedding_tsne.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Saved: figures/clip_embedding_tsne.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Conditional WGAN-GP for Embedding Synthesis\n",
        "\n",
        "This section implements a **conditional Wasserstein GAN with Gradient Penalty (cWGAN-GP)** to synthesize EEG embeddings for unseen classes.\n",
        "\n",
        "**Goal:** Generate synthetic EEG embeddings `e_hat = G(z, s_c)` conditioned on text prototypes `s_c`.\n",
        "\n",
        "**Architecture:**\n",
        "- Generator: [z, s_c] \u2192 e_hat (L2-normalised output)\n",
        "- Critic: [e, s_c] \u2192 scalar (no sigmoid)\n",
        "\n",
        "**Training:** Only on seen-class embeddings from CLIP encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# cWGAN-GP CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "WGAN_CONFIG = {\n",
        "    'z_dim': 100,              # Noise dimension\n",
        "    'embed_dim': 64,           # Must match CLIP embed_dim\n",
        "    'lr': 1e-4,                # Learning rate\n",
        "    'betas': (0.0, 0.9),       # Adam betas for WGAN-GP\n",
        "    'lambda_gp': 10,           # Gradient penalty coefficient\n",
        "    'n_critic': 5,             # Critic updates per generator update\n",
        "    'n_steps': 10000,          # Total generator steps\n",
        "    'batch_size': 256,         # Batch size\n",
        "    'n_synth_per_class': 20,   # Synthetic samples per unseen class\n",
        "    'seed': 42,\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"cWGAN-GP CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "for k, v in WGAN_CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# cWGAN-GP MODEL DEFINITIONS\n",
        "# =============================================================================\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"Conditional generator: [z, s_c] -> e_hat.\"\"\"\n",
        "    \n",
        "    def __init__(self, z_dim=100, embed_dim=64):\n",
        "        super().__init__()\n",
        "        input_dim = z_dim + embed_dim  # Concatenate noise and condition\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, embed_dim),\n",
        "        )\n",
        "        \n",
        "    def forward(self, z, s_c):\n",
        "        x = torch.cat([z, s_c], dim=-1)\n",
        "        e_hat = self.net(x)\n",
        "        # L2 normalise output to stay on unit sphere\n",
        "        e_hat = F.normalize(e_hat, p=2, dim=-1)\n",
        "        return e_hat\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"Conditional critic: [e, s_c] -> scalar score.\"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim=64):\n",
        "        super().__init__()\n",
        "        input_dim = 2 * embed_dim  # Concatenate embedding and condition\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),  # Scalar output, NO sigmoid\n",
        "        )\n",
        "        \n",
        "    def forward(self, e, s_c):\n",
        "        x = torch.cat([e, s_c], dim=-1)\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def compute_gradient_penalty(critic, real_samples, fake_samples, s_c, device):\n",
        "    \"\"\"\n",
        "    Compute gradient penalty for WGAN-GP.\n",
        "    \n",
        "    GP = E[(||\u2207_{e_bar} D(e_bar, s_c)||_2 - 1)^2]\n",
        "    \"\"\"\n",
        "    batch_size = real_samples.size(0)\n",
        "    \n",
        "    # Random interpolation coefficient\n",
        "    eps = torch.rand(batch_size, 1, device=device)\n",
        "    \n",
        "    # Interpolated samples\n",
        "    e_bar = eps * real_samples + (1 - eps) * fake_samples\n",
        "    e_bar.requires_grad_(True)\n",
        "    \n",
        "    # Critic score on interpolated\n",
        "    d_interpolated = critic(e_bar, s_c)\n",
        "    \n",
        "    # Compute gradients\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolated,\n",
        "        inputs=e_bar,\n",
        "        grad_outputs=torch.ones_like(d_interpolated),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    \n",
        "    # Compute penalty\n",
        "    gradients = gradients.view(batch_size, -1)\n",
        "    gradient_norm = gradients.norm(2, dim=1)\n",
        "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
        "    \n",
        "    return gradient_penalty\n",
        "\n",
        "\n",
        "# Instantiate models\n",
        "generator = Generator(\n",
        "    z_dim=WGAN_CONFIG['z_dim'],\n",
        "    embed_dim=WGAN_CONFIG['embed_dim']\n",
        ").to(device)\n",
        "\n",
        "critic = Critic(\n",
        "    embed_dim=WGAN_CONFIG['embed_dim']\n",
        ").to(device)\n",
        "\n",
        "print(f\"Generator parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
        "print(f\"Critic parameters: {sum(p.numel() for p in critic.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# cWGAN-GP DATA PREPARATION\n",
        "# =============================================================================\n",
        "# Use only seen embeddings from CLIP encoder\n",
        "\n",
        "# Get semantic prototype for each training sample\n",
        "def get_prototype_for_labels(labels, prototypes_dict):\n",
        "    \"\"\"Get prototype vector for each sample based on its label.\"\"\"\n",
        "    protos = np.array([prototypes_dict[int(l)] for l in labels])\n",
        "    return protos\n",
        "\n",
        "# Training pairs: (e_real, s_c) where s_c is the class prototype\n",
        "E_train_tensor = torch.FloatTensor(E_train_seen)\n",
        "S_train_conditions = torch.FloatTensor(get_prototype_for_labels(label_train_seen, S_seen_prototypes))\n",
        "\n",
        "wgan_dataset = TensorDataset(E_train_tensor, S_train_conditions)\n",
        "wgan_loader = DataLoader(wgan_dataset, batch_size=WGAN_CONFIG['batch_size'], shuffle=True, drop_last=True)\n",
        "\n",
        "print(f\"WGAN training data: {len(wgan_dataset)} samples\")\n",
        "print(f\"Batch size: {WGAN_CONFIG['batch_size']}\")\n",
        "print(f\"Batches per epoch: {len(wgan_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# cWGAN-GP TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "# Optimizers\n",
        "g_optimizer = torch.optim.Adam(\n",
        "    generator.parameters(),\n",
        "    lr=WGAN_CONFIG['lr'],\n",
        "    betas=WGAN_CONFIG['betas']\n",
        ")\n",
        "c_optimizer = torch.optim.Adam(\n",
        "    critic.parameters(),\n",
        "    lr=WGAN_CONFIG['lr'],\n",
        "    betas=WGAN_CONFIG['betas']\n",
        ")\n",
        "\n",
        "# Training history\n",
        "g_losses = []\n",
        "c_losses = []\n",
        "gp_values = []\n",
        "\n",
        "print(f\"Training cWGAN-GP for {WGAN_CONFIG['n_steps']} generator steps...\")\n",
        "print(f\"Critic updates per G step: {WGAN_CONFIG['n_critic']}\")\n",
        "\n",
        "data_iter = iter(wgan_loader)\n",
        "g_step = 0\n",
        "\n",
        "while g_step < WGAN_CONFIG['n_steps']:\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Train Critic (n_critic times per generator step)\n",
        "    # -------------------------------------------------------------------------\n",
        "    for _ in range(WGAN_CONFIG['n_critic']):\n",
        "        try:\n",
        "            e_real, s_c = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(wgan_loader)\n",
        "            e_real, s_c = next(data_iter)\n",
        "        \n",
        "        e_real = e_real.to(device)\n",
        "        s_c = s_c.to(device)\n",
        "        batch_size = e_real.size(0)\n",
        "        \n",
        "        # Generate fake samples\n",
        "        z = torch.randn(batch_size, WGAN_CONFIG['z_dim'], device=device)\n",
        "        e_fake = generator(z, s_c)\n",
        "        \n",
        "        # Critic scores\n",
        "        d_real = critic(e_real, s_c)\n",
        "        d_fake = critic(e_fake.detach(), s_c)\n",
        "        \n",
        "        # Gradient penalty\n",
        "        gp = compute_gradient_penalty(critic, e_real, e_fake.detach(), s_c, device)\n",
        "        \n",
        "        # Critic loss: maximize E[D(real)] - E[D(fake)] - lambda*GP\n",
        "        # Minimizing: -E[D(real)] + E[D(fake)] + lambda*GP\n",
        "        c_loss = -d_real.mean() + d_fake.mean() + WGAN_CONFIG['lambda_gp'] * gp\n",
        "        \n",
        "        c_optimizer.zero_grad()\n",
        "        c_loss.backward()\n",
        "        c_optimizer.step()\n",
        "    \n",
        "    # -------------------------------------------------------------------------\n",
        "    # Train Generator\n",
        "    # -------------------------------------------------------------------------\n",
        "    try:\n",
        "        e_real, s_c = next(data_iter)\n",
        "    except StopIteration:\n",
        "        data_iter = iter(wgan_loader)\n",
        "        e_real, s_c = next(data_iter)\n",
        "    \n",
        "    s_c = s_c.to(device)\n",
        "    batch_size = s_c.size(0)\n",
        "    \n",
        "    z = torch.randn(batch_size, WGAN_CONFIG['z_dim'], device=device)\n",
        "    e_fake = generator(z, s_c)\n",
        "    d_fake = critic(e_fake, s_c)\n",
        "    \n",
        "    # Generator loss: minimize -E[D(fake)]\n",
        "    g_loss = -d_fake.mean()\n",
        "    \n",
        "    g_optimizer.zero_grad()\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "    \n",
        "    # Log\n",
        "    g_losses.append(g_loss.item())\n",
        "    c_losses.append(c_loss.item())\n",
        "    gp_values.append(gp.item())\n",
        "    \n",
        "    g_step += 1\n",
        "    \n",
        "    if g_step % 1000 == 0 or g_step == 1:\n",
        "        print(f\"  Step {g_step:5d}/{WGAN_CONFIG['n_steps']}: G_loss={g_loss.item():.4f}, C_loss={c_loss.item():.4f}, GP={gp.item():.4f}\")\n",
        "\n",
        "print(\"\\ncWGAN-GP training complete!\")\n",
        "print(f\"Final G_loss: {g_losses[-1]:.4f}\")\n",
        "print(f\"Final C_loss: {c_losses[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# cWGAN-GP LOSS CURVES\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Generator loss\n",
        "axes[0].plot(g_losses, alpha=0.7)\n",
        "axes[0].set_xlabel('Generator Step')\n",
        "axes[0].set_ylabel('Generator Loss')\n",
        "axes[0].set_title('Generator Loss')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Critic loss\n",
        "axes[1].plot(c_losses, alpha=0.7, color='orange')\n",
        "axes[1].set_xlabel('Generator Step')\n",
        "axes[1].set_ylabel('Critic Loss')\n",
        "axes[1].set_title('Critic Loss')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Gradient penalty\n",
        "axes[2].plot(gp_values, alpha=0.7, color='green')\n",
        "axes[2].set_xlabel('Generator Step')\n",
        "axes[2].set_ylabel('Gradient Penalty')\n",
        "axes[2].set_title('Gradient Penalty')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/wgan_losses.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Saved: figures/wgan_losses.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# GENERATE SYNTHETIC EMBEDDINGS FOR UNSEEN CLASSES\n",
        "# =============================================================================\n",
        "\n",
        "generator.eval()\n",
        "\n",
        "n_synth = WGAN_CONFIG['n_synth_per_class']\n",
        "synth_embeddings = []\n",
        "synth_labels = []\n",
        "\n",
        "print(f\"Generating {n_synth} synthetic embeddings per unseen class...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for c in unseen_classes:\n",
        "        # Get prototype for this unseen class\n",
        "        s_c = torch.FloatTensor(S_unseen_prototypes[c]).unsqueeze(0).repeat(n_synth, 1).to(device)\n",
        "        \n",
        "        # Sample noise and generate\n",
        "        z = torch.randn(n_synth, WGAN_CONFIG['z_dim'], device=device)\n",
        "        e_synth = generator(z, s_c).cpu().numpy()\n",
        "        \n",
        "        synth_embeddings.append(e_synth)\n",
        "        synth_labels.extend([c] * n_synth)\n",
        "\n",
        "E_synth_unseen = np.vstack(synth_embeddings)\n",
        "y_synth_unseen = np.array(synth_labels)\n",
        "\n",
        "print(f\"E_synth_unseen shape: {E_synth_unseen.shape}\")\n",
        "print(f\"y_synth_unseen shape: {y_synth_unseen.shape}\")\n",
        "print(f\"Total synthetic samples: {len(y_synth_unseen)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SANITY CHECK: SYNTHETIC EMBEDDING QUALITY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SYNTHETIC EMBEDDING QUALITY CHECK\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check that synthetic embeddings are normalised\n",
        "synth_norms = np.linalg.norm(E_synth_unseen, axis=1)\n",
        "print(f\"Synthetic embedding norms: mean={synth_norms.mean():.4f}, std={synth_norms.std():.6f}\")\n",
        "\n",
        "# Check variance (not collapsed)\n",
        "synth_variance = E_synth_unseen.var(axis=0).mean()\n",
        "real_variance = E_train_seen.var(axis=0).mean()\n",
        "print(f\"Mean per-dim variance: Real={real_variance:.4f}, Synthetic={synth_variance:.4f}\")\n",
        "\n",
        "# Check cosine similarity with prototypes\n",
        "cos_sims = []\n",
        "for i, c in enumerate(unseen_classes[:10]):  # Sample 10 classes\n",
        "    mask = y_synth_unseen == c\n",
        "    synth_c = E_synth_unseen[mask]\n",
        "    proto_c = S_unseen_prototypes[c]\n",
        "    sims = np.dot(synth_c, proto_c)  # cosine sim since both normalised\n",
        "    cos_sims.extend(sims)\n",
        "\n",
        "print(f\"Cosine sim (synth vs proto): mean={np.mean(cos_sims):.4f}, std={np.std(cos_sims):.4f}\")\n",
        "\n",
        "if synth_variance > 0.001:\n",
        "    print(\"\\n\u2713 Synthetic embeddings have reasonable variance (no mode collapse).\")\n",
        "else:\n",
        "    print(\"\\n\u26a0 Warning: Low variance in synthetic embeddings.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CACHE SYNTHETIC EMBEDDINGS\n",
        "# =============================================================================\n",
        "\n",
        "np.save('cached_arrays/E_synth_unseen.npy', E_synth_unseen)\n",
        "np.save('cached_arrays/y_synth_unseen.npy', y_synth_unseen)\n",
        "\n",
        "print(\"Saved synthetic embeddings to cached_arrays/:\")\n",
        "print(f\"  - E_synth_unseen.npy: {E_synth_unseen.shape}\")\n",
        "print(f\"  - y_synth_unseen.npy: {y_synth_unseen.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# t-SNE: REAL SEEN vs SYNTHETIC UNSEEN EMBEDDINGS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Computing t-SNE for real vs synthetic embeddings...\")\n",
        "\n",
        "# Sample subsets for visualization\n",
        "n_real_sample = min(1000, len(E_train_seen))\n",
        "n_synth_sample = min(1000, len(E_synth_unseen))\n",
        "\n",
        "real_idx = np.random.choice(len(E_train_seen), n_real_sample, replace=False)\n",
        "synth_idx = np.random.choice(len(E_synth_unseen), n_synth_sample, replace=False)\n",
        "\n",
        "E_real_sample = E_train_seen[real_idx]\n",
        "E_synth_sample = E_synth_unseen[synth_idx]\n",
        "\n",
        "# Combine for t-SNE\n",
        "combined = np.vstack([E_real_sample, E_synth_sample, S_seen_array[:30], S_unseen_array[:30]])\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "combined_2d = tsne.fit_transform(combined)\n",
        "\n",
        "# Split back\n",
        "n1, n2, n3, n4 = n_real_sample, n_synth_sample, 30, 30\n",
        "real_2d = combined_2d[:n1]\n",
        "synth_2d = combined_2d[n1:n1+n2]\n",
        "seen_proto_2d = combined_2d[n1+n2:n1+n2+n3]\n",
        "unseen_proto_2d = combined_2d[n1+n2+n3:]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(real_2d[:, 0], real_2d[:, 1], c='steelblue', alpha=0.3, s=15, label='Real seen embeddings')\n",
        "plt.scatter(synth_2d[:, 0], synth_2d[:, 1], c='darkorange', alpha=0.3, s=15, label='Synthetic unseen embeddings')\n",
        "plt.scatter(seen_proto_2d[:, 0], seen_proto_2d[:, 1], c='green', marker='*', s=150, label='Seen prototypes')\n",
        "plt.scatter(unseen_proto_2d[:, 0], unseen_proto_2d[:, 1], c='red', marker='^', s=150, label='Unseen prototypes')\n",
        "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
        "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
        "plt.title('Real Seen vs Synthetic Unseen Embeddings in CLIP Space', fontsize=14)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/real_vs_synth_tsne.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Saved: figures/real_vs_synth_tsne.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CLIP + cWGAN-GP IMPLEMENTATION SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CLIP + cWGAN-GP IMPLEMENTATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n### CLIP ENCODER ###\")\n",
        "print(f\"  Embedding dimension: {CLIP_CONFIG['embed_dim']}\")\n",
        "print(f\"  Training epochs: {CLIP_CONFIG['epochs']}\")\n",
        "print(f\"  Final contrastive loss: {clip_losses[-1]:.4f}\")\n",
        "print(f\"  Embeddings: E_train_seen {E_train_seen.shape}, E_test_seen {E_test_seen.shape}, E_unseen {E_unseen.shape}\")\n",
        "print(f\"  Prototypes: S_seen {S_seen_array.shape}, S_unseen {S_unseen_array.shape}\")\n",
        "\n",
        "print(\"\\n### cWGAN-GP ###\")\n",
        "print(f\"  Generator steps: {WGAN_CONFIG['n_steps']}\")\n",
        "print(f\"  Final G_loss: {g_losses[-1]:.4f}\")\n",
        "print(f\"  Final C_loss: {c_losses[-1]:.4f}\")\n",
        "print(f\"  Synthetic per class: {WGAN_CONFIG['n_synth_per_class']}\")\n",
        "print(f\"  Total synthetic samples: {len(y_synth_unseen)}\")\n",
        "\n",
        "print(\"\\n### CACHED ARRAYS ###\")\n",
        "for f in sorted(os.listdir('cached_arrays')):\n",
        "    print(f\"  - cached_arrays/{f}\")\n",
        "\n",
        "print(\"\\n### FIGURES GENERATED ###\")\n",
        "print(\"  - figures/clip_loss_curve.png\")\n",
        "print(\"  - figures/clip_embedding_tsne.png\")\n",
        "print(\"  - figures/wgan_losses.png\")\n",
        "print(\"  - figures/real_vs_synth_tsne.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"NEXT STEPS: Train GZSL Classifier [A+B] on real + synthetic embeddings\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Quantitative Diagnostics: CLIP + cWGAN-GP\n",
        "\n",
        "Before training the GZSL classifier, we run two diagnostic checks:\n",
        "\n",
        "1. **Prototype Alignment**: Verify embeddings are closer to their own class prototype than to others\n",
        "2. **Class-Conditional Diversity**: Check synthetic embeddings haven't collapsed (mode collapse)\n",
        "\n",
        "These diagnostics use **only cached arrays** \u2014 no retraining of CLIP or WGAN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LOAD CACHED ARRAYS FOR DIAGNOSTICS\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Load embeddings\n",
        "E_train_seen = np.load('cached_arrays/E_train_seen.npy')\n",
        "E_test_seen = np.load('cached_arrays/E_test_seen.npy')\n",
        "E_unseen = np.load('cached_arrays/E_unseen.npy')\n",
        "E_synth_unseen = np.load('cached_arrays/E_synth_unseen.npy')\n",
        "\n",
        "# Load labels\n",
        "y_train_seen = np.load('cached_arrays/y_train_seen.npy')\n",
        "y_test_seen = np.load('cached_arrays/y_test_seen.npy')\n",
        "y_unseen = np.load('cached_arrays/y_unseen.npy')\n",
        "y_synth_unseen = np.load('cached_arrays/y_synth_unseen.npy')\n",
        "\n",
        "# Load prototypes\n",
        "S_seen_array = np.load('cached_arrays/S_seen_prototypes.npy')\n",
        "S_unseen_array = np.load('cached_arrays/S_unseen_prototypes.npy')\n",
        "seen_classes = np.load('cached_arrays/seen_classes.npy')\n",
        "unseen_classes = np.load('cached_arrays/unseen_classes.npy')\n",
        "\n",
        "# Build label-to-index mappings (labels are 1-indexed, arrays are 0-indexed)\n",
        "seen_label_to_idx = {c: i for i, c in enumerate(seen_classes)}\n",
        "unseen_label_to_idx = {c: i for i, c in enumerate(unseen_classes)}\n",
        "\n",
        "print(\"Loaded arrays:\")\n",
        "print(f\"  E_test_seen: {E_test_seen.shape}\")\n",
        "print(f\"  E_synth_unseen: {E_synth_unseen.shape}\")\n",
        "print(f\"  S_seen_array: {S_seen_array.shape} ({len(seen_classes)} classes)\")\n",
        "print(f\"  S_unseen_array: {S_unseen_array.shape} ({len(unseen_classes)} classes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check 1: Prototype Alignment\n",
        "\n",
        "Verify that embeddings are **closer to their own class prototype** than to random other prototypes.\n",
        "\n",
        "Metrics:\n",
        "- **sim_pos**: cosine similarity between embedding and its own class prototype\n",
        "- **sim_neg**: cosine similarity between embedding and random other prototypes\n",
        "- **margin**: sim_pos - sim_neg (should be positive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CHECK 1A: PROTOTYPE ALIGNMENT \u2014 SEEN (REAL) EMBEDDINGS\n",
        "# =============================================================================\n",
        "\n",
        "np.random.seed(42)\n",
        "n_neg_samples = 10  # Number of negative prototypes to sample per embedding\n",
        "\n",
        "sim_pos_seen = []\n",
        "sim_neg_seen = []\n",
        "\n",
        "for i in range(len(E_test_seen)):\n",
        "    e = E_test_seen[i]\n",
        "    label = y_test_seen[i]\n",
        "    \n",
        "    # Get the correct prototype index\n",
        "    if label not in seen_label_to_idx:\n",
        "        continue\n",
        "    pos_idx = seen_label_to_idx[label]\n",
        "    \n",
        "    # Positive similarity (dot product = cosine since L2-normalised)\n",
        "    s_pos = np.dot(e, S_seen_array[pos_idx])\n",
        "    sim_pos_seen.append(s_pos)\n",
        "    \n",
        "    # Negative similarities (sample random other classes)\n",
        "    other_indices = [j for j in range(len(seen_classes)) if j != pos_idx]\n",
        "    neg_indices = np.random.choice(other_indices, min(n_neg_samples, len(other_indices)), replace=False)\n",
        "    for neg_idx in neg_indices:\n",
        "        s_neg = np.dot(e, S_seen_array[neg_idx])\n",
        "        sim_neg_seen.append(s_neg)\n",
        "\n",
        "sim_pos_seen = np.array(sim_pos_seen)\n",
        "sim_neg_seen = np.array(sim_neg_seen)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHECK 1A: PROTOTYPE ALIGNMENT \u2014 SEEN (REAL)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Positive similarities (e vs own proto):\")\n",
        "print(f\"  Mean: {sim_pos_seen.mean():.4f} \u00b1 {sim_pos_seen.std():.4f}\")\n",
        "print(f\"Negative similarities (e vs random other proto):\")\n",
        "print(f\"  Mean: {sim_neg_seen.mean():.4f} \u00b1 {sim_neg_seen.std():.4f}\")\n",
        "print(f\"Mean Margin (pos - neg): {sim_pos_seen.mean() - sim_neg_seen.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PLOT: SEEN PROTOTYPE ALIGNMENT HISTOGRAM\n",
        "# =============================================================================\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(sim_pos_seen, bins=50, alpha=0.7, label=f'Positive (own proto), \u03bc={sim_pos_seen.mean():.3f}', color='green', density=True)\n",
        "plt.hist(sim_neg_seen, bins=50, alpha=0.7, label=f'Negative (other proto), \u03bc={sim_neg_seen.mean():.3f}', color='red', density=True)\n",
        "plt.xlabel('Cosine Similarity', fontsize=12)\n",
        "plt.ylabel('Density', fontsize=12)\n",
        "plt.title('Check 1A: Seen Embeddings \u2014 Prototype Alignment', fontsize=14)\n",
        "plt.legend()\n",
        "plt.axvline(sim_pos_seen.mean(), color='darkgreen', linestyle='--', linewidth=2)\n",
        "plt.axvline(sim_neg_seen.mean(), color='darkred', linestyle='--', linewidth=2)\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/prototype_alignment_seen.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Saved: figures/prototype_alignment_seen.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CHECK 1B: PROTOTYPE ALIGNMENT \u2014 UNSEEN (SYNTHETIC) EMBEDDINGS\n",
        "# =============================================================================\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "sim_pos_synth = []\n",
        "sim_neg_synth = []\n",
        "\n",
        "for i in range(len(E_synth_unseen)):\n",
        "    e_hat = E_synth_unseen[i]\n",
        "    label = y_synth_unseen[i]\n",
        "    \n",
        "    if label not in unseen_label_to_idx:\n",
        "        continue\n",
        "    pos_idx = unseen_label_to_idx[label]\n",
        "    \n",
        "    # Positive similarity\n",
        "    s_pos = np.dot(e_hat, S_unseen_array[pos_idx])\n",
        "    sim_pos_synth.append(s_pos)\n",
        "    \n",
        "    # Negative similarities\n",
        "    other_indices = [j for j in range(len(unseen_classes)) if j != pos_idx]\n",
        "    neg_indices = np.random.choice(other_indices, min(n_neg_samples, len(other_indices)), replace=False)\n",
        "    for neg_idx in neg_indices:\n",
        "        s_neg = np.dot(e_hat, S_unseen_array[neg_idx])\n",
        "        sim_neg_synth.append(s_neg)\n",
        "\n",
        "sim_pos_synth = np.array(sim_pos_synth)\n",
        "sim_neg_synth = np.array(sim_neg_synth)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHECK 1B: PROTOTYPE ALIGNMENT \u2014 UNSEEN (SYNTHETIC)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Positive similarities (e_hat vs own proto):\")\n",
        "print(f\"  Mean: {sim_pos_synth.mean():.4f} \u00b1 {sim_pos_synth.std():.4f}\")\n",
        "print(f\"Negative similarities (e_hat vs random other proto):\")\n",
        "print(f\"  Mean: {sim_neg_synth.mean():.4f} \u00b1 {sim_neg_synth.std():.4f}\")\n",
        "print(f\"Mean Margin (pos - neg): {sim_pos_synth.mean() - sim_neg_synth.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PLOT: SYNTHETIC UNSEEN PROTOTYPE ALIGNMENT HISTOGRAM\n",
        "# =============================================================================\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(sim_pos_synth, bins=50, alpha=0.7, label=f'Positive (own proto), \u03bc={sim_pos_synth.mean():.3f}', color='green', density=True)\n",
        "plt.hist(sim_neg_synth, bins=50, alpha=0.7, label=f'Negative (other proto), \u03bc={sim_neg_synth.mean():.3f}', color='red', density=True)\n",
        "plt.xlabel('Cosine Similarity', fontsize=12)\n",
        "plt.ylabel('Density', fontsize=12)\n",
        "plt.title('Check 1B: Synthetic Unseen Embeddings \u2014 Prototype Alignment', fontsize=14)\n",
        "plt.legend()\n",
        "plt.axvline(sim_pos_synth.mean(), color='darkgreen', linestyle='--', linewidth=2)\n",
        "plt.axvline(sim_neg_synth.mean(), color='darkred', linestyle='--', linewidth=2)\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/prototype_alignment_unseen_synth.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Saved: figures/prototype_alignment_unseen_synth.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CHECK 1C: TOP-1 PROTOTYPE RETRIEVAL ACCURACY\n",
        "# =============================================================================\n",
        "\n",
        "def top1_prototype_accuracy(embeddings, labels, prototypes, label_to_idx, class_list):\n",
        "    \"\"\"Compute top-1 accuracy by predicting class = argmax cosine(e, proto).\"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Compute all similarities at once\n",
        "    similarities = np.dot(embeddings, prototypes.T)  # (N, n_classes)\n",
        "    pred_indices = np.argmax(similarities, axis=1)  # (N,)\n",
        "    pred_labels = class_list[pred_indices]\n",
        "    \n",
        "    accuracy = (pred_labels == labels).mean()\n",
        "    return accuracy\n",
        "\n",
        "# Seen embeddings retrieval accuracy\n",
        "acc_seen_retrieval = top1_prototype_accuracy(\n",
        "    E_test_seen, y_test_seen, S_seen_array, seen_label_to_idx, seen_classes\n",
        ")\n",
        "\n",
        "# Synthetic unseen retrieval accuracy\n",
        "acc_synth_retrieval = top1_prototype_accuracy(\n",
        "    E_synth_unseen, y_synth_unseen, S_unseen_array, unseen_label_to_idx, unseen_classes\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHECK 1C: TOP-1 PROTOTYPE RETRIEVAL ACCURACY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Seen (real) test embeddings:     {acc_seen_retrieval:.4f} ({acc_seen_retrieval*100:.2f}%)\")\n",
        "print(f\"Unseen (synthetic) embeddings:   {acc_synth_retrieval:.4f} ({acc_synth_retrieval*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check 2: Class-Conditional Diversity\n",
        "\n",
        "Check that synthetic embeddings have **sufficient diversity within each class** (no mode collapse).\n",
        "\n",
        "For each unseen class:\n",
        "- Average pairwise cosine similarity within the class\n",
        "- Average pairwise cosine distance = 1 - similarity\n",
        "- Per-dimension variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CHECK 2: CLASS-CONDITIONAL DIVERSITY (MODE COLLAPSE CHECK)\n",
        "# =============================================================================\n",
        "\n",
        "class_diversity_stats = []\n",
        "\n",
        "for c in unseen_classes:\n",
        "    mask = y_synth_unseen == c\n",
        "    E_class = E_synth_unseen[mask]\n",
        "    \n",
        "    if len(E_class) < 2:\n",
        "        continue\n",
        "    \n",
        "    # Pairwise cosine similarities (upper triangle only)\n",
        "    sim_matrix = np.dot(E_class, E_class.T)\n",
        "    n = len(E_class)\n",
        "    upper_tri_idx = np.triu_indices(n, k=1)\n",
        "    pairwise_sims = sim_matrix[upper_tri_idx]\n",
        "    \n",
        "    avg_sim = pairwise_sims.mean()\n",
        "    avg_dist = 1 - avg_sim\n",
        "    \n",
        "    # Per-dimension variance\n",
        "    per_dim_var = E_class.var(axis=0).mean()\n",
        "    \n",
        "    class_diversity_stats.append({\n",
        "        'class': c,\n",
        "        'avg_sim': avg_sim,\n",
        "        'avg_dist': avg_dist,\n",
        "        'per_dim_var': per_dim_var,\n",
        "        'n_samples': len(E_class)\n",
        "    })\n",
        "\n",
        "# Convert to arrays for analysis\n",
        "avg_sims = np.array([s['avg_sim'] for s in class_diversity_stats])\n",
        "avg_dists = np.array([s['avg_dist'] for s in class_diversity_stats])\n",
        "per_dim_vars = np.array([s['per_dim_var'] for s in class_diversity_stats])\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHECK 2: CLASS-CONDITIONAL DIVERSITY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Across {len(class_diversity_stats)} unseen classes:\")\n",
        "print(f\"  Avg within-class cosine similarity: {avg_sims.mean():.4f} \u00b1 {avg_sims.std():.4f}\")\n",
        "print(f\"  Avg within-class cosine distance:   {avg_dists.mean():.4f} \u00b1 {avg_dists.std():.4f}\")\n",
        "print(f\"  Avg per-dimension variance:         {per_dim_vars.mean():.6f} \u00b1 {per_dim_vars.std():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IDENTIFY LOW AND HIGH DIVERSITY CLASSES\n",
        "# =============================================================================\n",
        "\n",
        "# Sort by diversity (distance) \u2014 low distance = low diversity = potential collapse\n",
        "sorted_stats = sorted(class_diversity_stats, key=lambda x: x['avg_dist'])\n",
        "\n",
        "print(\"\\n5 LOWEST DIVERSITY CLASSES (potential mode collapse):\")\n",
        "print(\"-\" * 50)\n",
        "for s in sorted_stats[:5]:\n",
        "    print(f\"  Class {s['class']:4d}: avg_dist={s['avg_dist']:.4f}, per_dim_var={s['per_dim_var']:.6f}\")\n",
        "\n",
        "print(\"\\n5 HIGHEST DIVERSITY CLASSES:\")\n",
        "print(\"-\" * 50)\n",
        "for s in sorted_stats[-5:]:\n",
        "    print(f\"  Class {s['class']:4d}: avg_dist={s['avg_dist']:.4f}, per_dim_var={s['per_dim_var']:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PLOT: DIVERSITY DISTRIBUTION ACROSS CLASSES\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram of average cosine distance\n",
        "axes[0].hist(avg_dists, bins=30, color='teal', edgecolor='black', alpha=0.7)\n",
        "axes[0].axvline(avg_dists.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_dists.mean():.4f}')\n",
        "axes[0].set_xlabel('Avg Within-Class Cosine Distance', fontsize=12)\n",
        "axes[0].set_ylabel('Number of Classes', fontsize=12)\n",
        "axes[0].set_title('Synthetic Embedding Diversity by Class', fontsize=14)\n",
        "axes[0].legend()\n",
        "\n",
        "# Sorted bar plot\n",
        "sorted_dists = np.sort(avg_dists)\n",
        "axes[1].bar(range(len(sorted_dists)), sorted_dists, color='teal', alpha=0.7)\n",
        "axes[1].axhline(avg_dists.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_dists.mean():.4f}')\n",
        "axes[1].set_xlabel('Class (sorted by diversity)', fontsize=12)\n",
        "axes[1].set_ylabel('Avg Within-Class Cosine Distance', fontsize=12)\n",
        "axes[1].set_title('Per-Class Diversity (Sorted)', fontsize=14)\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/synth_diversity_by_class.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Saved: figures/synth_diversity_by_class.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DIAGNOSTICS SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DIAGNOSTICS SUMMARY: CLIP + cWGAN-GP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n### CHECK 1: PROTOTYPE ALIGNMENT ###\")\n",
        "print(\"\")\n",
        "print(\"  (A) Seen (real) embeddings:\")\n",
        "print(f\"      Positive sim: {sim_pos_seen.mean():.4f} \u00b1 {sim_pos_seen.std():.4f}\")\n",
        "print(f\"      Negative sim: {sim_neg_seen.mean():.4f} \u00b1 {sim_neg_seen.std():.4f}\")\n",
        "print(f\"      Margin:       {sim_pos_seen.mean() - sim_neg_seen.mean():.4f}\")\n",
        "print(\"\")\n",
        "print(\"  (B) Unseen (synthetic) embeddings:\")\n",
        "print(f\"      Positive sim: {sim_pos_synth.mean():.4f} \u00b1 {sim_pos_synth.std():.4f}\")\n",
        "print(f\"      Negative sim: {sim_neg_synth.mean():.4f} \u00b1 {sim_neg_synth.std():.4f}\")\n",
        "print(f\"      Margin:       {sim_pos_synth.mean() - sim_neg_synth.mean():.4f}\")\n",
        "print(\"\")\n",
        "print(\"  (C) Top-1 Prototype Retrieval Accuracy:\")\n",
        "print(f\"      Seen (real):       {acc_seen_retrieval:.4f} ({acc_seen_retrieval*100:.2f}%)\")\n",
        "print(f\"      Unseen (synth):    {acc_synth_retrieval:.4f} ({acc_synth_retrieval*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n### CHECK 2: CLASS-CONDITIONAL DIVERSITY ###\")\n",
        "print(\"\")\n",
        "print(f\"  Avg within-class cosine distance: {avg_dists.mean():.4f} \u00b1 {avg_dists.std():.4f}\")\n",
        "print(f\"  Avg per-dimension variance:       {per_dim_vars.mean():.6f} \u00b1 {per_dim_vars.std():.6f}\")\n",
        "print(f\"  Min class diversity (distance):   {avg_dists.min():.4f}\")\n",
        "print(f\"  Max class diversity (distance):   {avg_dists.max():.4f}\")\n",
        "\n",
        "print(\"\\n### FIGURES SAVED ###\")\n",
        "print(\"  - figures/prototype_alignment_seen.png\")\n",
        "print(\"  - figures/prototype_alignment_unseen_synth.png\")\n",
        "print(\"  - figures/synth_diversity_by_class.png\")\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INTERPRETATION\")\n",
        "print(\"=\"*70)\n",
        "margin_seen = sim_pos_seen.mean() - sim_neg_seen.mean()\n",
        "margin_synth = sim_pos_synth.mean() - sim_neg_synth.mean()\n",
        "\n",
        "if margin_seen > 0.1:\n",
        "    print(\"\u2713 CLIP alignment (seen): Good margin between positive and negative.\")\n",
        "else:\n",
        "    print(\"\u26a0 CLIP alignment (seen): Low margin \u2014 encoder may need more training.\")\n",
        "\n",
        "if margin_synth > 0.05:\n",
        "    print(\"\u2713 GAN conditioning (synth): Synthetic embeddings align with prototypes.\")\n",
        "else:\n",
        "    print(\"\u26a0 GAN conditioning (synth): Weak alignment \u2014 check conditioning quality.\")\n",
        "\n",
        "if avg_dists.mean() > 0.05:\n",
        "    print(\"\u2713 Diversity check: No severe mode collapse detected.\")\n",
        "else:\n",
        "    print(\"\u26a0 Diversity check: Low diversity \u2014 possible mode collapse.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"NEXT: Await confirmation before training GZSL classifier [A+B]\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# GZSL Classifier [A+B]: Real Seen + Synthetic Unseen\n",
        "\n",
        "This section trains the final **Generalised Zero-Shot Learning classifier** using:\n",
        "\n",
        "**Training data:**\n",
        "- Real seen embeddings: `E_train_seen` (from CLIP encoder)\n",
        "- Synthetic unseen embeddings: `E_synth_unseen` (from cWGAN-GP)\n",
        "\n",
        "**Evaluation:**\n",
        "- Seen test set: `E_test_seen` \u2192 `Acc_seen`\n",
        "- Unseen real data: `E_unseen` \u2192 `Acc_unseen`\n",
        "- Harmonic mean: `H = 2 * Acc_seen * Acc_unseen / (Acc_seen + Acc_unseen)`\n",
        "\n",
        "**Comparison with Baseline [A]:** The baseline was trained on raw EEG and could only predict seen classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# GZSL CLASSIFIER: DATA PREPARATION\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Load cached embeddings\n",
        "E_train_seen = np.load('cached_arrays/E_train_seen.npy')\n",
        "E_test_seen = np.load('cached_arrays/E_test_seen.npy')\n",
        "E_unseen = np.load('cached_arrays/E_unseen.npy')\n",
        "E_synth_unseen = np.load('cached_arrays/E_synth_unseen.npy')\n",
        "\n",
        "# Load labels\n",
        "y_train_seen = np.load('cached_arrays/y_train_seen.npy')\n",
        "y_test_seen = np.load('cached_arrays/y_test_seen.npy')\n",
        "y_unseen = np.load('cached_arrays/y_unseen.npy')\n",
        "y_synth_unseen = np.load('cached_arrays/y_synth_unseen.npy')\n",
        "\n",
        "print(\"Loaded embeddings:\")\n",
        "print(f\"  E_train_seen:   {E_train_seen.shape}\")\n",
        "print(f\"  E_test_seen:    {E_test_seen.shape}\")\n",
        "print(f\"  E_unseen:       {E_unseen.shape}\")\n",
        "print(f\"  E_synth_unseen: {E_synth_unseen.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMBINE REAL SEEN + SYNTHETIC UNSEEN FOR TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "# Combine training data\n",
        "X_train_gzsl = np.vstack([E_train_seen, E_synth_unseen])\n",
        "y_train_gzsl = np.concatenate([y_train_seen, y_synth_unseen])\n",
        "\n",
        "print(\"GZSL Training Data:\")\n",
        "print(f\"  Real seen:      {len(E_train_seen)} samples, {len(np.unique(y_train_seen))} classes\")\n",
        "print(f\"  Synthetic unseen: {len(E_synth_unseen)} samples, {len(np.unique(y_synth_unseen))} classes\")\n",
        "print(f\"  Combined:       {len(X_train_gzsl)} samples, {len(np.unique(y_train_gzsl))} classes\")\n",
        "\n",
        "# Verify no label overlap between seen and unseen\n",
        "seen_labels = set(np.unique(y_train_seen))\n",
        "unseen_labels = set(np.unique(y_synth_unseen))\n",
        "overlap = seen_labels.intersection(unseen_labels)\n",
        "print(f\"\\nLabel overlap check: {len(overlap)} overlapping labels\")\n",
        "if len(overlap) == 0:\n",
        "    print(\"  \u2713 No overlap \u2014 seen and unseen label spaces are disjoint.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAIN GZSL LOGISTIC REGRESSION CLASSIFIER\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Training GZSL classifier (Logistic Regression)...\")\n",
        "print(f\"  Total classes: {len(np.unique(y_train_gzsl))}\")\n",
        "print(f\"  Total samples: {len(X_train_gzsl)}\")\n",
        "\n",
        "gzsl_clf = LogisticRegression(\n",
        "    multi_class='multinomial',\n",
        "    solver='lbfgs',\n",
        "    max_iter=1000,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "gzsl_clf.fit(X_train_gzsl, y_train_gzsl)\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EVALUATE ON SEEN CLASSES (E_test_seen)\n",
        "# =============================================================================\n",
        "\n",
        "y_pred_seen = gzsl_clf.predict(E_test_seen)\n",
        "\n",
        "acc_seen_gzsl = accuracy_score(y_test_seen, y_pred_seen)\n",
        "f1_seen_gzsl = f1_score(y_test_seen, y_pred_seen, average='macro', zero_division=0)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GZSL EVALUATION: SEEN CLASSES\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy (seen):  {acc_seen_gzsl:.4f} ({acc_seen_gzsl*100:.2f}%)\")\n",
        "print(f\"Macro F1 (seen):  {f1_seen_gzsl:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EVALUATE ON UNSEEN CLASSES (E_unseen \u2014 real EEG, never seen during training)\n",
        "# =============================================================================\n",
        "\n",
        "y_pred_unseen = gzsl_clf.predict(E_unseen)\n",
        "\n",
        "acc_unseen_gzsl = accuracy_score(y_unseen, y_pred_unseen)\n",
        "f1_unseen_gzsl = f1_score(y_unseen, y_pred_unseen, average='macro', zero_division=0)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GZSL EVALUATION: UNSEEN CLASSES\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy (unseen): {acc_unseen_gzsl:.4f} ({acc_unseen_gzsl*100:.2f}%)\")\n",
        "print(f\"Macro F1 (unseen): {f1_unseen_gzsl:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPUTE HARMONIC MEAN (H)\n",
        "# =============================================================================\n",
        "\n",
        "# H = 2 * Acc_seen * Acc_unseen / (Acc_seen + Acc_unseen)\n",
        "if acc_seen_gzsl + acc_unseen_gzsl > 0:\n",
        "    H_gzsl = 2 * acc_seen_gzsl * acc_unseen_gzsl / (acc_seen_gzsl + acc_unseen_gzsl)\n",
        "else:\n",
        "    H_gzsl = 0.0\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GZSL HARMONIC MEAN\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"H = 2 * {acc_seen_gzsl:.4f} * {acc_unseen_gzsl:.4f} / ({acc_seen_gzsl:.4f} + {acc_unseen_gzsl:.4f})\")\n",
        "print(f\"H = {H_gzsl:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STORE GZSL RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "gzsl_results = {\n",
        "    'model': 'GZSL [A+B] (LR on CLIP embeddings + cWGAN-GP synthetic)',\n",
        "    'acc_seen': acc_seen_gzsl,\n",
        "    'acc_unseen': acc_unseen_gzsl,\n",
        "    'H': H_gzsl,\n",
        "    'macro_f1_seen': f1_seen_gzsl,\n",
        "    'macro_f1_unseen': f1_unseen_gzsl,\n",
        "    'n_train_seen': len(E_train_seen),\n",
        "    'n_train_synth': len(E_synth_unseen),\n",
        "    'n_test_seen': len(E_test_seen),\n",
        "    'n_test_unseen': len(E_unseen),\n",
        "    'n_classes_seen': len(np.unique(y_train_seen)),\n",
        "    'n_classes_unseen': len(np.unique(y_synth_unseen))\n",
        "}\n",
        "\n",
        "print(\"GZSL results stored in 'gzsl_results' dictionary.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Comparison: Baseline [A] vs GZSL [A+B]\n",
        "\n",
        "Comparing the baseline logistic regression (raw EEG, seen classes only) with the GZSL classifier (CLIP embeddings + synthetic unseen)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPARISON: BASELINE [A] vs GZSL [A+B]\n",
        "# =============================================================================\n",
        "\n",
        "# Retrieve baseline results (computed earlier in notebook)\n",
        "# baseline_gzsl_results should exist from the GZSL baseline evaluation\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPARISON: BASELINE [A] vs GZSL [A+B]\")\n",
        "print(\"=\"*70)\n",
        "print(\"\")\n",
        "print(f\"{'Metric':<20} {'Baseline [A]':>15} {'GZSL [A+B]':>15} {'Improvement':>15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Seen accuracy\n",
        "base_seen = baseline_gzsl_results['acc_seen']\n",
        "gzsl_seen = gzsl_results['acc_seen']\n",
        "delta_seen = gzsl_seen - base_seen\n",
        "print(f\"{'Acc (seen)':<20} {base_seen:>15.4f} {gzsl_seen:>15.4f} {delta_seen:>+15.4f}\")\n",
        "\n",
        "# Unseen accuracy\n",
        "base_unseen = baseline_gzsl_results['acc_unseen']\n",
        "gzsl_unseen = gzsl_results['acc_unseen']\n",
        "delta_unseen = gzsl_unseen - base_unseen\n",
        "print(f\"{'Acc (unseen)':<20} {base_unseen:>15.4f} {gzsl_unseen:>15.4f} {delta_unseen:>+15.4f}\")\n",
        "\n",
        "# Harmonic mean\n",
        "base_H = baseline_gzsl_results['H']\n",
        "gzsl_H = gzsl_results['H']\n",
        "delta_H = gzsl_H - base_H\n",
        "print(f\"{'Harmonic Mean (H)':<20} {base_H:>15.4f} {gzsl_H:>15.4f} {delta_H:>+15.4f}\")\n",
        "\n",
        "# Macro F1 seen\n",
        "base_f1_seen = baseline_gzsl_results['macro_f1_seen']\n",
        "gzsl_f1_seen = gzsl_results['macro_f1_seen']\n",
        "print(f\"{'F1 (seen)':<20} {base_f1_seen:>15.4f} {gzsl_f1_seen:>15.4f} {gzsl_f1_seen - base_f1_seen:>+15.4f}\")\n",
        "\n",
        "# Macro F1 unseen\n",
        "base_f1_unseen = baseline_gzsl_results['macro_f1_unseen']\n",
        "gzsl_f1_unseen = gzsl_results['macro_f1_unseen']\n",
        "print(f\"{'F1 (unseen)':<20} {base_f1_unseen:>15.4f} {gzsl_f1_unseen:>15.4f} {gzsl_f1_unseen - base_f1_unseen:>+15.4f}\")\n",
        "\n",
        "print(\"-\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# VISUALIZATION: BASELINE vs GZSL COMPARISON\n",
        "# =============================================================================\n",
        "\n",
        "metrics = ['Acc (seen)', 'Acc (unseen)', 'Harmonic Mean']\n",
        "baseline_vals = [baseline_gzsl_results['acc_seen'], baseline_gzsl_results['acc_unseen'], baseline_gzsl_results['H']]\n",
        "gzsl_vals = [gzsl_results['acc_seen'], gzsl_results['acc_unseen'], gzsl_results['H']]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars1 = ax.bar(x - width/2, baseline_vals, width, label='Baseline [A]', color='steelblue')\n",
        "bars2 = ax.bar(x + width/2, gzsl_vals, width, label='GZSL [A+B]', color='darkorange')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height:.3f}',\n",
        "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 3), textcoords=\"offset points\",\n",
        "                ha='center', va='bottom', fontsize=10)\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height:.3f}',\n",
        "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 3), textcoords=\"offset points\",\n",
        "                ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('GZSL Performance: Baseline [A] vs Customised Model [A+B]', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics, fontsize=12)\n",
        "ax.legend(fontsize=11)\n",
        "ax.set_ylim(0, max(max(baseline_vals), max(gzsl_vals)) * 1.2)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/gzsl_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Saved: figures/gzsl_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PREDICTION DISTRIBUTION ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "# Check if GZSL classifier predicts unseen labels (which baseline couldn't)\n",
        "pred_unseen_labels = set(np.unique(y_pred_unseen))\n",
        "true_unseen_labels = set(np.unique(y_unseen))\n",
        "seen_label_set = set(np.unique(y_train_seen))\n",
        "\n",
        "# How many predictions are in the unseen label space?\n",
        "n_pred_in_unseen = sum(1 for p in y_pred_unseen if p in true_unseen_labels)\n",
        "n_pred_in_seen = sum(1 for p in y_pred_unseen if p in seen_label_set)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PREDICTION DISTRIBUTION ON UNSEEN DATA\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total predictions on unseen data: {len(y_pred_unseen)}\")\n",
        "print(f\"  Predictions in unseen label space: {n_pred_in_unseen} ({100*n_pred_in_unseen/len(y_pred_unseen):.1f}%)\")\n",
        "print(f\"  Predictions in seen label space:   {n_pred_in_seen} ({100*n_pred_in_seen/len(y_pred_unseen):.1f}%)\")\n",
        "print(\"\")\n",
        "print(\"KEY OBSERVATION:\")\n",
        "if n_pred_in_unseen > 0:\n",
        "    print(f\"  \u2713 GZSL classifier CAN predict unseen classes (unlike baseline).\")\n",
        "    print(f\"    This demonstrates successful zero-shot transfer via CLIP + cWGAN-GP.\")\n",
        "else:\n",
        "    print(f\"  \u26a0 Classifier still only predicts seen labels \u2014 check training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FINAL GZSL IMPLEMENTATION SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL SUMMARY: GZSL CLASSIFIER [A+B]\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n### MODEL PIPELINE ###\")\n",
        "print(\"  1. Brain-Text CLIP encoder: Maps EEG \u2192 64D semantic space\")\n",
        "print(\"  2. cWGAN-GP: Generates synthetic unseen embeddings from text prototypes\")\n",
        "print(\"  3. GZSL Classifier: Logistic Regression on real seen + synthetic unseen\")\n",
        "\n",
        "print(\"\\n### GZSL RESULTS ###\")\n",
        "print(f\"  Acc (seen):        {gzsl_results['acc_seen']:.4f} ({gzsl_results['acc_seen']*100:.2f}%)\")\n",
        "print(f\"  Acc (unseen):      {gzsl_results['acc_unseen']:.4f} ({gzsl_results['acc_unseen']*100:.2f}%)\")\n",
        "print(f\"  Harmonic Mean (H): {gzsl_results['H']:.4f}\")\n",
        "print(f\"  Macro F1 (seen):   {gzsl_results['macro_f1_seen']:.4f}\")\n",
        "print(f\"  Macro F1 (unseen): {gzsl_results['macro_f1_unseen']:.4f}\")\n",
        "\n",
        "print(\"\\n### IMPROVEMENT OVER BASELINE ###\")\n",
        "print(f\"  \u0394 Acc (seen):   {gzsl_results['acc_seen'] - baseline_gzsl_results['acc_seen']:+.4f}\")\n",
        "print(f\"  \u0394 Acc (unseen): {gzsl_results['acc_unseen'] - baseline_gzsl_results['acc_unseen']:+.4f}\")\n",
        "print(f\"  \u0394 H:            {gzsl_results['H'] - baseline_gzsl_results['H']:+.4f}\")\n",
        "\n",
        "print(\"\\n### KEY INSIGHT ###\")\n",
        "print(\"  The baseline achieves ~0% on unseen classes because it only knows seen labels.\")\n",
        "print(\"  GZSL [A+B] achieves non-zero unseen accuracy by:\")\n",
        "print(\"    - Using CLIP to create a shared EEG-text semantic space\")\n",
        "print(\"    - Using cWGAN-GP to synthesize training data for unseen classes\")\n",
        "print(\"    - Training a classifier that sees both seen and (synthetic) unseen classes\")\n",
        "\n",
        "print(\"\\n### FIGURES GENERATED ###\")\n",
        "print(\"  - figures/gzsl_comparison.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GZSL IMPLEMENTATION COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Ablation Study: Attributing GZSL Performance Gains\n",
        "\n",
        "This section compares four methods to isolate the contribution of each component:\n",
        "\n",
        "| Method | Description | Training Data |\n",
        "|--------|-------------|---------------|\n",
        "| **A** | Raw EEG + LR (baseline) | Seen raw EEG only |\n",
        "| **B** | CLIP Prototype Retrieval | No training (cosine similarity) |\n",
        "| **C** | CLIP Embedding + LR | Seen CLIP embeddings only |\n",
        "| **D** | Full CLIP + cWGAN-GP + LR | Seen real + Unseen synthetic |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ABLATION STUDY: LOAD ALL REQUIRED DATA\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Load CLIP embeddings\n",
        "E_train_seen = np.load('cached_arrays/E_train_seen.npy')\n",
        "E_test_seen = np.load('cached_arrays/E_test_seen.npy')\n",
        "E_unseen = np.load('cached_arrays/E_unseen.npy')\n",
        "E_synth_unseen = np.load('cached_arrays/E_synth_unseen.npy')\n",
        "\n",
        "# Load labels\n",
        "y_train_seen = np.load('cached_arrays/y_train_seen.npy')\n",
        "y_test_seen = np.load('cached_arrays/y_test_seen.npy')\n",
        "y_unseen = np.load('cached_arrays/y_unseen.npy')\n",
        "y_synth_unseen = np.load('cached_arrays/y_synth_unseen.npy')\n",
        "\n",
        "# Load prototypes and class lists\n",
        "S_seen_array = np.load('cached_arrays/S_seen_prototypes.npy')\n",
        "S_unseen_array = np.load('cached_arrays/S_unseen_prototypes.npy')\n",
        "seen_classes = np.load('cached_arrays/seen_classes.npy')\n",
        "unseen_classes = np.load('cached_arrays/unseen_classes.npy')\n",
        "\n",
        "# Build label-to-index mappings\n",
        "seen_label_to_idx = {c: i for i, c in enumerate(seen_classes)}\n",
        "unseen_label_to_idx = {c: i for i, c in enumerate(unseen_classes)}\n",
        "\n",
        "# Also need raw EEG for baseline\n",
        "# Recreate train/test split from raw data\n",
        "brain_seen_np = brain_seen.numpy()\n",
        "label_seen_np = label_seen.numpy().flatten()\n",
        "\n",
        "indices_seen = np.arange(len(brain_seen_np))\n",
        "train_idx, test_idx = train_test_split(\n",
        "    indices_seen, test_size=0.2, random_state=SEED, stratify=label_seen_np\n",
        ")\n",
        "\n",
        "X_train_raw = brain_seen_np[train_idx]\n",
        "X_test_raw = brain_seen_np[test_idx]\n",
        "y_train_raw = label_seen_np[train_idx]\n",
        "y_test_raw = label_seen_np[test_idx]\n",
        "X_unseen_raw = brain_unseen.numpy()\n",
        "y_unseen_raw = label_unseen.numpy().flatten()\n",
        "\n",
        "print(\"Data loaded for ablation study.\")\n",
        "print(f\"  Seen classes: {len(seen_classes)}, Unseen classes: {len(unseen_classes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HELPER FUNCTIONS FOR ABLATION\n",
        "# =============================================================================\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    \"\"\"Compute accuracy and macro F1.\"\"\"\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    return acc, f1\n",
        "\n",
        "def compute_harmonic_mean(acc_seen, acc_unseen):\n",
        "    \"\"\"Compute H = 2 * Acc_S * Acc_U / (Acc_S + Acc_U).\"\"\"\n",
        "    if acc_seen + acc_unseen > 0:\n",
        "        return 2 * acc_seen * acc_unseen / (acc_seen + acc_unseen)\n",
        "    return 0.0\n",
        "\n",
        "def prototype_retrieval(embeddings, prototypes, class_list):\n",
        "    \"\"\"Predict class by argmax cosine similarity to prototypes.\"\"\"\n",
        "    sims = np.dot(embeddings, prototypes.T)  # (N, n_classes)\n",
        "    pred_indices = np.argmax(sims, axis=1)\n",
        "    pred_labels = class_list[pred_indices]\n",
        "    return pred_labels\n",
        "\n",
        "# Store all ablation results\n",
        "ablation_results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# METHOD A: BASELINE LR (RAW EEG, SEEN-ONLY)\n",
        "# =============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"METHOD A: Raw EEG + LR (Baseline)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Standardise\n",
        "scaler_a = StandardScaler()\n",
        "X_train_a = scaler_a.fit_transform(X_train_raw)\n",
        "X_test_a = scaler_a.transform(X_test_raw)\n",
        "X_unseen_a = scaler_a.transform(X_unseen_raw)\n",
        "\n",
        "# Train\n",
        "clf_a = LogisticRegression(\n",
        "    multi_class='multinomial', solver='lbfgs', max_iter=1000, \n",
        "    random_state=SEED, n_jobs=-1\n",
        ")\n",
        "clf_a.fit(X_train_a, y_train_raw)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_seen_a = clf_a.predict(X_test_a)\n",
        "y_pred_unseen_a = clf_a.predict(X_unseen_a)\n",
        "\n",
        "acc_seen_a, f1_seen_a = compute_metrics(y_test_raw, y_pred_seen_a)\n",
        "acc_unseen_a, f1_unseen_a = compute_metrics(y_unseen_raw, y_pred_unseen_a)\n",
        "H_a = compute_harmonic_mean(acc_seen_a, acc_unseen_a)\n",
        "\n",
        "print(f\"  Acc_seen:  {acc_seen_a:.4f}, Acc_unseen: {acc_unseen_a:.4f}, H: {H_a:.4f}\")\n",
        "\n",
        "ablation_results.append({\n",
        "    'Method': 'Raw EEG LR (A)',\n",
        "    'Acc_seen': acc_seen_a,\n",
        "    'Acc_unseen': acc_unseen_a,\n",
        "    'H': H_a,\n",
        "    'MacroF1_seen': f1_seen_a,\n",
        "    'MacroF1_unseen': f1_unseen_a\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# METHOD B: CLIP PROTOTYPE RETRIEVAL (NO CLASSIFIER)\n",
        "# =============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"METHOD B: CLIP Prototype Retrieval\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Split-wise retrieval (recommended variant)\n",
        "# Seen test: predict among seen prototypes only\n",
        "y_pred_seen_b = prototype_retrieval(E_test_seen, S_seen_array, seen_classes)\n",
        "\n",
        "# Unseen test: predict among unseen prototypes only\n",
        "y_pred_unseen_b = prototype_retrieval(E_unseen, S_unseen_array, unseen_classes)\n",
        "\n",
        "acc_seen_b, f1_seen_b = compute_metrics(y_test_seen, y_pred_seen_b)\n",
        "acc_unseen_b, f1_unseen_b = compute_metrics(y_unseen, y_pred_unseen_b)\n",
        "H_b = compute_harmonic_mean(acc_seen_b, acc_unseen_b)\n",
        "\n",
        "print(f\"  Acc_seen:  {acc_seen_b:.4f}, Acc_unseen: {acc_unseen_b:.4f}, H: {H_b:.4f}\")\n",
        "\n",
        "ablation_results.append({\n",
        "    'Method': 'CLIP Prototype Retrieval (B)',\n",
        "    'Acc_seen': acc_seen_b,\n",
        "    'Acc_unseen': acc_unseen_b,\n",
        "    'H': H_b,\n",
        "    'MacroF1_seen': f1_seen_b,\n",
        "    'MacroF1_unseen': f1_unseen_b\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# METHOD C: CLIP EMBEDDING + LR (SEEN-ONLY CLASSIFIER)\n",
        "# =============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"METHOD C: CLIP Embedding + LR (seen-only)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Train LR on CLIP embeddings (seen only)\n",
        "clf_c = LogisticRegression(\n",
        "    multi_class='multinomial', solver='lbfgs', max_iter=1000,\n",
        "    random_state=SEED, n_jobs=-1\n",
        ")\n",
        "clf_c.fit(E_train_seen, y_train_seen)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_seen_c = clf_c.predict(E_test_seen)\n",
        "y_pred_unseen_c = clf_c.predict(E_unseen)\n",
        "\n",
        "acc_seen_c, f1_seen_c = compute_metrics(y_test_seen, y_pred_seen_c)\n",
        "acc_unseen_c, f1_unseen_c = compute_metrics(y_unseen, y_pred_unseen_c)\n",
        "H_c = compute_harmonic_mean(acc_seen_c, acc_unseen_c)\n",
        "\n",
        "print(f\"  Acc_seen:  {acc_seen_c:.4f}, Acc_unseen: {acc_unseen_c:.4f}, H: {H_c:.4f}\")\n",
        "\n",
        "ablation_results.append({\n",
        "    'Method': 'CLIP Embedding + LR (C)',\n",
        "    'Acc_seen': acc_seen_c,\n",
        "    'Acc_unseen': acc_unseen_c,\n",
        "    'H': H_c,\n",
        "    'MacroF1_seen': f1_seen_c,\n",
        "    'MacroF1_unseen': f1_unseen_c\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# METHOD D: FULL CLIP + cWGAN-GP + LR (GZSL)\n",
        "# =============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"METHOD D: CLIP + cWGAN-GP + LR (Full A+B)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Combine real seen + synthetic unseen\n",
        "X_train_d = np.vstack([E_train_seen, E_synth_unseen])\n",
        "y_train_d = np.concatenate([y_train_seen, y_synth_unseen])\n",
        "\n",
        "# Train LR\n",
        "clf_d = LogisticRegression(\n",
        "    multi_class='multinomial', solver='lbfgs', max_iter=1000,\n",
        "    random_state=SEED, n_jobs=-1\n",
        ")\n",
        "clf_d.fit(X_train_d, y_train_d)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_seen_d = clf_d.predict(E_test_seen)\n",
        "y_pred_unseen_d = clf_d.predict(E_unseen)\n",
        "\n",
        "acc_seen_d, f1_seen_d = compute_metrics(y_test_seen, y_pred_seen_d)\n",
        "acc_unseen_d, f1_unseen_d = compute_metrics(y_unseen, y_pred_unseen_d)\n",
        "H_d = compute_harmonic_mean(acc_seen_d, acc_unseen_d)\n",
        "\n",
        "print(f\"  Acc_seen:  {acc_seen_d:.4f}, Acc_unseen: {acc_unseen_d:.4f}, H: {H_d:.4f}\")\n",
        "\n",
        "ablation_results.append({\n",
        "    'Method': 'CLIP + cWGAN-GP + LR (D)',\n",
        "    'Acc_seen': acc_seen_d,\n",
        "    'Acc_unseen': acc_unseen_d,\n",
        "    'H': H_d,\n",
        "    'MacroF1_seen': f1_seen_d,\n",
        "    'MacroF1_unseen': f1_unseen_d\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ABLATION SUMMARY TABLE\n",
        "# =============================================================================\n",
        "\n",
        "df_ablation = pd.DataFrame(ablation_results)\n",
        "df_ablation = df_ablation.set_index('Method')\n",
        "\n",
        "# Format for display\n",
        "print(\"=\"*80)\n",
        "print(\"ABLATION STUDY: SUMMARY TABLE\")\n",
        "print(\"=\"*80)\n",
        "print(df_ablation.to_string(float_format='%.4f'))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 2\u00d72 BIAS TABLE: TRUE GROUP vs PREDICTED GROUP (FULL MODEL D)\n",
        "# =============================================================================\n",
        "\n",
        "seen_label_set = set(seen_classes)\n",
        "unseen_label_set = set(unseen_classes)\n",
        "\n",
        "def count_group_predictions(y_true, y_pred, true_is_seen):\n",
        "    \"\"\"Count predictions falling into seen vs unseen label space.\"\"\"\n",
        "    n_pred_seen = sum(1 for p in y_pred if p in seen_label_set)\n",
        "    n_pred_unseen = sum(1 for p in y_pred if p in unseen_label_set)\n",
        "    n_total = len(y_pred)\n",
        "    return n_pred_seen, n_pred_unseen, n_total\n",
        "\n",
        "# Seen test samples\n",
        "n_seen_pred_s, n_seen_pred_u, n_seen_total = count_group_predictions(\n",
        "    y_test_seen, y_pred_seen_d, True\n",
        ")\n",
        "\n",
        "# Unseen test samples\n",
        "n_unseen_pred_s, n_unseen_pred_u, n_unseen_total = count_group_predictions(\n",
        "    y_unseen, y_pred_unseen_d, False\n",
        ")\n",
        "\n",
        "# Create bias table\n",
        "bias_table = pd.DataFrame({\n",
        "    'Pred: Seen': [\n",
        "        f\"{n_seen_pred_s} ({100*n_seen_pred_s/n_seen_total:.1f}%)\",\n",
        "        f\"{n_unseen_pred_s} ({100*n_unseen_pred_s/n_unseen_total:.1f}%)\"\n",
        "    ],\n",
        "    'Pred: Unseen': [\n",
        "        f\"{n_seen_pred_u} ({100*n_seen_pred_u/n_seen_total:.1f}%)\",\n",
        "        f\"{n_unseen_pred_u} ({100*n_unseen_pred_u/n_unseen_total:.1f}%)\"\n",
        "    ]\n",
        "}, index=['True: Seen', 'True: Unseen'])\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"2\u00d72 BIAS TABLE: TRUE GROUP vs PREDICTED GROUP (Full Model D)\")\n",
        "print(\"=\"*60)\n",
        "print(bias_table.to_string())\n",
        "print(\"=\"*60)\n",
        "print(\"\")\n",
        "print(\"Interpretation:\")\n",
        "print(f\"  - Seen samples predicted as seen:     {100*n_seen_pred_s/n_seen_total:.1f}%\")\n",
        "print(f\"  - Unseen samples predicted as unseen: {100*n_unseen_pred_u/n_unseen_total:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ABLATION BAR CHART\n",
        "# =============================================================================\n",
        "\n",
        "methods = ['Raw EEG (A)', 'CLIP Proto (B)', 'CLIP+LR (C)', 'Full GZSL (D)']\n",
        "acc_seen_vals = [acc_seen_a, acc_seen_b, acc_seen_c, acc_seen_d]\n",
        "acc_unseen_vals = [acc_unseen_a, acc_unseen_b, acc_unseen_c, acc_unseen_d]\n",
        "H_vals = [H_a, H_b, H_c, H_d]\n",
        "\n",
        "x = np.arange(len(methods))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "bars1 = ax.bar(x - width, acc_seen_vals, width, label='Acc_seen', color='steelblue')\n",
        "bars2 = ax.bar(x, acc_unseen_vals, width, label='Acc_unseen', color='darkorange')\n",
        "bars3 = ax.bar(x + width, H_vals, width, label='Harmonic Mean (H)', color='green')\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars1, bars2, bars3]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:.3f}',\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3), textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('Ablation Study: GZSL Performance by Method', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(methods, fontsize=11)\n",
        "ax.legend(fontsize=11)\n",
        "ax.set_ylim(0, max(max(acc_seen_vals), max(acc_unseen_vals), max(H_vals)) * 1.25)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/gzsl_ablation_bar.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Saved: figures/gzsl_ablation_bar.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ABLATION STUDY: KEY FINDINGS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ABLATION STUDY: KEY FINDINGS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n### COMPONENT CONTRIBUTIONS ###\")\n",
        "print(\"\")\n",
        "\n",
        "# CLIP representation gain\n",
        "clip_gain_seen = acc_seen_c - acc_seen_a\n",
        "print(f\"1. CLIP Representation (C vs A):\")\n",
        "print(f\"   \u0394 Acc_seen = {clip_gain_seen:+.4f}\")\n",
        "print(f\"   \u2192 CLIP embeddings {'improve' if clip_gain_seen > 0 else 'do not improve'} seen-class performance.\")\n",
        "\n",
        "# cWGAN-GP generation gain\n",
        "gan_gain_unseen = acc_unseen_d - acc_unseen_c\n",
        "print(f\"\")\n",
        "print(f\"2. cWGAN-GP Synthesis (D vs C):\")\n",
        "print(f\"   \u0394 Acc_unseen = {gan_gain_unseen:+.4f}\")\n",
        "print(f\"   \u2192 Synthetic embeddings {'enable' if gan_gain_unseen > 0 else 'do not enable'} zero-shot transfer.\")\n",
        "\n",
        "# Overall improvement\n",
        "overall_H_gain = H_d - H_a\n",
        "print(f\"\")\n",
        "print(f\"3. Overall Improvement (D vs A):\")\n",
        "print(f\"   \u0394 H = {overall_H_gain:+.4f}\")\n",
        "print(f\"   \u2192 Full model {'achieves' if overall_H_gain > 0 else 'does not achieve'} better GZSL trade-off.\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"### ABLATION TABLE (PANDAS) ###\")\n",
        "print(\"\")\n",
        "print(df_ablation.round(4).to_string())\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ABLATION COMPLETE \u2014 awaiting confirmation before further analysis.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Corrected 2\u00d72 Bias Table (Full Model D)\n",
        "\n",
        "Recomputing the true-group vs predicted-group bias table with proper counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CORRECTED 2\u00d72 BIAS TABLE FOR FULL MODEL (D)\n",
        "# =============================================================================\n",
        "# Recompute from scratch using proper boolean masks.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. Get ground truth labels\n",
        "# -----------------------------------------------------------------------------\n",
        "y_true_seen = y_test_seen  # from cached arrays (3308 samples)\n",
        "y_true_unseen = y_unseen   # from cached arrays (16000 samples)\n",
        "\n",
        "print(f\"Ground truth sizes: seen test = {len(y_true_seen)}, unseen test = {len(y_true_unseen)}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. Get Model D predictions (clf_d was trained on seen+synth unseen)\n",
        "# -----------------------------------------------------------------------------\n",
        "# clf_d should already exist from the ablation study\n",
        "y_pred_on_seen = clf_d.predict(E_test_seen)\n",
        "y_pred_on_unseen = clf_d.predict(E_unseen)\n",
        "\n",
        "print(f\"Prediction sizes: on seen = {len(y_pred_on_seen)}, on unseen = {len(y_pred_on_unseen)}\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. Define seen/unseen label sets\n",
        "# -----------------------------------------------------------------------------\n",
        "seen_set = set(seen_classes.tolist())\n",
        "unseen_set = set(unseen_classes.tolist())\n",
        "\n",
        "print(f\"Label sets: |seen| = {len(seen_set)}, |unseen| = {len(unseen_set)}\")\n",
        "print(f\"Overlap check: {len(seen_set & unseen_set)} overlapping labels (should be 0)\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Compute boolean masks for predictions\n",
        "# -----------------------------------------------------------------------------\n",
        "# For predictions on seen test data\n",
        "pred_seen_is_seen = np.isin(y_pred_on_seen, list(seen_set))\n",
        "pred_seen_is_unseen = np.isin(y_pred_on_seen, list(unseen_set))\n",
        "\n",
        "# For predictions on unseen test data\n",
        "pred_unseen_is_seen = np.isin(y_pred_on_unseen, list(seen_set))\n",
        "pred_unseen_is_unseen = np.isin(y_pred_on_unseen, list(unseen_set))\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. Sanity check: every prediction must be in exactly one set\n",
        "# -----------------------------------------------------------------------------\n",
        "assert np.all(pred_seen_is_seen ^ pred_seen_is_unseen), \"Some predictions on seen data are not in seen or unseen set!\"\n",
        "assert np.all(pred_unseen_is_seen ^ pred_unseen_is_unseen), \"Some predictions on unseen data are not in seen or unseen set!\"\n",
        "print(\"\\n\u2713 Sanity check passed: all predictions fall into exactly one label set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BUILD 2\u00d72 BIAS TABLE WITH CORRECT COUNTS\n",
        "# =============================================================================\n",
        "\n",
        "# Compute raw counts\n",
        "# Rows: True group (Seen, Unseen)\n",
        "# Cols: Predicted group (Seen, Unseen)\n",
        "\n",
        "count_true_seen_pred_seen = pred_seen_is_seen.sum()\n",
        "count_true_seen_pred_unseen = pred_seen_is_unseen.sum()\n",
        "count_true_unseen_pred_seen = pred_unseen_is_seen.sum()\n",
        "count_true_unseen_pred_unseen = pred_unseen_is_unseen.sum()\n",
        "\n",
        "# Row totals\n",
        "total_seen_test = len(y_true_seen)\n",
        "total_unseen_test = len(y_true_unseen)\n",
        "\n",
        "# Verify row totals\n",
        "row1_sum = count_true_seen_pred_seen + count_true_seen_pred_unseen\n",
        "row2_sum = count_true_unseen_pred_seen + count_true_unseen_pred_unseen\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"2\u00d72 BIAS TABLE: RAW COUNTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"                    Pred: Seen    Pred: Unseen    Row Total\")\n",
        "print(f\"True: Seen          {count_true_seen_pred_seen:10d}    {count_true_seen_pred_unseen:12d}    {row1_sum:9d}\")\n",
        "print(f\"True: Unseen        {count_true_unseen_pred_seen:10d}    {count_true_unseen_pred_unseen:12d}    {row2_sum:9d}\")\n",
        "print(\"\")\n",
        "print(f\"Expected row totals: Seen test = {total_seen_test}, Unseen test = {total_unseen_test}\")\n",
        "assert row1_sum == total_seen_test, f\"Row 1 sum mismatch: {row1_sum} != {total_seen_test}\"\n",
        "assert row2_sum == total_unseen_test, f\"Row 2 sum mismatch: {row2_sum} != {total_unseen_test}\"\n",
        "print(\"\u2713 Row totals verified!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ROW-NORMALISED PERCENTAGES\n",
        "# =============================================================================\n",
        "\n",
        "pct_true_seen_pred_seen = 100 * count_true_seen_pred_seen / total_seen_test\n",
        "pct_true_seen_pred_unseen = 100 * count_true_seen_pred_unseen / total_seen_test\n",
        "pct_true_unseen_pred_seen = 100 * count_true_unseen_pred_seen / total_unseen_test\n",
        "pct_true_unseen_pred_unseen = 100 * count_true_unseen_pred_unseen / total_unseen_test\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"2\u00d72 BIAS TABLE: ROW-NORMALISED PERCENTAGES\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"                    Pred: Seen    Pred: Unseen\")\n",
        "print(f\"True: Seen          {pct_true_seen_pred_seen:10.1f}%    {pct_true_seen_pred_unseen:11.1f}%\")\n",
        "print(f\"True: Unseen        {pct_true_unseen_pred_seen:10.1f}%    {pct_true_unseen_pred_unseen:11.1f}%\")\n",
        "print(\"\")\n",
        "print(\"Interpretation:\")\n",
        "print(f\"  - {pct_true_seen_pred_seen:.1f}% of seen samples are correctly routed to seen labels\")\n",
        "print(f\"  - {pct_true_unseen_pred_unseen:.1f}% of unseen samples are correctly routed to unseen labels\")\n",
        "print(f\"  - {pct_true_unseen_pred_seen:.1f}% of unseen samples are misrouted to seen labels (seen-class bias)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CREATE PANDAS DATAFRAME VERSION\n",
        "# =============================================================================\n",
        "\n",
        "bias_table_counts = pd.DataFrame(\n",
        "    [[count_true_seen_pred_seen, count_true_seen_pred_unseen],\n",
        "     [count_true_unseen_pred_seen, count_true_unseen_pred_unseen]],\n",
        "    index=['True: Seen', 'True: Unseen'],\n",
        "    columns=['Pred: Seen', 'Pred: Unseen']\n",
        ")\n",
        "\n",
        "bias_table_pct = pd.DataFrame(\n",
        "    [[pct_true_seen_pred_seen, pct_true_seen_pred_unseen],\n",
        "     [pct_true_unseen_pred_seen, pct_true_unseen_pred_unseen]],\n",
        "    index=['True: Seen', 'True: Unseen'],\n",
        "    columns=['Pred: Seen', 'Pred: Unseen']\n",
        ")\n",
        "\n",
        "print(\"\\nBias Table (Counts):\")\n",
        "print(bias_table_counts.to_string())\n",
        "print(\"\\nBias Table (Percentages):\")\n",
        "print(bias_table_pct.round(1).to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HEATMAP VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Counts heatmap\n",
        "im1 = axes[0].imshow(bias_table_counts.values, cmap='Blues')\n",
        "axes[0].set_xticks([0, 1])\n",
        "axes[0].set_yticks([0, 1])\n",
        "axes[0].set_xticklabels(['Pred: Seen', 'Pred: Unseen'])\n",
        "axes[0].set_yticklabels(['True: Seen', 'True: Unseen'])\n",
        "axes[0].set_title('Bias Table (Counts)', fontsize=12)\n",
        "\n",
        "# Add count annotations\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        val = bias_table_counts.values[i, j]\n",
        "        color = 'white' if val > bias_table_counts.values.max() / 2 else 'black'\n",
        "        axes[0].text(j, i, f'{val:,}', ha='center', va='center', color=color, fontsize=14)\n",
        "\n",
        "# Percentage heatmap\n",
        "im2 = axes[1].imshow(bias_table_pct.values, cmap='Oranges', vmin=0, vmax=100)\n",
        "axes[1].set_xticks([0, 1])\n",
        "axes[1].set_yticks([0, 1])\n",
        "axes[1].set_xticklabels(['Pred: Seen', 'Pred: Unseen'])\n",
        "axes[1].set_yticklabels(['True: Seen', 'True: Unseen'])\n",
        "axes[1].set_title('Bias Table (Row %)', fontsize=12)\n",
        "\n",
        "# Add percentage annotations\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        val = bias_table_pct.values[i, j]\n",
        "        color = 'white' if val > 50 else 'black'\n",
        "        axes[1].text(j, i, f'{val:.1f}%', ha='center', va='center', color=color, fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/bias_table_full_model.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nSaved: figures/bias_table_full_model.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Debug + Fixed 2\u00d72 Bias Table\n",
        "\n",
        "First debug the label sets, then compute bias table using classifier's actual label space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DEBUG: IDENTIFY LABEL SET MISMATCH\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get label sets from different sources\n",
        "clf_d_classes = set(clf_d.classes_.tolist())\n",
        "seen_classes_set = set(seen_classes.tolist())\n",
        "unseen_classes_set = set(unseen_classes.tolist())\n",
        "\n",
        "# Also check y_train_seen and y_synth_unseen\n",
        "train_seen_labels = set(np.unique(y_train_seen).tolist())\n",
        "synth_unseen_labels = set(np.unique(y_synth_unseen).tolist())\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LABEL SET DEBUGGING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"clf_d knows {len(clf_d_classes)} classes\")\n",
        "print(f\"seen_classes (cached): {len(seen_classes_set)} classes\")\n",
        "print(f\"unseen_classes (cached): {len(unseen_classes_set)} classes\")\n",
        "print(f\"y_train_seen unique: {len(train_seen_labels)} classes\")\n",
        "print(f\"y_synth_unseen unique: {len(synth_unseen_labels)} classes\")\n",
        "print(\"\")\n",
        "\n",
        "# Check overlaps\n",
        "cached_union = seen_classes_set | unseen_classes_set\n",
        "training_union = train_seen_labels | synth_unseen_labels\n",
        "\n",
        "print(f\"cached seen \u222a unseen: {len(cached_union)} classes\")\n",
        "print(f\"training labels union: {len(training_union)} classes\")\n",
        "print(\"\")\n",
        "\n",
        "# Find mismatches\n",
        "in_clf_not_cached = clf_d_classes - cached_union\n",
        "in_cached_not_clf = cached_union - clf_d_classes\n",
        "\n",
        "print(f\"Labels in clf_d but NOT in cached union: {len(in_clf_not_cached)}\")\n",
        "print(f\"Labels in cached union but NOT in clf_d: {len(in_cached_not_clf)}\")\n",
        "\n",
        "if len(in_clf_not_cached) > 0:\n",
        "    print(f\"  Sample of missing from cached: {list(in_clf_not_cached)[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FIXED BIAS TABLE: USE CLASSIFIER'S ACTUAL LABEL SPACE\n",
        "# =============================================================================\n",
        "# Instead of using cached seen_classes/unseen_classes, use the actual labels\n",
        "# from the training data: y_train_seen and y_synth_unseen\n",
        "\n",
        "# Define sets from actual training labels (what clf_d was trained on)\n",
        "actual_seen_set = set(np.unique(y_train_seen).tolist())\n",
        "actual_unseen_set = set(np.unique(y_synth_unseen).tolist())\n",
        "\n",
        "print(f\"Using actual training label sets:\")\n",
        "print(f\"  Seen classes (from y_train_seen): {len(actual_seen_set)}\")\n",
        "print(f\"  Unseen classes (from y_synth_unseen): {len(actual_unseen_set)}\")\n",
        "print(f\"  Overlap: {len(actual_seen_set & actual_unseen_set)} (should be 0)\")\n",
        "\n",
        "# Get predictions\n",
        "y_pred_on_seen = clf_d.predict(E_test_seen)\n",
        "y_pred_on_unseen = clf_d.predict(E_unseen)\n",
        "\n",
        "# Ground truth\n",
        "y_true_seen = y_test_seen\n",
        "y_true_unseen = y_unseen\n",
        "\n",
        "print(f\"\\nPrediction counts: on_seen={len(y_pred_on_seen)}, on_unseen={len(y_pred_on_unseen)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPUTE BOOLEAN MASKS WITH ACTUAL LABEL SETS\n",
        "# =============================================================================\n",
        "\n",
        "# For predictions on seen test data\n",
        "pred_seen_is_seen = np.isin(y_pred_on_seen, list(actual_seen_set))\n",
        "pred_seen_is_unseen = np.isin(y_pred_on_seen, list(actual_unseen_set))\n",
        "\n",
        "# For predictions on unseen test data  \n",
        "pred_unseen_is_seen = np.isin(y_pred_on_unseen, list(actual_seen_set))\n",
        "pred_unseen_is_unseen = np.isin(y_pred_on_unseen, list(actual_unseen_set))\n",
        "\n",
        "# Check coverage\n",
        "seen_coverage = pred_seen_is_seen | pred_seen_is_unseen\n",
        "unseen_coverage = pred_unseen_is_seen | pred_unseen_is_unseen\n",
        "\n",
        "print(f\"Predictions on seen data covered: {seen_coverage.sum()}/{len(seen_coverage)} ({100*seen_coverage.mean():.1f}%)\")\n",
        "print(f\"Predictions on unseen data covered: {unseen_coverage.sum()}/{len(unseen_coverage)} ({100*unseen_coverage.mean():.1f}%)\")\n",
        "\n",
        "# If not 100%, show what's missing\n",
        "if not np.all(seen_coverage):\n",
        "    uncovered = y_pred_on_seen[~seen_coverage]\n",
        "    print(f\"  Uncovered predictions on seen: {len(uncovered)}, sample: {np.unique(uncovered)[:5]}\")\n",
        "if not np.all(unseen_coverage):\n",
        "    uncovered = y_pred_on_unseen[~unseen_coverage]\n",
        "    print(f\"  Uncovered predictions on unseen: {len(uncovered)}, sample: {np.unique(uncovered)[:5]}\")\n",
        "\n",
        "# For bias table, we proceed with what IS covered\n",
        "print(\"\\n\u2713 Proceeding with bias table computation...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BUILD 2\u00d72 BIAS TABLE\n",
        "# =============================================================================\n",
        "\n",
        "# Raw counts\n",
        "count_true_seen_pred_seen = pred_seen_is_seen.sum()\n",
        "count_true_seen_pred_unseen = pred_seen_is_unseen.sum()\n",
        "count_true_unseen_pred_seen = pred_unseen_is_seen.sum()\n",
        "count_true_unseen_pred_unseen = pred_unseen_is_unseen.sum()\n",
        "\n",
        "# Row totals (these should equal dataset sizes if all predictions are covered)\n",
        "total_seen_test = len(y_true_seen)\n",
        "total_unseen_test = len(y_true_unseen)\n",
        "\n",
        "row1_sum = count_true_seen_pred_seen + count_true_seen_pred_unseen\n",
        "row2_sum = count_true_unseen_pred_seen + count_true_unseen_pred_unseen\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"2\u00d72 BIAS TABLE: RAW COUNTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"                    Pred: Seen    Pred: Unseen    Row Total\")\n",
        "print(f\"True: Seen          {count_true_seen_pred_seen:10d}    {count_true_seen_pred_unseen:12d}    {row1_sum:9d}\")\n",
        "print(f\"True: Unseen        {count_true_unseen_pred_seen:10d}    {count_true_unseen_pred_unseen:12d}    {row2_sum:9d}\")\n",
        "print(f\"\")\n",
        "print(f\"Expected totals: seen_test={total_seen_test}, unseen_test={total_unseen_test}\")\n",
        "\n",
        "# Row-normalised percentages  \n",
        "pct_true_seen_pred_seen = 100 * count_true_seen_pred_seen / total_seen_test\n",
        "pct_true_seen_pred_unseen = 100 * count_true_seen_pred_unseen / total_seen_test\n",
        "pct_true_unseen_pred_seen = 100 * count_true_unseen_pred_seen / total_unseen_test\n",
        "pct_true_unseen_pred_unseen = 100 * count_true_unseen_pred_unseen / total_unseen_test\n",
        "\n",
        "print(\"\")\n",
        "print(\"=\" * 60)\n",
        "print(\"2\u00d72 BIAS TABLE: ROW-NORMALISED PERCENTAGES\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"                    Pred: Seen    Pred: Unseen\")\n",
        "print(f\"True: Seen          {pct_true_seen_pred_seen:10.1f}%    {pct_true_seen_pred_unseen:11.1f}%\")\n",
        "print(f\"True: Unseen        {pct_true_unseen_pred_seen:10.1f}%    {pct_true_unseen_pred_unseen:11.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HEATMAP VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "# Create DataFrames\n",
        "bias_table_counts = pd.DataFrame(\n",
        "    [[count_true_seen_pred_seen, count_true_seen_pred_unseen],\n",
        "     [count_true_unseen_pred_seen, count_true_unseen_pred_unseen]],\n",
        "    index=['True: Seen', 'True: Unseen'],\n",
        "    columns=['Pred: Seen', 'Pred: Unseen']\n",
        ")\n",
        "\n",
        "bias_table_pct = pd.DataFrame(\n",
        "    [[pct_true_seen_pred_seen, pct_true_seen_pred_unseen],\n",
        "     [pct_true_unseen_pred_seen, pct_true_unseen_pred_unseen]],\n",
        "    index=['True: Seen', 'True: Unseen'],\n",
        "    columns=['Pred: Seen', 'Pred: Unseen']\n",
        ")\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Counts heatmap\n",
        "im1 = axes[0].imshow(bias_table_counts.values, cmap='Blues')\n",
        "axes[0].set_xticks([0, 1])\n",
        "axes[0].set_yticks([0, 1])\n",
        "axes[0].set_xticklabels(['Pred: Seen', 'Pred: Unseen'])\n",
        "axes[0].set_yticklabels(['True: Seen', 'True: Unseen'])\n",
        "axes[0].set_title('Bias Table (Counts)', fontsize=12)\n",
        "\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        val = bias_table_counts.values[i, j]\n",
        "        color = 'white' if val > bias_table_counts.values.max() / 2 else 'black'\n",
        "        axes[0].text(j, i, f'{val:,}', ha='center', va='center', color=color, fontsize=14)\n",
        "\n",
        "# Percentage heatmap\n",
        "im2 = axes[1].imshow(bias_table_pct.values, cmap='Oranges', vmin=0, vmax=100)\n",
        "axes[1].set_xticks([0, 1])\n",
        "axes[1].set_yticks([0, 1])\n",
        "axes[1].set_xticklabels(['Pred: Seen', 'Pred: Unseen'])\n",
        "axes[1].set_yticklabels(['True: Seen', 'True: Unseen'])\n",
        "axes[1].set_title('Bias Table (Row %)', fontsize=12)\n",
        "\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        val = bias_table_pct.values[i, j]\n",
        "        color = 'white' if val > 50 else 'black'\n",
        "        axes[1].text(j, i, f'{val:.1f}%', ha='center', va='center', color=color, fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/bias_table_full_model.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nSaved: figures/bias_table_full_model.png\")\n",
        "print(\"\")\n",
        "print(\"Interpretation:\")\n",
        "print(f\"  - {pct_true_seen_pred_seen:.1f}% of seen samples \u2192 predicted as seen\")\n",
        "print(f\"  - {pct_true_unseen_pred_unseen:.1f}% of unseen samples \u2192 predicted as unseen\")\n",
        "print(f\"  - {pct_true_unseen_pred_seen:.1f}% of unseen samples \u2192 misrouted to seen (seen-class bias)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# FIX: Label Collision Resolution\n",
        "\n",
        "**Problem Identified:** Unseen labels (1-200) collide with seen labels (1-200).\n",
        "\n",
        "**Solution:** Remap unseen labels to a disjoint range: `unseen_label_new = unseen_label_old + max(seen_labels)`\n",
        "\n",
        "This creates:\n",
        "- Seen classes: 1-1654\n",
        "- Unseen classes: 1655-1854"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 1: COMPUTE LABEL OFFSET\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Load cached embeddings and labels\n",
        "E_train_seen = np.load('cached_arrays/E_train_seen.npy')\n",
        "E_test_seen = np.load('cached_arrays/E_test_seen.npy')\n",
        "E_unseen = np.load('cached_arrays/E_unseen.npy')\n",
        "E_synth_unseen = np.load('cached_arrays/E_synth_unseen.npy')\n",
        "\n",
        "y_train_seen = np.load('cached_arrays/y_train_seen.npy')\n",
        "y_test_seen = np.load('cached_arrays/y_test_seen.npy')\n",
        "y_unseen_original = np.load('cached_arrays/y_unseen.npy')\n",
        "y_synth_unseen_original = np.load('cached_arrays/y_synth_unseen.npy')\n",
        "\n",
        "# Compute offset = max seen label\n",
        "LABEL_OFFSET = int(np.max(y_train_seen))\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LABEL COLLISION FIX\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Max seen label: {LABEL_OFFSET}\")\n",
        "print(f\"Original unseen labels: {y_unseen_original.min()} to {y_unseen_original.max()}\")\n",
        "print(f\"New unseen labels will be: {y_unseen_original.min() + LABEL_OFFSET} to {y_unseen_original.max() + LABEL_OFFSET}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 2: REMAP UNSEEN LABELS\n",
        "# =============================================================================\n",
        "\n",
        "# Remap unseen labels to avoid collision\n",
        "y_unseen_remapped = y_unseen_original + LABEL_OFFSET\n",
        "y_synth_unseen_remapped = y_synth_unseen_original + LABEL_OFFSET\n",
        "\n",
        "print(f\"Remapped y_unseen: {y_unseen_remapped.min()} to {y_unseen_remapped.max()}\")\n",
        "print(f\"Remapped y_synth_unseen: {y_synth_unseen_remapped.min()} to {y_synth_unseen_remapped.max()}\")\n",
        "\n",
        "# Verify no overlap\n",
        "seen_labels_set = set(np.unique(y_train_seen).tolist())\n",
        "unseen_labels_set = set(np.unique(y_unseen_remapped).tolist())\n",
        "overlap = seen_labels_set & unseen_labels_set\n",
        "\n",
        "print(f\"\\nSeen labels: {len(seen_labels_set)} unique\")\n",
        "print(f\"Unseen labels (remapped): {len(unseen_labels_set)} unique\")\n",
        "print(f\"Overlap: {len(overlap)} (should be 0)\")\n",
        "\n",
        "assert len(overlap) == 0, \"Label overlap still exists!\"\n",
        "print(\"\\n\u2713 Labels are now disjoint!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 3: RETRAIN GZSL CLASSIFIER WITH CORRECT LABELS\n",
        "# =============================================================================\n",
        "\n",
        "# Combine real seen + synthetic unseen (with remapped labels)\n",
        "X_train_gzsl_fixed = np.vstack([E_train_seen, E_synth_unseen])\n",
        "y_train_gzsl_fixed = np.concatenate([y_train_seen, y_synth_unseen_remapped])\n",
        "\n",
        "print(\"GZSL Training Data (Fixed):\")\n",
        "print(f\"  Real seen: {len(E_train_seen)} samples, labels {y_train_seen.min()}-{y_train_seen.max()}\")\n",
        "print(f\"  Synth unseen: {len(E_synth_unseen)} samples, labels {y_synth_unseen_remapped.min()}-{y_synth_unseen_remapped.max()}\")\n",
        "print(f\"  Combined: {len(X_train_gzsl_fixed)} samples, {len(np.unique(y_train_gzsl_fixed))} classes\")\n",
        "\n",
        "# Train\n",
        "print(\"\\nTraining GZSL classifier (fixed labels)...\")\n",
        "clf_gzsl_fixed = LogisticRegression(\n",
        "    multi_class='multinomial', solver='lbfgs', max_iter=1000,\n",
        "    random_state=SEED, n_jobs=-1, verbose=1\n",
        ")\n",
        "clf_gzsl_fixed.fit(X_train_gzsl_fixed, y_train_gzsl_fixed)\n",
        "print(\"\\n\u2713 Training complete!\")\n",
        "print(f\"Classifier knows {len(clf_gzsl_fixed.classes_)} classes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 4: EVALUATE GZSL CLASSIFIER\n",
        "# =============================================================================\n",
        "\n",
        "# Predictions\n",
        "y_pred_seen_fixed = clf_gzsl_fixed.predict(E_test_seen)\n",
        "y_pred_unseen_fixed = clf_gzsl_fixed.predict(E_unseen)\n",
        "\n",
        "# Metrics for seen (ground truth uses original labels)\n",
        "acc_seen_fixed = accuracy_score(y_test_seen, y_pred_seen_fixed)\n",
        "f1_seen_fixed = f1_score(y_test_seen, y_pred_seen_fixed, average='macro', zero_division=0)\n",
        "\n",
        "# Metrics for unseen (ground truth uses REMAPPED labels)\n",
        "acc_unseen_fixed = accuracy_score(y_unseen_remapped, y_pred_unseen_fixed)\n",
        "f1_unseen_fixed = f1_score(y_unseen_remapped, y_pred_unseen_fixed, average='macro', zero_division=0)\n",
        "\n",
        "# Harmonic mean\n",
        "if acc_seen_fixed + acc_unseen_fixed > 0:\n",
        "    H_fixed = 2 * acc_seen_fixed * acc_unseen_fixed / (acc_seen_fixed + acc_unseen_fixed)\n",
        "else:\n",
        "    H_fixed = 0.0\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"GZSL EVALUATION (FIXED LABELS)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Acc (seen):      {acc_seen_fixed:.4f} ({acc_seen_fixed*100:.2f}%)\")\n",
        "print(f\"Acc (unseen):    {acc_unseen_fixed:.4f} ({acc_unseen_fixed*100:.2f}%)\")\n",
        "print(f\"Harmonic Mean:   {H_fixed:.4f}\")\n",
        "print(f\"Macro F1 (seen): {f1_seen_fixed:.4f}\")\n",
        "print(f\"Macro F1 (unseen): {f1_unseen_fixed:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 5: CORRECTED 2\u00d72 BIAS TABLE\n",
        "# =============================================================================\n",
        "\n",
        "# Define label sets\n",
        "seen_set = seen_labels_set\n",
        "unseen_set = unseen_labels_set\n",
        "\n",
        "# Boolean masks for predictions\n",
        "pred_on_seen_is_seen = np.isin(y_pred_seen_fixed, list(seen_set))\n",
        "pred_on_seen_is_unseen = np.isin(y_pred_seen_fixed, list(unseen_set))\n",
        "\n",
        "pred_on_unseen_is_seen = np.isin(y_pred_unseen_fixed, list(seen_set))\n",
        "pred_on_unseen_is_unseen = np.isin(y_pred_unseen_fixed, list(unseen_set))\n",
        "\n",
        "# Sanity check\n",
        "assert np.all(pred_on_seen_is_seen | pred_on_seen_is_unseen), \"Some predictions uncovered!\"\n",
        "assert np.all(pred_on_unseen_is_seen | pred_on_unseen_is_unseen), \"Some predictions uncovered!\"\n",
        "print(\"\u2713 All predictions fall into exactly one label set.\")\n",
        "\n",
        "# Counts\n",
        "count_ss = pred_on_seen_is_seen.sum()      # True Seen, Pred Seen\n",
        "count_su = pred_on_seen_is_unseen.sum()    # True Seen, Pred Unseen\n",
        "count_us = pred_on_unseen_is_seen.sum()    # True Unseen, Pred Seen\n",
        "count_uu = pred_on_unseen_is_unseen.sum()  # True Unseen, Pred Unseen\n",
        "\n",
        "total_seen = len(y_pred_seen_fixed)\n",
        "total_unseen = len(y_pred_unseen_fixed)\n",
        "\n",
        "# Verify\n",
        "assert count_ss + count_su == total_seen, \"Row 1 mismatch\"\n",
        "assert count_us + count_uu == total_unseen, \"Row 2 mismatch\"\n",
        "print(f\"\u2713 Row totals verified: seen={total_seen}, unseen={total_unseen}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PRINT BIAS TABLE\n",
        "# =============================================================================\n",
        "\n",
        "pct_ss = 100 * count_ss / total_seen\n",
        "pct_su = 100 * count_su / total_seen\n",
        "pct_us = 100 * count_us / total_unseen\n",
        "pct_uu = 100 * count_uu / total_unseen\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"2\u00d72 BIAS TABLE (FIXED LABELS)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\")\n",
        "print(\"RAW COUNTS:\")\n",
        "print(f\"                    Pred: Seen    Pred: Unseen    Total\")\n",
        "print(f\"True: Seen          {count_ss:10d}    {count_su:12d}    {total_seen:6d}\")\n",
        "print(f\"True: Unseen        {count_us:10d}    {count_uu:12d}    {total_unseen:6d}\")\n",
        "print(\"\")\n",
        "print(\"ROW-NORMALISED PERCENTAGES:\")\n",
        "print(f\"                    Pred: Seen    Pred: Unseen\")\n",
        "print(f\"True: Seen          {pct_ss:10.1f}%    {pct_su:11.1f}%\")\n",
        "print(f\"True: Unseen        {pct_us:10.1f}%    {pct_uu:11.1f}%\")\n",
        "print(\"\")\n",
        "print(\"Interpretation:\")\n",
        "print(f\"  - {pct_ss:.1f}% of seen samples correctly predicted as seen\")\n",
        "print(f\"  - {pct_uu:.1f}% of unseen samples correctly predicted as unseen\")\n",
        "print(f\"  - {pct_us:.1f}% of unseen samples misrouted to seen (seen-bias)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# BIAS TABLE HEATMAP\n",
        "# =============================================================================\n",
        "\n",
        "bias_counts = np.array([[count_ss, count_su], [count_us, count_uu]])\n",
        "bias_pct = np.array([[pct_ss, pct_su], [pct_us, pct_uu]])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Counts\n",
        "im1 = axes[0].imshow(bias_counts, cmap='Blues')\n",
        "axes[0].set_xticks([0, 1])\n",
        "axes[0].set_yticks([0, 1])\n",
        "axes[0].set_xticklabels(['Pred: Seen', 'Pred: Unseen'])\n",
        "axes[0].set_yticklabels(['True: Seen', 'True: Unseen'])\n",
        "axes[0].set_title('Bias Table (Counts)', fontsize=12)\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        color = 'white' if bias_counts[i,j] > bias_counts.max()/2 else 'black'\n",
        "        axes[0].text(j, i, f'{bias_counts[i,j]:,}', ha='center', va='center', color=color, fontsize=14)\n",
        "\n",
        "# Percentages\n",
        "im2 = axes[1].imshow(bias_pct, cmap='Oranges', vmin=0, vmax=100)\n",
        "axes[1].set_xticks([0, 1])\n",
        "axes[1].set_yticks([0, 1])\n",
        "axes[1].set_xticklabels(['Pred: Seen', 'Pred: Unseen'])\n",
        "axes[1].set_yticklabels(['True: Seen', 'True: Unseen'])\n",
        "axes[1].set_title('Bias Table (Row %)', fontsize=12)\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        color = 'white' if bias_pct[i,j] > 50 else 'black'\n",
        "        axes[1].text(j, i, f'{bias_pct[i,j]:.1f}%', ha='center', va='center', color=color, fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/bias_table_full_model.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: figures/bias_table_full_model.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CORRECTED ABLATION TABLE\n",
        "# =============================================================================\n",
        "\n",
        "# Method A: Baseline (raw EEG) \u2014 already computed, use baseline_gzsl_results\n",
        "# Method D: Full GZSL (fixed) \u2014 just computed above\n",
        "\n",
        "# For ablation, we need to also run Methods B and C with correct labels\n",
        "# But those don't involve synthetic unseen, so they remain valid\n",
        "\n",
        "# Create ablation table\n",
        "ablation_fixed = [\n",
        "    {\n",
        "        'Method': 'Raw EEG LR (A)',\n",
        "        'Acc_seen': baseline_gzsl_results['acc_seen'],\n",
        "        'Acc_unseen': baseline_gzsl_results['acc_unseen'],\n",
        "        'H': baseline_gzsl_results['H'],\n",
        "        'MacroF1_seen': baseline_gzsl_results['macro_f1_seen'],\n",
        "        'MacroF1_unseen': baseline_gzsl_results['macro_f1_unseen']\n",
        "    },\n",
        "    {\n",
        "        'Method': 'CLIP + cWGAN-GP + LR (D) [FIXED]',\n",
        "        'Acc_seen': acc_seen_fixed,\n",
        "        'Acc_unseen': acc_unseen_fixed,\n",
        "        'H': H_fixed,\n",
        "        'MacroF1_seen': f1_seen_fixed,\n",
        "        'MacroF1_unseen': f1_unseen_fixed\n",
        "    }\n",
        "]\n",
        "\n",
        "df_ablation_fixed = pd.DataFrame(ablation_fixed).set_index('Method')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CORRECTED ABLATION TABLE\")\n",
        "print(\"=\" * 70)\n",
        "print(df_ablation_fixed.round(4).to_string())\n",
        "print(\"\")\n",
        "print(f\"Improvement in H: {H_fixed - baseline_gzsl_results['H']:+.4f}\")\n",
        "print(f\"Improvement in Acc_unseen: {acc_unseen_fixed - baseline_gzsl_results['acc_unseen']:+.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CORRECTED ABLATION BAR CHART\n",
        "# =============================================================================\n",
        "\n",
        "methods = ['Baseline (A)', 'GZSL [A+B] Fixed']\n",
        "acc_s = [baseline_gzsl_results['acc_seen'], acc_seen_fixed]\n",
        "acc_u = [baseline_gzsl_results['acc_unseen'], acc_unseen_fixed]\n",
        "H_vals = [baseline_gzsl_results['H'], H_fixed]\n",
        "\n",
        "x = np.arange(len(methods))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars1 = ax.bar(x - width, acc_s, width, label='Acc_seen', color='steelblue')\n",
        "bars2 = ax.bar(x, acc_u, width, label='Acc_unseen', color='darkorange')\n",
        "bars3 = ax.bar(x + width, H_vals, width, label='Harmonic Mean (H)', color='green')\n",
        "\n",
        "for bars in [bars1, bars2, bars3]:\n",
        "    for bar in bars:\n",
        "        h = bar.get_height()\n",
        "        ax.annotate(f'{h:.3f}', xy=(bar.get_x() + bar.get_width()/2, h),\n",
        "                    xytext=(0, 3), textcoords='offset points', ha='center', fontsize=11)\n",
        "\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('GZSL Performance: Baseline vs Fixed Model', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(methods, fontsize=12)\n",
        "ax.legend(fontsize=11)\n",
        "ax.set_ylim(0, max(max(acc_s), max(acc_u), max(H_vals)) * 1.25)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/gzsl_ablation_bar.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: figures/gzsl_ablation_bar.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FINAL SUMMARY\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"LABEL FIX COMPLETE \u2014 FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(\"\")\n",
        "print(\"The label collision has been fixed:\")\n",
        "print(f\"  - Seen labels: 1 to {LABEL_OFFSET}\")\n",
        "print(f\"  - Unseen labels (remapped): {LABEL_OFFSET+1} to {LABEL_OFFSET + len(unseen_set)}\")\n",
        "print(f\"  - Total classes in GZSL classifier: {len(clf_gzsl_fixed.classes_)}\")\n",
        "print(\"\")\n",
        "print(\"GZSL [A+B] Results (with fix):\")\n",
        "print(f\"  Acc_seen:   {acc_seen_fixed:.4f}\")\n",
        "print(f\"  Acc_unseen: {acc_unseen_fixed:.4f}\")\n",
        "print(f\"  H:          {H_fixed:.4f}\")\n",
        "print(\"\")\n",
        "print(\"Figures saved:\")\n",
        "print(\"  - figures/bias_table_full_model.png\")\n",
        "print(\"  - figures/gzsl_ablation_bar.png\")\n",
        "print(\"\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Complete Ablation Study (Methods A-D)\n",
        "\n",
        "Computing all four ablation methods with corrected label handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPLETE ABLATION: ALL METHODS A-D\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# We need raw EEG data for Method A\n",
        "# Recreate train/test split from raw data\n",
        "brain_seen_np = brain_seen.numpy()\n",
        "label_seen_np = label_seen.numpy().flatten()\n",
        "brain_unseen_np = brain_unseen.numpy()\n",
        "label_unseen_np = label_unseen.numpy().flatten()\n",
        "\n",
        "indices_seen = np.arange(len(brain_seen_np))\n",
        "train_idx, test_idx = train_test_split(\n",
        "    indices_seen, test_size=0.2, random_state=SEED, stratify=label_seen_np\n",
        ")\n",
        "\n",
        "X_train_raw = brain_seen_np[train_idx]\n",
        "X_test_raw = brain_seen_np[test_idx]\n",
        "y_train_raw = label_seen_np[train_idx]\n",
        "y_test_raw = label_seen_np[test_idx]\n",
        "X_unseen_raw = brain_unseen_np\n",
        "y_unseen_raw = label_unseen_np\n",
        "\n",
        "# Remap unseen raw labels for consistent comparison\n",
        "y_unseen_raw_remapped = y_unseen_raw + LABEL_OFFSET\n",
        "\n",
        "print(\"Raw EEG data prepared for ablation.\")\n",
        "print(f\"  Train: {len(X_train_raw)}, Test: {len(X_test_raw)}, Unseen: {len(X_unseen_raw)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# METHOD A: RAW EEG + LR (BASELINE)\n",
        "# =============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"METHOD A: Raw EEG + LR (Baseline)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "scaler_a = StandardScaler()\n",
        "X_train_a = scaler_a.fit_transform(X_train_raw)\n",
        "X_test_a = scaler_a.transform(X_test_raw)\n",
        "X_unseen_a = scaler_a.transform(X_unseen_raw)\n",
        "\n",
        "clf_a = LogisticRegression(\n",
        "    multi_class='multinomial', solver='lbfgs', max_iter=1000,\n",
        "    random_state=SEED, n_jobs=-1\n",
        ")\n",
        "clf_a.fit(X_train_a, y_train_raw)\n",
        "\n",
        "y_pred_seen_a = clf_a.predict(X_test_a)\n",
        "y_pred_unseen_a = clf_a.predict(X_unseen_a)\n",
        "\n",
        "# Note: Baseline can only predict seen labels, so unseen accuracy will be ~0\n",
        "acc_seen_a = accuracy_score(y_test_raw, y_pred_seen_a)\n",
        "f1_seen_a = f1_score(y_test_raw, y_pred_seen_a, average='macro', zero_division=0)\n",
        "\n",
        "# For unseen, compare against remapped labels (but baseline only predicts seen)\n",
        "acc_unseen_a = accuracy_score(y_unseen_raw_remapped, y_pred_unseen_a)\n",
        "f1_unseen_a = f1_score(y_unseen_raw_remapped, y_pred_unseen_a, average='macro', zero_division=0)\n",
        "\n",
        "H_a = 2 * acc_seen_a * acc_unseen_a / (acc_seen_a + acc_unseen_a) if (acc_seen_a + acc_unseen_a) > 0 else 0\n",
        "\n",
        "print(f\"Acc_seen: {acc_seen_a:.4f}, Acc_unseen: {acc_unseen_a:.4f}, H: {H_a:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# METHOD B: CLIP PROTOTYPE RETRIEVAL\n",
        "# =============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"METHOD B: CLIP Prototype Retrieval\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load prototypes\n",
        "S_seen_array = np.load('cached_arrays/S_seen_prototypes.npy')\n",
        "S_unseen_array = np.load('cached_arrays/S_unseen_prototypes.npy')\n",
        "seen_classes_cached = np.load('cached_arrays/seen_classes.npy')\n",
        "unseen_classes_cached = np.load('cached_arrays/unseen_classes.npy')\n",
        "\n",
        "# Remap unseen class ids to match our label convention\n",
        "unseen_classes_remapped = unseen_classes_cached + LABEL_OFFSET\n",
        "\n",
        "def prototype_retrieval(embeddings, prototypes, class_list):\n",
        "    \"\"\"Predict class by argmax cosine similarity.\"\"\"\n",
        "    sims = np.dot(embeddings, prototypes.T)\n",
        "    pred_indices = np.argmax(sims, axis=1)\n",
        "    return class_list[pred_indices]\n",
        "\n",
        "# Split-wise retrieval\n",
        "y_pred_seen_b = prototype_retrieval(E_test_seen, S_seen_array, seen_classes_cached)\n",
        "y_pred_unseen_b = prototype_retrieval(E_unseen, S_unseen_array, unseen_classes_remapped)\n",
        "\n",
        "acc_seen_b = accuracy_score(y_test_seen, y_pred_seen_b)\n",
        "f1_seen_b = f1_score(y_test_seen, y_pred_seen_b, average='macro', zero_division=0)\n",
        "acc_unseen_b = accuracy_score(y_unseen_remapped, y_pred_unseen_b)\n",
        "f1_unseen_b = f1_score(y_unseen_remapped, y_pred_unseen_b, average='macro', zero_division=0)\n",
        "\n",
        "H_b = 2 * acc_seen_b * acc_unseen_b / (acc_seen_b + acc_unseen_b) if (acc_seen_b + acc_unseen_b) > 0 else 0\n",
        "\n",
        "print(f\"Acc_seen: {acc_seen_b:.4f}, Acc_unseen: {acc_unseen_b:.4f}, H: {H_b:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# METHOD C: CLIP EMBEDDING + LR (SEEN-ONLY)\n",
        "# =============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"METHOD C: CLIP Embedding + LR (seen-only)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "clf_c = LogisticRegression(\n",
        "    multi_class='multinomial', solver='lbfgs', max_iter=1000,\n",
        "    random_state=SEED, n_jobs=-1\n",
        ")\n",
        "clf_c.fit(E_train_seen, y_train_seen)\n",
        "\n",
        "y_pred_seen_c = clf_c.predict(E_test_seen)\n",
        "y_pred_unseen_c = clf_c.predict(E_unseen)\n",
        "\n",
        "acc_seen_c = accuracy_score(y_test_seen, y_pred_seen_c)\n",
        "f1_seen_c = f1_score(y_test_seen, y_pred_seen_c, average='macro', zero_division=0)\n",
        "\n",
        "# Unseen accuracy (model only knows seen labels, so expect ~0)\n",
        "acc_unseen_c = accuracy_score(y_unseen_remapped, y_pred_unseen_c)\n",
        "f1_unseen_c = f1_score(y_unseen_remapped, y_pred_unseen_c, average='macro', zero_division=0)\n",
        "\n",
        "H_c = 2 * acc_seen_c * acc_unseen_c / (acc_seen_c + acc_unseen_c) if (acc_seen_c + acc_unseen_c) > 0 else 0\n",
        "\n",
        "print(f\"Acc_seen: {acc_seen_c:.4f}, Acc_unseen: {acc_unseen_c:.4f}, H: {H_c:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# METHOD D: FULL GZSL (CLIP + cWGAN-GP + LR) \u2014 Already computed above\n",
        "# =============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"METHOD D: CLIP + cWGAN-GP + LR (Full GZSL)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use the already-computed results from clf_gzsl_fixed\n",
        "acc_seen_d = acc_seen_fixed\n",
        "acc_unseen_d = acc_unseen_fixed\n",
        "H_d = H_fixed\n",
        "f1_seen_d = f1_seen_fixed\n",
        "f1_unseen_d = f1_unseen_fixed\n",
        "\n",
        "print(f\"Acc_seen: {acc_seen_d:.4f}, Acc_unseen: {acc_unseen_d:.4f}, H: {H_d:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPLETE ABLATION TABLE (A-D)\n",
        "# =============================================================================\n",
        "\n",
        "ablation_all = [\n",
        "    {'Method': 'A: Raw EEG + LR', 'Acc_seen': acc_seen_a, 'Acc_unseen': acc_unseen_a, \n",
        "     'H': H_a, 'MacroF1_seen': f1_seen_a, 'MacroF1_unseen': f1_unseen_a},\n",
        "    {'Method': 'B: CLIP Prototype', 'Acc_seen': acc_seen_b, 'Acc_unseen': acc_unseen_b,\n",
        "     'H': H_b, 'MacroF1_seen': f1_seen_b, 'MacroF1_unseen': f1_unseen_b},\n",
        "    {'Method': 'C: CLIP + LR (seen)', 'Acc_seen': acc_seen_c, 'Acc_unseen': acc_unseen_c,\n",
        "     'H': H_c, 'MacroF1_seen': f1_seen_c, 'MacroF1_unseen': f1_unseen_c},\n",
        "    {'Method': 'D: CLIP + GAN + LR', 'Acc_seen': acc_seen_d, 'Acc_unseen': acc_unseen_d,\n",
        "     'H': H_d, 'MacroF1_seen': f1_seen_d, 'MacroF1_unseen': f1_unseen_d},\n",
        "]\n",
        "\n",
        "df_ablation_all = pd.DataFrame(ablation_all).set_index('Method')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPLETE ABLATION TABLE (METHODS A-D)\")\n",
        "print(\"=\"*80)\n",
        "print(df_ablation_all.round(4).to_string())\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPLETE ABLATION BAR CHART (A-D)\n",
        "# =============================================================================\n",
        "\n",
        "methods = ['A: Raw EEG', 'B: CLIP Proto', 'C: CLIP+LR', 'D: Full GZSL']\n",
        "acc_seen_all = [acc_seen_a, acc_seen_b, acc_seen_c, acc_seen_d]\n",
        "acc_unseen_all = [acc_unseen_a, acc_unseen_b, acc_unseen_c, acc_unseen_d]\n",
        "H_all = [H_a, H_b, H_c, H_d]\n",
        "\n",
        "x = np.arange(len(methods))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "bars1 = ax.bar(x - width, acc_seen_all, width, label='Acc_seen', color='steelblue')\n",
        "bars2 = ax.bar(x, acc_unseen_all, width, label='Acc_unseen', color='darkorange')\n",
        "bars3 = ax.bar(x + width, H_all, width, label='Harmonic Mean (H)', color='green')\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars1, bars2, bars3]:\n",
        "    for bar in bars:\n",
        "        h = bar.get_height()\n",
        "        ax.annotate(f'{h:.3f}', xy=(bar.get_x() + bar.get_width()/2, h),\n",
        "                    xytext=(0, 3), textcoords='offset points', ha='center', fontsize=9)\n",
        "\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('Ablation Study: GZSL Performance (Methods A-D)', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(methods, fontsize=11)\n",
        "ax.legend(fontsize=11)\n",
        "ax.set_ylim(0, max(max(acc_seen_all), max(acc_unseen_all), max(H_all)) * 1.3)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/gzsl_ablation_bar.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"\\nSaved: figures/gzsl_ablation_bar.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# COMPONENT CONTRIBUTION ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPONENT CONTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\")\n",
        "\n",
        "# CLIP representation gain (C vs A)\n",
        "print(\"1. CLIP Representation (Method C vs A):\")\n",
        "print(f\"   \u0394 Acc_seen = {acc_seen_c - acc_seen_a:+.4f}\")\n",
        "print(f\"   \u2192 CLIP embeddings {'improve' if acc_seen_c > acc_seen_a else 'do not improve'} seen-class discrimination.\")\n",
        "print(\"\")\n",
        "\n",
        "# Prototype retrieval (B)\n",
        "print(\"2. Zero-shot with Prototypes (Method B):\")\n",
        "print(f\"   Acc_unseen (proto retrieval) = {acc_unseen_b:.4f}\")\n",
        "print(f\"   \u2192 CLIP alignment enables {acc_unseen_b*100:.1f}% unseen accuracy via nearest prototype.\")\n",
        "print(\"\")\n",
        "\n",
        "# GAN synthesis gain (D vs C)\n",
        "print(\"3. cWGAN-GP Synthesis (Method D vs C):\")\n",
        "print(f\"   \u0394 Acc_unseen = {acc_unseen_d - acc_unseen_c:+.4f}\")\n",
        "print(f\"   \u2192 Synthetic embeddings {'enable' if acc_unseen_d > acc_unseen_c else 'do not enable'} zero-shot transfer via classifier.\")\n",
        "print(\"\")\n",
        "\n",
        "# Overall improvement (D vs A)\n",
        "print(\"4. Overall Improvement (Method D vs A):\")\n",
        "print(f\"   \u0394 H = {H_d - H_a:+.4f}\")\n",
        "print(f\"   \u2192 Full model {'achieves' if H_d > H_a else 'does not achieve'} better GZSL performance.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "k-kvTFW_3_FH",
        "KPAlCPa5fTtY",
        "-N9UtHCl8SOu"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}