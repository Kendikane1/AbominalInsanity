#!/usr/bin/env python3
"""
Script to add CLIP encoder and cWGAN-GP cells to the notebook.

Run this script from the project directory:
    python add_clip_wgan_cells.py
"""

import json
import os

NOTEBOOK_PATH = "COMP2261_ArizMLCW_with_baseline.ipynb"
OUTPUT_PATH = "COMP2261_ArizMLCW_with_baseline.ipynb"

NEW_CELLS = [
    # =========================================================================
    # SECTION: CLIP ENCODER
    # =========================================================================
    
    # Cell 1: Markdown - CLIP Header
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "---\n",
            "\n",
            "# Brain-Text CLIP Encoder\n",
            "\n",
            "This section implements a **CLIP-style contrastive encoder** that aligns EEG embeddings and text embeddings in a shared semantic space.\n",
            "\n",
            "**Architecture:**\n",
            "- Brain Encoder `f_b`: 561 → 1024 → 512 → d (default d=64)\n",
            "- Text Projector `g`: 512 → 512 → d\n",
            "- Both outputs are L2-normalised\n",
            "\n",
            "**Loss:** Symmetric InfoNCE / CLIP contrastive loss\n",
            "\n",
            "**Training data:** Seen EEG trials only (no leakage from unseen)"
        ]
    },
    
    # Cell 2: Code - CLIP Config
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# CLIP CONFIGURATION\n",
            "# =============================================================================\n",
            "# All hyperparameters in one place for reproducibility\n",
            "\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "from torch.utils.data import DataLoader, TensorDataset\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.manifold import TSNE\n",
            "import os\n",
            "import random\n",
            "\n",
            "# Configuration dictionary\n",
            "CLIP_CONFIG = {\n",
            "    'embed_dim': 64,           # Output embedding dimension d\n",
            "    'tau': 0.07,               # Temperature for contrastive loss\n",
            "    'epochs': 20,              # Training epochs\n",
            "    'batch_size': 256,         # Batch size\n",
            "    'lr': 1e-4,                # Learning rate\n",
            "    'weight_decay': 1e-4,      # AdamW weight decay\n",
            "    'dropout': 0.1,            # Dropout rate\n",
            "    'seed': 42,                # Random seed\n",
            "}\n",
            "\n",
            "# Set seeds for reproducibility\n",
            "SEED = CLIP_CONFIG['seed']\n",
            "np.random.seed(SEED)\n",
            "torch.manual_seed(SEED)\n",
            "random.seed(SEED)\n",
            "if torch.cuda.is_available():\n",
            "    torch.cuda.manual_seed_all(SEED)\n",
            "\n",
            "# Device\n",
            "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "\n",
            "print(\"=\" * 60)\n",
            "print(\"CLIP CONFIGURATION\")\n",
            "print(\"=\" * 60)\n",
            "for k, v in CLIP_CONFIG.items():\n",
            "    print(f\"  {k}: {v}\")\n",
            "print(f\"  device: {device}\")\n",
            "print(\"=\" * 60)"
        ]
    },
    
    # Cell 3: Code - CLIP Model Definitions
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# CLIP MODEL DEFINITIONS\n",
            "# =============================================================================\n",
            "\n",
            "class BrainEncoder(nn.Module):\n",
            "    \"\"\"EEG feature encoder: 561 -> d with L2 normalisation.\"\"\"\n",
            "    \n",
            "    def __init__(self, input_dim=561, hidden_dims=[1024, 512], embed_dim=64, dropout=0.1):\n",
            "        super().__init__()\n",
            "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
            "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
            "        self.fc3 = nn.Linear(hidden_dims[1], embed_dim)\n",
            "        self.dropout = nn.Dropout(dropout)\n",
            "        \n",
            "    def forward(self, x):\n",
            "        x = F.relu(self.fc1(x))\n",
            "        x = self.dropout(x)\n",
            "        x = F.relu(self.fc2(x))\n",
            "        x = self.dropout(x)\n",
            "        x = self.fc3(x)\n",
            "        # L2 normalisation\n",
            "        x = F.normalize(x, p=2, dim=-1)\n",
            "        return x\n",
            "\n",
            "\n",
            "class TextProjector(nn.Module):\n",
            "    \"\"\"Text embedding projector: 512 -> d with L2 normalisation.\"\"\"\n",
            "    \n",
            "    def __init__(self, input_dim=512, hidden_dim=512, embed_dim=64):\n",
            "        super().__init__()\n",
            "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
            "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
            "        \n",
            "    def forward(self, x):\n",
            "        x = F.relu(self.fc1(x))\n",
            "        x = self.fc2(x)\n",
            "        # L2 normalisation\n",
            "        x = F.normalize(x, p=2, dim=-1)\n",
            "        return x\n",
            "\n",
            "\n",
            "def clip_loss(brain_embeds, text_embeds, tau=0.07):\n",
            "    \"\"\"\n",
            "    Symmetric CLIP/InfoNCE contrastive loss.\n",
            "    \n",
            "    Args:\n",
            "        brain_embeds: (B, d) L2-normalised brain embeddings\n",
            "        text_embeds: (B, d) L2-normalised text embeddings\n",
            "        tau: temperature parameter\n",
            "    \n",
            "    Returns:\n",
            "        Scalar loss: (L_e2t + L_t2e) / 2\n",
            "    \"\"\"\n",
            "    # Similarity matrix (dot product = cosine sim due to L2 norm)\n",
            "    logits = torch.matmul(brain_embeds, text_embeds.T) / tau  # (B, B)\n",
            "    \n",
            "    # Targets: diagonal (i.e., matched pairs)\n",
            "    batch_size = brain_embeds.size(0)\n",
            "    targets = torch.arange(batch_size, device=brain_embeds.device)\n",
            "    \n",
            "    # Cross-entropy in both directions\n",
            "    loss_e2t = F.cross_entropy(logits, targets)  # brain -> text\n",
            "    loss_t2e = F.cross_entropy(logits.T, targets)  # text -> brain\n",
            "    \n",
            "    return (loss_e2t + loss_t2e) / 2\n",
            "\n",
            "\n",
            "# Instantiate models\n",
            "brain_encoder = BrainEncoder(\n",
            "    input_dim=561,\n",
            "    embed_dim=CLIP_CONFIG['embed_dim'],\n",
            "    dropout=CLIP_CONFIG['dropout']\n",
            ").to(device)\n",
            "\n",
            "text_projector = TextProjector(\n",
            "    input_dim=512,\n",
            "    embed_dim=CLIP_CONFIG['embed_dim']\n",
            ").to(device)\n",
            "\n",
            "print(f\"Brain Encoder parameters: {sum(p.numel() for p in brain_encoder.parameters()):,}\")\n",
            "print(f\"Text Projector parameters: {sum(p.numel() for p in text_projector.parameters()):,}\")"
        ]
    },
    
    # Cell 4: Code - CLIP Data Preparation
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# CLIP DATA PREPARATION\n",
            "# =============================================================================\n",
            "# Use only seen train split for CLIP training (no leakage!)\n",
            "\n",
            "# The baseline already created X_train, X_test using train_test_split\n",
            "# We need to also get the corresponding text embeddings\n",
            "\n",
            "# Get indices for train/test split (recreate with same seed)\n",
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "# Recreate the split indices\n",
            "indices_seen = np.arange(len(brain_seen))\n",
            "train_idx, test_idx = train_test_split(\n",
            "    indices_seen, test_size=0.2, random_state=42, stratify=label_seen.numpy().flatten()\n",
            ")\n",
            "\n",
            "# Get brain and text for train/test\n",
            "brain_train_seen = brain_seen[train_idx].numpy()\n",
            "brain_test_seen_np = brain_seen[test_idx].numpy()\n",
            "text_train_seen = text_seen[train_idx].numpy()\n",
            "text_test_seen = text_seen[test_idx].numpy()\n",
            "label_train_seen = label_seen[train_idx].numpy().flatten()\n",
            "label_test_seen = label_seen[test_idx].numpy().flatten()\n",
            "\n",
            "# Standardise brain features (fit on train only)\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "\n",
            "clip_scaler = StandardScaler()\n",
            "brain_train_seen_scaled = clip_scaler.fit_transform(brain_train_seen)\n",
            "brain_test_seen_scaled = clip_scaler.transform(brain_test_seen_np)\n",
            "brain_unseen_scaled = clip_scaler.transform(brain_unseen.numpy())\n",
            "\n",
            "# Convert to tensors\n",
            "X_train_tensor = torch.FloatTensor(brain_train_seen_scaled)\n",
            "T_train_tensor = torch.FloatTensor(text_train_seen)\n",
            "Y_train_tensor = torch.LongTensor(label_train_seen)\n",
            "\n",
            "X_test_tensor = torch.FloatTensor(brain_test_seen_scaled)\n",
            "T_test_tensor = torch.FloatTensor(text_test_seen)\n",
            "Y_test_tensor = torch.LongTensor(label_test_seen)\n",
            "\n",
            "X_unseen_tensor = torch.FloatTensor(brain_unseen_scaled)\n",
            "T_unseen_tensor = torch.FloatTensor(text_unseen.numpy())\n",
            "Y_unseen_tensor = torch.LongTensor(label_unseen.numpy().flatten())\n",
            "\n",
            "# Create DataLoader for training\n",
            "train_dataset = TensorDataset(X_train_tensor, T_train_tensor, Y_train_tensor)\n",
            "train_loader = DataLoader(train_dataset, batch_size=CLIP_CONFIG['batch_size'], shuffle=True)\n",
            "\n",
            "print(f\"CLIP Training data: {len(X_train_tensor)} samples\")\n",
            "print(f\"CLIP Test data (seen): {len(X_test_tensor)} samples\")\n",
            "print(f\"Unseen data (test only): {len(X_unseen_tensor)} samples\")"
        ]
    },
    
    # Cell 5: Code - CLIP Training Loop
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# CLIP TRAINING\n",
            "# =============================================================================\n",
            "\n",
            "# Optimizer\n",
            "clip_params = list(brain_encoder.parameters()) + list(text_projector.parameters())\n",
            "clip_optimizer = torch.optim.AdamW(\n",
            "    clip_params,\n",
            "    lr=CLIP_CONFIG['lr'],\n",
            "    weight_decay=CLIP_CONFIG['weight_decay']\n",
            ")\n",
            "\n",
            "# Training loop\n",
            "clip_losses = []\n",
            "print(\"Training CLIP encoder...\")\n",
            "print(f\"Epochs: {CLIP_CONFIG['epochs']}, Batches per epoch: {len(train_loader)}\")\n",
            "\n",
            "for epoch in range(CLIP_CONFIG['epochs']):\n",
            "    brain_encoder.train()\n",
            "    text_projector.train()\n",
            "    epoch_loss = 0.0\n",
            "    \n",
            "    for batch_idx, (brain_batch, text_batch, _) in enumerate(train_loader):\n",
            "        brain_batch = brain_batch.to(device)\n",
            "        text_batch = text_batch.to(device)\n",
            "        \n",
            "        # Forward pass\n",
            "        brain_embeds = brain_encoder(brain_batch)\n",
            "        text_embeds = text_projector(text_batch)\n",
            "        \n",
            "        # Compute loss\n",
            "        loss = clip_loss(brain_embeds, text_embeds, tau=CLIP_CONFIG['tau'])\n",
            "        \n",
            "        # Backward pass\n",
            "        clip_optimizer.zero_grad()\n",
            "        loss.backward()\n",
            "        clip_optimizer.step()\n",
            "        \n",
            "        epoch_loss += loss.item()\n",
            "    \n",
            "    avg_loss = epoch_loss / len(train_loader)\n",
            "    clip_losses.append(avg_loss)\n",
            "    \n",
            "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
            "        print(f\"  Epoch {epoch+1:3d}/{CLIP_CONFIG['epochs']}: Loss = {avg_loss:.4f}\")\n",
            "\n",
            "print(\"\\nCLIP training complete!\")\n",
            "print(f\"Final loss: {clip_losses[-1]:.4f}\")"
        ]
    },
    
    # Cell 6: Code - Save CLIP Loss Curve
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# CLIP LOSS CURVE\n",
            "# =============================================================================\n",
            "\n",
            "plt.figure(figsize=(10, 5))\n",
            "plt.plot(range(1, len(clip_losses)+1), clip_losses, 'b-', linewidth=2)\n",
            "plt.xlabel('Epoch', fontsize=12)\n",
            "plt.ylabel('CLIP Contrastive Loss', fontsize=12)\n",
            "plt.title('CLIP Encoder Training Loss', fontsize=14)\n",
            "plt.grid(True, alpha=0.3)\n",
            "plt.tight_layout()\n",
            "plt.savefig('figures/clip_loss_curve.png', dpi=150, bbox_inches='tight')\n",
            "plt.show()\n",
            "\n",
            "print(f\"Saved: figures/clip_loss_curve.png\")"
        ]
    },
    
    # Cell 7: Code - Compute and Cache CLIP Embeddings
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# COMPUTE AND CACHE CLIP EMBEDDINGS\n",
            "# =============================================================================\n",
            "\n",
            "brain_encoder.eval()\n",
            "text_projector.eval()\n",
            "\n",
            "with torch.no_grad():\n",
            "    # Brain embeddings\n",
            "    E_train_seen = brain_encoder(X_train_tensor.to(device)).cpu().numpy()\n",
            "    E_test_seen = brain_encoder(X_test_tensor.to(device)).cpu().numpy()\n",
            "    E_unseen = brain_encoder(X_unseen_tensor.to(device)).cpu().numpy()\n",
            "    \n",
            "    # Text embeddings for prototypes\n",
            "    T_train_embeds = text_projector(T_train_tensor.to(device)).cpu().numpy()\n",
            "    T_test_embeds = text_projector(T_test_tensor.to(device)).cpu().numpy()\n",
            "    T_unseen_embeds = text_projector(T_unseen_tensor.to(device)).cpu().numpy()\n",
            "\n",
            "print(f\"E_train_seen shape: {E_train_seen.shape}\")\n",
            "print(f\"E_test_seen shape: {E_test_seen.shape}\")\n",
            "print(f\"E_unseen shape: {E_unseen.shape}\")"
        ]
    },
    
    # Cell 8: Code - Compute Semantic Prototypes
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# COMPUTE SEMANTIC PROTOTYPES (CLASS CENTROIDS IN TEXT SPACE)\n",
            "# =============================================================================\n",
            "\n",
            "def compute_prototypes(embeddings, labels):\n",
            "    \"\"\"\n",
            "    Compute class prototypes as mean of embeddings per class, then L2-normalise.\n",
            "    \n",
            "    Returns:\n",
            "        prototypes: dict mapping class_id -> prototype vector\n",
            "    \"\"\"\n",
            "    unique_labels = np.unique(labels)\n",
            "    prototypes = {}\n",
            "    \n",
            "    for c in unique_labels:\n",
            "        mask = labels == c\n",
            "        class_embeds = embeddings[mask]\n",
            "        # Mean then normalise\n",
            "        proto = class_embeds.mean(axis=0)\n",
            "        proto = proto / (np.linalg.norm(proto) + 1e-8)\n",
            "        prototypes[c] = proto\n",
            "    \n",
            "    return prototypes\n",
            "\n",
            "# Compute prototypes for seen classes (from training text embeddings)\n",
            "S_seen_prototypes = compute_prototypes(T_train_embeds, label_train_seen)\n",
            "\n",
            "# Compute prototypes for unseen classes (from unseen text embeddings)\n",
            "S_unseen_prototypes = compute_prototypes(T_unseen_embeds, Y_unseen_tensor.numpy())\n",
            "\n",
            "print(f\"Seen class prototypes: {len(S_seen_prototypes)} classes\")\n",
            "print(f\"Unseen class prototypes: {len(S_unseen_prototypes)} classes\")\n",
            "\n",
            "# Convert to arrays for easier use\n",
            "seen_classes = sorted(S_seen_prototypes.keys())\n",
            "unseen_classes = sorted(S_unseen_prototypes.keys())\n",
            "\n",
            "S_seen_array = np.array([S_seen_prototypes[c] for c in seen_classes])  # (n_seen_classes, d)\n",
            "S_unseen_array = np.array([S_unseen_prototypes[c] for c in unseen_classes])  # (n_unseen_classes, d)\n",
            "\n",
            "print(f\"S_seen_array shape: {S_seen_array.shape}\")\n",
            "print(f\"S_unseen_array shape: {S_unseen_array.shape}\")"
        ]
    },
    
    # Cell 9: Code - Save Cached Arrays
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# CACHE EMBEDDINGS AND PROTOTYPES TO DISK\n",
            "# =============================================================================\n",
            "\n",
            "os.makedirs('cached_arrays', exist_ok=True)\n",
            "\n",
            "# Save embeddings\n",
            "np.save('cached_arrays/E_train_seen.npy', E_train_seen)\n",
            "np.save('cached_arrays/E_test_seen.npy', E_test_seen)\n",
            "np.save('cached_arrays/E_unseen.npy', E_unseen)\n",
            "\n",
            "# Save labels\n",
            "np.save('cached_arrays/y_train_seen.npy', label_train_seen)\n",
            "np.save('cached_arrays/y_test_seen.npy', label_test_seen)\n",
            "np.save('cached_arrays/y_unseen.npy', Y_unseen_tensor.numpy())\n",
            "\n",
            "# Save prototypes\n",
            "np.save('cached_arrays/S_seen_prototypes.npy', S_seen_array)\n",
            "np.save('cached_arrays/S_unseen_prototypes.npy', S_unseen_array)\n",
            "np.save('cached_arrays/seen_classes.npy', np.array(seen_classes))\n",
            "np.save('cached_arrays/unseen_classes.npy', np.array(unseen_classes))\n",
            "\n",
            "print(\"Cached arrays saved to cached_arrays/:\")\n",
            "for f in os.listdir('cached_arrays'):\n",
            "    print(f\"  - {f}\")"
        ]
    },
    
    # Cell 10: Code - t-SNE Visualization of CLIP Space
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# t-SNE VISUALIZATION OF CLIP EMBEDDING SPACE\n",
            "# =============================================================================\n",
            "\n",
            "print(\"Computing t-SNE (this may take a moment)...\")\n",
            "\n",
            "# Sample subset of test embeddings for visualization\n",
            "n_sample = min(1000, len(E_test_seen))\n",
            "sample_idx = np.random.choice(len(E_test_seen), n_sample, replace=False)\n",
            "E_test_sample = E_test_seen[sample_idx]\n",
            "y_test_sample = label_test_seen[sample_idx]\n",
            "\n",
            "# Combine brain embeddings and prototypes\n",
            "combined = np.vstack([E_test_sample, S_seen_array[:50], S_unseen_array[:50]])  # Limit prototypes for clarity\n",
            "\n",
            "# t-SNE\n",
            "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
            "combined_2d = tsne.fit_transform(combined)\n",
            "\n",
            "# Split back\n",
            "n_brain = len(E_test_sample)\n",
            "n_seen_proto = 50\n",
            "n_unseen_proto = 50\n",
            "\n",
            "brain_2d = combined_2d[:n_brain]\n",
            "seen_proto_2d = combined_2d[n_brain:n_brain+n_seen_proto]\n",
            "unseen_proto_2d = combined_2d[n_brain+n_seen_proto:]\n",
            "\n",
            "# Plot\n",
            "plt.figure(figsize=(12, 8))\n",
            "plt.scatter(brain_2d[:, 0], brain_2d[:, 1], c='steelblue', alpha=0.3, s=10, label='EEG embeddings (seen test)')\n",
            "plt.scatter(seen_proto_2d[:, 0], seen_proto_2d[:, 1], c='green', marker='*', s=100, label='Seen class prototypes')\n",
            "plt.scatter(unseen_proto_2d[:, 0], unseen_proto_2d[:, 1], c='red', marker='^', s=100, label='Unseen class prototypes')\n",
            "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
            "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
            "plt.title('CLIP Embedding Space: EEG Embeddings and Text Prototypes', fontsize=14)\n",
            "plt.legend()\n",
            "plt.tight_layout()\n",
            "plt.savefig('figures/clip_embedding_tsne.png', dpi=150, bbox_inches='tight')\n",
            "plt.show()\n",
            "\n",
            "print(f\"Saved: figures/clip_embedding_tsne.png\")"
        ]
    },
    
    # =========================================================================
    # SECTION: cWGAN-GP
    # =========================================================================
    
    # Cell 11: Markdown - cWGAN-GP Header
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "---\n",
            "\n",
            "# Conditional WGAN-GP for Embedding Synthesis\n",
            "\n",
            "This section implements a **conditional Wasserstein GAN with Gradient Penalty (cWGAN-GP)** to synthesize EEG embeddings for unseen classes.\n",
            "\n",
            "**Goal:** Generate synthetic EEG embeddings `e_hat = G(z, s_c)` conditioned on text prototypes `s_c`.\n",
            "\n",
            "**Architecture:**\n",
            "- Generator: [z, s_c] → e_hat (L2-normalised output)\n",
            "- Critic: [e, s_c] → scalar (no sigmoid)\n",
            "\n",
            "**Training:** Only on seen-class embeddings from CLIP encoder."
        ]
    },
    
    # Cell 12: Code - WGAN Config
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# cWGAN-GP CONFIGURATION\n",
            "# =============================================================================\n",
            "\n",
            "WGAN_CONFIG = {\n",
            "    'z_dim': 100,              # Noise dimension\n",
            "    'embed_dim': 64,           # Must match CLIP embed_dim\n",
            "    'lr': 1e-4,                # Learning rate\n",
            "    'betas': (0.0, 0.9),       # Adam betas for WGAN-GP\n",
            "    'lambda_gp': 10,           # Gradient penalty coefficient\n",
            "    'n_critic': 5,             # Critic updates per generator update\n",
            "    'n_steps': 10000,          # Total generator steps\n",
            "    'batch_size': 256,         # Batch size\n",
            "    'n_synth_per_class': 20,   # Synthetic samples per unseen class\n",
            "    'seed': 42,\n",
            "}\n",
            "\n",
            "print(\"=\" * 60)\n",
            "print(\"cWGAN-GP CONFIGURATION\")\n",
            "print(\"=\" * 60)\n",
            "for k, v in WGAN_CONFIG.items():\n",
            "    print(f\"  {k}: {v}\")\n",
            "print(\"=\" * 60)"
        ]
    },
    
    # Cell 13: Code - WGAN Model Definitions
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# cWGAN-GP MODEL DEFINITIONS\n",
            "# =============================================================================\n",
            "\n",
            "class Generator(nn.Module):\n",
            "    \"\"\"Conditional generator: [z, s_c] -> e_hat.\"\"\"\n",
            "    \n",
            "    def __init__(self, z_dim=100, embed_dim=64):\n",
            "        super().__init__()\n",
            "        input_dim = z_dim + embed_dim  # Concatenate noise and condition\n",
            "        self.net = nn.Sequential(\n",
            "            nn.Linear(input_dim, 256),\n",
            "            nn.LeakyReLU(0.2),\n",
            "            nn.Linear(256, 256),\n",
            "            nn.LeakyReLU(0.2),\n",
            "            nn.Linear(256, embed_dim),\n",
            "        )\n",
            "        \n",
            "    def forward(self, z, s_c):\n",
            "        x = torch.cat([z, s_c], dim=-1)\n",
            "        e_hat = self.net(x)\n",
            "        # L2 normalise output to stay on unit sphere\n",
            "        e_hat = F.normalize(e_hat, p=2, dim=-1)\n",
            "        return e_hat\n",
            "\n",
            "\n",
            "class Critic(nn.Module):\n",
            "    \"\"\"Conditional critic: [e, s_c] -> scalar score.\"\"\"\n",
            "    \n",
            "    def __init__(self, embed_dim=64):\n",
            "        super().__init__()\n",
            "        input_dim = 2 * embed_dim  # Concatenate embedding and condition\n",
            "        self.net = nn.Sequential(\n",
            "            nn.Linear(input_dim, 256),\n",
            "            nn.LeakyReLU(0.2),\n",
            "            nn.Linear(256, 256),\n",
            "            nn.LeakyReLU(0.2),\n",
            "            nn.Linear(256, 1),  # Scalar output, NO sigmoid\n",
            "        )\n",
            "        \n",
            "    def forward(self, e, s_c):\n",
            "        x = torch.cat([e, s_c], dim=-1)\n",
            "        return self.net(x)\n",
            "\n",
            "\n",
            "def compute_gradient_penalty(critic, real_samples, fake_samples, s_c, device):\n",
            "    \"\"\"\n",
            "    Compute gradient penalty for WGAN-GP.\n",
            "    \n",
            "    GP = E[(||∇_{e_bar} D(e_bar, s_c)||_2 - 1)^2]\n",
            "    \"\"\"\n",
            "    batch_size = real_samples.size(0)\n",
            "    \n",
            "    # Random interpolation coefficient\n",
            "    eps = torch.rand(batch_size, 1, device=device)\n",
            "    \n",
            "    # Interpolated samples\n",
            "    e_bar = eps * real_samples + (1 - eps) * fake_samples\n",
            "    e_bar.requires_grad_(True)\n",
            "    \n",
            "    # Critic score on interpolated\n",
            "    d_interpolated = critic(e_bar, s_c)\n",
            "    \n",
            "    # Compute gradients\n",
            "    gradients = torch.autograd.grad(\n",
            "        outputs=d_interpolated,\n",
            "        inputs=e_bar,\n",
            "        grad_outputs=torch.ones_like(d_interpolated),\n",
            "        create_graph=True,\n",
            "        retain_graph=True,\n",
            "    )[0]\n",
            "    \n",
            "    # Compute penalty\n",
            "    gradients = gradients.view(batch_size, -1)\n",
            "    gradient_norm = gradients.norm(2, dim=1)\n",
            "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
            "    \n",
            "    return gradient_penalty\n",
            "\n",
            "\n",
            "# Instantiate models\n",
            "generator = Generator(\n",
            "    z_dim=WGAN_CONFIG['z_dim'],\n",
            "    embed_dim=WGAN_CONFIG['embed_dim']\n",
            ").to(device)\n",
            "\n",
            "critic = Critic(\n",
            "    embed_dim=WGAN_CONFIG['embed_dim']\n",
            ").to(device)\n",
            "\n",
            "print(f\"Generator parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
            "print(f\"Critic parameters: {sum(p.numel() for p in critic.parameters()):,}\")"
        ]
    },
    
    # Cell 14: Code - WGAN Data Preparation
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# cWGAN-GP DATA PREPARATION\n",
            "# =============================================================================\n",
            "# Use only seen embeddings from CLIP encoder\n",
            "\n",
            "# Get semantic prototype for each training sample\n",
            "def get_prototype_for_labels(labels, prototypes_dict):\n",
            "    \"\"\"Get prototype vector for each sample based on its label.\"\"\"\n",
            "    protos = np.array([prototypes_dict[int(l)] for l in labels])\n",
            "    return protos\n",
            "\n",
            "# Training pairs: (e_real, s_c) where s_c is the class prototype\n",
            "E_train_tensor = torch.FloatTensor(E_train_seen)\n",
            "S_train_conditions = torch.FloatTensor(get_prototype_for_labels(label_train_seen, S_seen_prototypes))\n",
            "\n",
            "wgan_dataset = TensorDataset(E_train_tensor, S_train_conditions)\n",
            "wgan_loader = DataLoader(wgan_dataset, batch_size=WGAN_CONFIG['batch_size'], shuffle=True, drop_last=True)\n",
            "\n",
            "print(f\"WGAN training data: {len(wgan_dataset)} samples\")\n",
            "print(f\"Batch size: {WGAN_CONFIG['batch_size']}\")\n",
            "print(f\"Batches per epoch: {len(wgan_loader)}\")"
        ]
    },
    
    # Cell 15: Code - WGAN Training Loop
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# cWGAN-GP TRAINING\n",
            "# =============================================================================\n",
            "\n",
            "# Optimizers\n",
            "g_optimizer = torch.optim.Adam(\n",
            "    generator.parameters(),\n",
            "    lr=WGAN_CONFIG['lr'],\n",
            "    betas=WGAN_CONFIG['betas']\n",
            ")\n",
            "c_optimizer = torch.optim.Adam(\n",
            "    critic.parameters(),\n",
            "    lr=WGAN_CONFIG['lr'],\n",
            "    betas=WGAN_CONFIG['betas']\n",
            ")\n",
            "\n",
            "# Training history\n",
            "g_losses = []\n",
            "c_losses = []\n",
            "gp_values = []\n",
            "\n",
            "print(f\"Training cWGAN-GP for {WGAN_CONFIG['n_steps']} generator steps...\")\n",
            "print(f\"Critic updates per G step: {WGAN_CONFIG['n_critic']}\")\n",
            "\n",
            "data_iter = iter(wgan_loader)\n",
            "g_step = 0\n",
            "\n",
            "while g_step < WGAN_CONFIG['n_steps']:\n",
            "    # -------------------------------------------------------------------------\n",
            "    # Train Critic (n_critic times per generator step)\n",
            "    # -------------------------------------------------------------------------\n",
            "    for _ in range(WGAN_CONFIG['n_critic']):\n",
            "        try:\n",
            "            e_real, s_c = next(data_iter)\n",
            "        except StopIteration:\n",
            "            data_iter = iter(wgan_loader)\n",
            "            e_real, s_c = next(data_iter)\n",
            "        \n",
            "        e_real = e_real.to(device)\n",
            "        s_c = s_c.to(device)\n",
            "        batch_size = e_real.size(0)\n",
            "        \n",
            "        # Generate fake samples\n",
            "        z = torch.randn(batch_size, WGAN_CONFIG['z_dim'], device=device)\n",
            "        e_fake = generator(z, s_c)\n",
            "        \n",
            "        # Critic scores\n",
            "        d_real = critic(e_real, s_c)\n",
            "        d_fake = critic(e_fake.detach(), s_c)\n",
            "        \n",
            "        # Gradient penalty\n",
            "        gp = compute_gradient_penalty(critic, e_real, e_fake.detach(), s_c, device)\n",
            "        \n",
            "        # Critic loss: maximize E[D(real)] - E[D(fake)] - lambda*GP\n",
            "        # Minimizing: -E[D(real)] + E[D(fake)] + lambda*GP\n",
            "        c_loss = -d_real.mean() + d_fake.mean() + WGAN_CONFIG['lambda_gp'] * gp\n",
            "        \n",
            "        c_optimizer.zero_grad()\n",
            "        c_loss.backward()\n",
            "        c_optimizer.step()\n",
            "    \n",
            "    # -------------------------------------------------------------------------\n",
            "    # Train Generator\n",
            "    # -------------------------------------------------------------------------\n",
            "    try:\n",
            "        e_real, s_c = next(data_iter)\n",
            "    except StopIteration:\n",
            "        data_iter = iter(wgan_loader)\n",
            "        e_real, s_c = next(data_iter)\n",
            "    \n",
            "    s_c = s_c.to(device)\n",
            "    batch_size = s_c.size(0)\n",
            "    \n",
            "    z = torch.randn(batch_size, WGAN_CONFIG['z_dim'], device=device)\n",
            "    e_fake = generator(z, s_c)\n",
            "    d_fake = critic(e_fake, s_c)\n",
            "    \n",
            "    # Generator loss: minimize -E[D(fake)]\n",
            "    g_loss = -d_fake.mean()\n",
            "    \n",
            "    g_optimizer.zero_grad()\n",
            "    g_loss.backward()\n",
            "    g_optimizer.step()\n",
            "    \n",
            "    # Log\n",
            "    g_losses.append(g_loss.item())\n",
            "    c_losses.append(c_loss.item())\n",
            "    gp_values.append(gp.item())\n",
            "    \n",
            "    g_step += 1\n",
            "    \n",
            "    if g_step % 1000 == 0 or g_step == 1:\n",
            "        print(f\"  Step {g_step:5d}/{WGAN_CONFIG['n_steps']}: G_loss={g_loss.item():.4f}, C_loss={c_loss.item():.4f}, GP={gp.item():.4f}\")\n",
            "\n",
            "print(\"\\ncWGAN-GP training complete!\")\n",
            "print(f\"Final G_loss: {g_losses[-1]:.4f}\")\n",
            "print(f\"Final C_loss: {c_losses[-1]:.4f}\")"
        ]
    },
    
    # Cell 16: Code - Save WGAN Loss Curves
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# cWGAN-GP LOSS CURVES\n",
            "# =============================================================================\n",
            "\n",
            "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
            "\n",
            "# Generator loss\n",
            "axes[0].plot(g_losses, alpha=0.7)\n",
            "axes[0].set_xlabel('Generator Step')\n",
            "axes[0].set_ylabel('Generator Loss')\n",
            "axes[0].set_title('Generator Loss')\n",
            "axes[0].grid(True, alpha=0.3)\n",
            "\n",
            "# Critic loss\n",
            "axes[1].plot(c_losses, alpha=0.7, color='orange')\n",
            "axes[1].set_xlabel('Generator Step')\n",
            "axes[1].set_ylabel('Critic Loss')\n",
            "axes[1].set_title('Critic Loss')\n",
            "axes[1].grid(True, alpha=0.3)\n",
            "\n",
            "# Gradient penalty\n",
            "axes[2].plot(gp_values, alpha=0.7, color='green')\n",
            "axes[2].set_xlabel('Generator Step')\n",
            "axes[2].set_ylabel('Gradient Penalty')\n",
            "axes[2].set_title('Gradient Penalty')\n",
            "axes[2].grid(True, alpha=0.3)\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.savefig('figures/wgan_losses.png', dpi=150, bbox_inches='tight')\n",
            "plt.show()\n",
            "\n",
            "print(f\"Saved: figures/wgan_losses.png\")"
        ]
    },
    
    # Cell 17: Code - Generate Synthetic Unseen Embeddings
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# GENERATE SYNTHETIC EMBEDDINGS FOR UNSEEN CLASSES\n",
            "# =============================================================================\n",
            "\n",
            "generator.eval()\n",
            "\n",
            "n_synth = WGAN_CONFIG['n_synth_per_class']\n",
            "synth_embeddings = []\n",
            "synth_labels = []\n",
            "\n",
            "print(f\"Generating {n_synth} synthetic embeddings per unseen class...\")\n",
            "\n",
            "with torch.no_grad():\n",
            "    for c in unseen_classes:\n",
            "        # Get prototype for this unseen class\n",
            "        s_c = torch.FloatTensor(S_unseen_prototypes[c]).unsqueeze(0).repeat(n_synth, 1).to(device)\n",
            "        \n",
            "        # Sample noise and generate\n",
            "        z = torch.randn(n_synth, WGAN_CONFIG['z_dim'], device=device)\n",
            "        e_synth = generator(z, s_c).cpu().numpy()\n",
            "        \n",
            "        synth_embeddings.append(e_synth)\n",
            "        synth_labels.extend([c] * n_synth)\n",
            "\n",
            "E_synth_unseen = np.vstack(synth_embeddings)\n",
            "y_synth_unseen = np.array(synth_labels)\n",
            "\n",
            "print(f\"E_synth_unseen shape: {E_synth_unseen.shape}\")\n",
            "print(f\"y_synth_unseen shape: {y_synth_unseen.shape}\")\n",
            "print(f\"Total synthetic samples: {len(y_synth_unseen)}\")"
        ]
    },
    
    # Cell 18: Code - Sanity Check on Synthetic Embeddings
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# SANITY CHECK: SYNTHETIC EMBEDDING QUALITY\n",
            "# =============================================================================\n",
            "\n",
            "print(\"=\" * 60)\n",
            "print(\"SYNTHETIC EMBEDDING QUALITY CHECK\")\n",
            "print(\"=\" * 60)\n",
            "\n",
            "# Check that synthetic embeddings are normalised\n",
            "synth_norms = np.linalg.norm(E_synth_unseen, axis=1)\n",
            "print(f\"Synthetic embedding norms: mean={synth_norms.mean():.4f}, std={synth_norms.std():.6f}\")\n",
            "\n",
            "# Check variance (not collapsed)\n",
            "synth_variance = E_synth_unseen.var(axis=0).mean()\n",
            "real_variance = E_train_seen.var(axis=0).mean()\n",
            "print(f\"Mean per-dim variance: Real={real_variance:.4f}, Synthetic={synth_variance:.4f}\")\n",
            "\n",
            "# Check cosine similarity with prototypes\n",
            "cos_sims = []\n",
            "for i, c in enumerate(unseen_classes[:10]):  # Sample 10 classes\n",
            "    mask = y_synth_unseen == c\n",
            "    synth_c = E_synth_unseen[mask]\n",
            "    proto_c = S_unseen_prototypes[c]\n",
            "    sims = np.dot(synth_c, proto_c)  # cosine sim since both normalised\n",
            "    cos_sims.extend(sims)\n",
            "\n",
            "print(f\"Cosine sim (synth vs proto): mean={np.mean(cos_sims):.4f}, std={np.std(cos_sims):.4f}\")\n",
            "\n",
            "if synth_variance > 0.001:\n",
            "    print(\"\\n✓ Synthetic embeddings have reasonable variance (no mode collapse).\")\n",
            "else:\n",
            "    print(\"\\n⚠ Warning: Low variance in synthetic embeddings.\")"
        ]
    },
    
    # Cell 19: Code - Save Synthetic Embeddings
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# CACHE SYNTHETIC EMBEDDINGS\n",
            "# =============================================================================\n",
            "\n",
            "np.save('cached_arrays/E_synth_unseen.npy', E_synth_unseen)\n",
            "np.save('cached_arrays/y_synth_unseen.npy', y_synth_unseen)\n",
            "\n",
            "print(\"Saved synthetic embeddings to cached_arrays/:\")\n",
            "print(f\"  - E_synth_unseen.npy: {E_synth_unseen.shape}\")\n",
            "print(f\"  - y_synth_unseen.npy: {y_synth_unseen.shape}\")"
        ]
    },
    
    # Cell 20: Code - Real vs Synthetic t-SNE
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# t-SNE: REAL SEEN vs SYNTHETIC UNSEEN EMBEDDINGS\n",
            "# =============================================================================\n",
            "\n",
            "print(\"Computing t-SNE for real vs synthetic embeddings...\")\n",
            "\n",
            "# Sample subsets for visualization\n",
            "n_real_sample = min(1000, len(E_train_seen))\n",
            "n_synth_sample = min(1000, len(E_synth_unseen))\n",
            "\n",
            "real_idx = np.random.choice(len(E_train_seen), n_real_sample, replace=False)\n",
            "synth_idx = np.random.choice(len(E_synth_unseen), n_synth_sample, replace=False)\n",
            "\n",
            "E_real_sample = E_train_seen[real_idx]\n",
            "E_synth_sample = E_synth_unseen[synth_idx]\n",
            "\n",
            "# Combine for t-SNE\n",
            "combined = np.vstack([E_real_sample, E_synth_sample, S_seen_array[:30], S_unseen_array[:30]])\n",
            "\n",
            "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
            "combined_2d = tsne.fit_transform(combined)\n",
            "\n",
            "# Split back\n",
            "n1, n2, n3, n4 = n_real_sample, n_synth_sample, 30, 30\n",
            "real_2d = combined_2d[:n1]\n",
            "synth_2d = combined_2d[n1:n1+n2]\n",
            "seen_proto_2d = combined_2d[n1+n2:n1+n2+n3]\n",
            "unseen_proto_2d = combined_2d[n1+n2+n3:]\n",
            "\n",
            "# Plot\n",
            "plt.figure(figsize=(12, 8))\n",
            "plt.scatter(real_2d[:, 0], real_2d[:, 1], c='steelblue', alpha=0.3, s=15, label='Real seen embeddings')\n",
            "plt.scatter(synth_2d[:, 0], synth_2d[:, 1], c='darkorange', alpha=0.3, s=15, label='Synthetic unseen embeddings')\n",
            "plt.scatter(seen_proto_2d[:, 0], seen_proto_2d[:, 1], c='green', marker='*', s=150, label='Seen prototypes')\n",
            "plt.scatter(unseen_proto_2d[:, 0], unseen_proto_2d[:, 1], c='red', marker='^', s=150, label='Unseen prototypes')\n",
            "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
            "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
            "plt.title('Real Seen vs Synthetic Unseen Embeddings in CLIP Space', fontsize=14)\n",
            "plt.legend()\n",
            "plt.tight_layout()\n",
            "plt.savefig('figures/real_vs_synth_tsne.png', dpi=150, bbox_inches='tight')\n",
            "plt.show()\n",
            "\n",
            "print(f\"Saved: figures/real_vs_synth_tsne.png\")"
        ]
    },
    
    # Cell 21: Code - Final CLIP + WGAN Summary
    {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# =============================================================================\n",
            "# CLIP + cWGAN-GP IMPLEMENTATION SUMMARY\n",
            "# =============================================================================\n",
            "\n",
            "print(\"=\"*70)\n",
            "print(\"CLIP + cWGAN-GP IMPLEMENTATION SUMMARY\")\n",
            "print(\"=\"*70)\n",
            "\n",
            "print(\"\\n### CLIP ENCODER ###\")\n",
            "print(f\"  Embedding dimension: {CLIP_CONFIG['embed_dim']}\")\n",
            "print(f\"  Training epochs: {CLIP_CONFIG['epochs']}\")\n",
            "print(f\"  Final contrastive loss: {clip_losses[-1]:.4f}\")\n",
            "print(f\"  Embeddings: E_train_seen {E_train_seen.shape}, E_test_seen {E_test_seen.shape}, E_unseen {E_unseen.shape}\")\n",
            "print(f\"  Prototypes: S_seen {S_seen_array.shape}, S_unseen {S_unseen_array.shape}\")\n",
            "\n",
            "print(\"\\n### cWGAN-GP ###\")\n",
            "print(f\"  Generator steps: {WGAN_CONFIG['n_steps']}\")\n",
            "print(f\"  Final G_loss: {g_losses[-1]:.4f}\")\n",
            "print(f\"  Final C_loss: {c_losses[-1]:.4f}\")\n",
            "print(f\"  Synthetic per class: {WGAN_CONFIG['n_synth_per_class']}\")\n",
            "print(f\"  Total synthetic samples: {len(y_synth_unseen)}\")\n",
            "\n",
            "print(\"\\n### CACHED ARRAYS ###\")\n",
            "for f in sorted(os.listdir('cached_arrays')):\n",
            "    print(f\"  - cached_arrays/{f}\")\n",
            "\n",
            "print(\"\\n### FIGURES GENERATED ###\")\n",
            "print(\"  - figures/clip_loss_curve.png\")\n",
            "print(\"  - figures/clip_embedding_tsne.png\")\n",
            "print(\"  - figures/wgan_losses.png\")\n",
            "print(\"  - figures/real_vs_synth_tsne.png\")\n",
            "\n",
            "print(\"\\n\" + \"=\"*70)\n",
            "print(\"NEXT STEPS: Train GZSL Classifier [A+B] on real + synthetic embeddings\")\n",
            "print(\"=\"*70)"
        ]
    }
]


def main():
    if not os.path.exists(NOTEBOOK_PATH):
        print(f"Error: Notebook not found at {NOTEBOOK_PATH}")
        return
    
    print(f"Loading notebook: {NOTEBOOK_PATH}")
    with open(NOTEBOOK_PATH, 'r', encoding='utf-8') as f:
        notebook = json.load(f)
    
    original_count = len(notebook['cells'])
    notebook['cells'].extend(NEW_CELLS)
    
    print(f"Adding {len(NEW_CELLS)} new cells...")
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(notebook, f, indent=2)
    
    print(f"\nSuccess! Updated: {OUTPUT_PATH}")
    print(f"  - Original cells: {original_count}")
    print(f"  - New cells added: {len(NEW_CELLS)}")
    print(f"  - Total cells: {len(notebook['cells'])}")
    print(f"\nRefresh the notebook in Jupyter and run the new cells.")


if __name__ == "__main__":
    main()
